<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Hypothesis Tests on Mean Vectors | STAT 372 Open Textbook (Python)</title>
  <meta name="description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Hypothesis Tests on Mean Vectors | STAT 372 Open Textbook (Python)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Hypothesis Tests on Mean Vectors | STAT 372 Open Textbook (Python)" />
  
  <meta name="twitter:description" content="This is an open textbook resource for the STAT372 course at MacEwan University, an introduction to Multivariate Statistics and Machine Learning." />
  

<meta name="author" content="Dr. Wanhua Su" />


<meta name="date" content="2025-08-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="displaying-multivariate-data-and-measures-of-distance.html"/>
<link rel="next" href="principal-component-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preamble</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#some-examples"><i class="fa fa-check"></i><b>1.2</b> Some Examples</a>
<ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-1-storm-survival-of-sparrows"><i class="fa fa-check"></i>Example 1: Storm Survival of Sparrows</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-2-spam-or-e-mail"><i class="fa fa-check"></i>Example 2: Spam or E-mail?</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-3-classification-of-iris"><i class="fa fa-check"></i>Example 3: Classification of Iris</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#multivariate-methods-covered-in-stat-372"><i class="fa fa-check"></i><b>1.3</b> Multivariate Methods Covered in STAT 372</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#review-univariate-analysis"><i class="fa fa-check"></i><b>1.4</b> Review: Univariate Analysis</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#random-variable-and-its-distribution"><i class="fa fa-check"></i><b>1.4.1</b> Random Variable and Its Distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>1.4.2</b> Properties of Expectation and Variance</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#continuous-random-variables"><i class="fa fa-check"></i><b>1.4.3</b> Continuous Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#revisit-learning-learning-outcomes"><i class="fa fa-check"></i>Revisit Learning Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="matrix-algebra.html"><a href="matrix-algebra.html"><i class="fa fa-check"></i><b>2</b> Matrix Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="matrix-algebra.html"><a href="matrix-algebra.html#learning-outcomes-1"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="2.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#vectors"><i class="fa fa-check"></i><b>2.1</b> Vectors</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#some-basic-operations-on-vectors"><i class="fa fa-check"></i><b>2.1.1</b> Some Basic Operations on Vectors</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrices"><i class="fa fa-check"></i><b>2.2</b> Matrices</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#basic-operations-on-matrix"><i class="fa fa-check"></i><b>2.2.1</b> Basic Operations on Matrix</a></li>
<li class="chapter" data-level="2.2.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.2.2</b> Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.2.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#spectral-eigen-decomposition"><i class="fa fa-check"></i><b>2.2.3</b> Spectral (Eigen) Decomposition</a></li>
<li class="chapter" data-level="2.2.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.2.4</b> Singular-Value Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#mean-vectors-and-covariance-matrices"><i class="fa fa-check"></i><b>2.3</b> Mean Vectors and Covariance Matrices</a></li>
<li class="chapter" data-level="2.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#sample-mean-vector-and-covariance-matrix"><i class="fa fa-check"></i><b>2.4</b> Sample Mean Vector and Covariance Matrix</a></li>
<li class="chapter" data-level="2.5" data-path="matrix-algebra.html"><a href="matrix-algebra.html#review-exercises"><i class="fa fa-check"></i><b>2.5</b> Review Exercises</a></li>
<li class="chapter" data-level="" data-path="matrix-algebra.html"><a href="matrix-algebra.html#revisit-the-learning-outcomes"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html"><i class="fa fa-check"></i><b>3</b> Displaying Multivariate Data and Measures of Distance</a>
<ul>
<li class="chapter" data-level="" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#learning-outcomes-2"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="3.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#display-multivariate-data"><i class="fa fa-check"></i><b>3.1</b> Display Multivariate Data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#scatterplot"><i class="fa fa-check"></i><b>3.1.1</b> Scatterplot</a></li>
<li class="chapter" data-level="3.1.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#graphs-of-growth-curves"><i class="fa fa-check"></i><b>3.1.2</b> Graphs of Growth Curves</a></li>
<li class="chapter" data-level="3.1.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#star-plots"><i class="fa fa-check"></i><b>3.1.3</b> Star Plots</a></li>
<li class="chapter" data-level="3.1.4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#chernoff-faces-plot"><i class="fa fa-check"></i><b>3.1.4</b> Chernoff Faces Plot</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-in-multivariate-analysis"><i class="fa fa-check"></i><b>3.2</b> Distance in Multivariate Analysis</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distances-for-quantitative-variables"><i class="fa fa-check"></i><b>3.2.1</b> Distances for Quantitative Variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-for-categorical-variables"><i class="fa fa-check"></i><b>3.2.2</b> Distance for Categorical Variables</a></li>
<li class="chapter" data-level="3.2.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distance-for-mixed-variable-types"><i class="fa fa-check"></i><b>3.2.3</b> Distance for Mixed Variable Types</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3</b> Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#properties-of-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3.1</b> Properties of Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="3.3.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3.2</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="3.3.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#contour-of-multivariate-normal-distribution"><i class="fa fa-check"></i><b>3.3.3</b> Contour of Multivariate Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#the-sampling-distribution-of-mathbfbar-x-and-boldsymbols"><i class="fa fa-check"></i><b>3.4</b> The Sampling Distribution of <span class="math inline">\(\mathbf{\bar X}\)</span> and <span class="math inline">\(\boldsymbol{S}\)</span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#distributions-related-to-normal-distribution"><i class="fa fa-check"></i><b>3.4.1</b> Distributions Related to Normal Distribution</a></li>
<li class="chapter" data-level="3.4.2" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#applications-to-distributions-related-to-sample-means"><i class="fa fa-check"></i><b>3.4.2</b> Applications to Distributions Related to Sample Means</a></li>
<li class="chapter" data-level="3.4.3" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#generalize-to-multivariate-cases"><i class="fa fa-check"></i><b>3.4.3</b> Generalize to Multivariate Cases</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#review-exercises-1"><i class="fa fa-check"></i><b>3.5</b> Review Exercises</a></li>
<li class="chapter" data-level="" data-path="displaying-multivariate-data-and-measures-of-distance.html"><a href="displaying-multivariate-data-and-measures-of-distance.html#revisit-the-learning-outcomes-1"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html"><i class="fa fa-check"></i><b>4</b> Hypothesis Tests on Mean Vectors</a>
<ul>
<li class="chapter" data-level="" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#learning-outcomes-3"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="4.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-one-mean-vector"><i class="fa fa-check"></i><b>4.1</b> Hypothesis Test for one Mean Vector</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case"><i class="fa fa-check"></i><b>4.1.1</b> Univariate Case</a></li>
<li class="chapter" data-level="4.1.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case"><i class="fa fa-check"></i><b>4.1.2</b> Multivariate Case</a></li>
<li class="chapter" data-level="4.1.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#evaluating-multivariate-normality"><i class="fa fa-check"></i><b>4.1.3</b> Evaluating Multivariate Normality</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-two-mean-vectors"><i class="fa fa-check"></i><b>4.2</b> Hypothesis Test for Two Mean Vectors</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-two-independent-samples"><i class="fa fa-check"></i><b>4.2.1</b> Univariate Case Based on Two Independent Samples</a></li>
<li class="chapter" data-level="4.2.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-two-independent-samples"><i class="fa fa-check"></i><b>4.2.2</b> Multivariate Case Based on Two Independent Samples</a></li>
<li class="chapter" data-level="4.2.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#two-sample-non-pooled-hotellings-t2-test"><i class="fa fa-check"></i><b>4.2.3</b> Two-sample Non-pooled Hotelling’s <span class="math inline">\(T^2\)</span> Test</a></li>
<li class="chapter" data-level="4.2.4" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#two-sample-hotellings-t2-confidence-interval"><i class="fa fa-check"></i><b>4.2.4</b> Two-sample Hotelling’s <span class="math inline">\(T^2\)</span> Confidence Interval</a></li>
<li class="chapter" data-level="4.2.5" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-a-paired-sample"><i class="fa fa-check"></i><b>4.2.5</b> Univariate Case Based on a Paired Sample</a></li>
<li class="chapter" data-level="4.2.6" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-a-paired-sample"><i class="fa fa-check"></i><b>4.2.6</b> Multivariate Case Based on a Paired Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-several-mean-vectors"><i class="fa fa-check"></i><b>4.3</b> Hypothesis Test for Several Mean Vectors</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#univariate-case-one-way-anova-f-test"><i class="fa fa-check"></i><b>4.3.1</b> Univariate Case: One-Way ANOVA F Test</a></li>
<li class="chapter" data-level="4.3.2" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-one-way-manova"><i class="fa fa-check"></i><b>4.3.2</b> Multivariate Case: One-Way MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests-on-mean-vectors.html"><a href="hypothesis-tests-on-mean-vectors.html#revisit-the-learning-outcomes-2"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#learning-outcomes-4"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#finding-the-principal-components"><i class="fa fa-check"></i><b>5.1</b> Finding the Principal Components</a></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#scaling-in-principal-component-analysis"><i class="fa fa-check"></i><b>5.2</b> Scaling in Principal Component Analysis</a></li>
<li class="chapter" data-level="5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#limitations-of-principal-component-analysis"><i class="fa fa-check"></i><b>5.3</b> Limitations of Principal Component Analysis</a></li>
<li class="chapter" data-level="5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#further-reading"><i class="fa fa-check"></i><b>5.4</b> Further Reading</a></li>
<li class="chapter" data-level="" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#revisit-the-learning-outcomes-3"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="factor-analysis.html"><a href="factor-analysis.html#learning-outcomes-5"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="6.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-of-factor-analysis"><i class="fa fa-check"></i><b>6.1</b> Model of Factor Analysis</a></li>
<li class="chapter" data-level="6.2" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-loadings-l_ij-and-specific-variance-psi_i"><i class="fa fa-check"></i><b>6.2</b> Estimating Factor Loadings <span class="math inline">\(l_{ij}\)</span> and Specific Variance <span class="math inline">\(\psi_i\)</span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#the-principle-component-method"><i class="fa fa-check"></i><b>6.2.1</b> The Principle Component Method</a></li>
<li class="chapter" data-level="6.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#the-maximum-likelihood-method"><i class="fa fa-check"></i><b>6.2.2</b> The Maximum Likelihood Method</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-rotation"><i class="fa fa-check"></i><b>6.3</b> Factor Rotation</a></li>
<li class="chapter" data-level="6.4" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-scores"><i class="fa fa-check"></i><b>6.4</b> Factor Scores</a></li>
<li class="chapter" data-level="" data-path="factor-analysis.html"><a href="factor-analysis.html#revisit-the-learning-outcomes-4"><i class="fa fa-check"></i>Revisit the Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html"><i class="fa fa-check"></i><b>7</b> Discriminant Analysis and Classification</a>
<ul>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#learning-outcomes-6"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="7.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#performance-measure"><i class="fa fa-check"></i><b>7.2</b> Performance Measure</a></li>
<li class="chapter" data-level="7.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#overfitting-and-cross-validation"><i class="fa fa-check"></i><b>7.3</b> Overfitting and Cross Validation</a></li>
<li class="chapter" data-level="7.4" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classification-models"><i class="fa fa-check"></i><b>7.4</b> Classification Models</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(K\)</span> Nearest Neighbors</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#logistic-regression-for-binary-response"><i class="fa fa-check"></i><b>7.5</b> Logistic Regression for Binary Response</a>
<ul>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#interpretation-of-beta_i"><i class="fa fa-check"></i>Interpretation of <span class="math inline">\(\beta_i\)</span></a></li>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#estimation-of-beta_i"><i class="fa fa-check"></i>Estimation of <span class="math inline">\(\beta_i\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#logistic-regression-for-multi-class-nominal-data"><i class="fa fa-check"></i><b>7.6</b> Logistic Regression for Multi-class Nominal Data</a></li>
<li class="chapter" data-level="7.7" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#cumulative-logit-model-for-multi-class-ordinal-data"><i class="fa fa-check"></i><b>7.7</b> Cumulative Logit Model for Multi-class Ordinal Data</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#cumulative-logit-models-with-proportional-odds"><i class="fa fa-check"></i><b>7.7.1</b> Cumulative Logit Models with Proportional Odds</a></li>
<li class="chapter" data-level="7.7.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-probability-of-each-category"><i class="fa fa-check"></i><b>7.7.2</b> Model Probability of Each Category</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-selection-for-logistic-regression"><i class="fa fa-check"></i><b>7.8</b> Model Selection for Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#aic-and-bic"><i class="fa fa-check"></i><b>7.8.1</b> AIC and BIC</a></li>
<li class="chapter" data-level="7.8.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#forward-selection"><i class="fa fa-check"></i><b>7.8.2</b> Forward Selection</a></li>
<li class="chapter" data-level="7.8.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#backward-elimination"><i class="fa fa-check"></i><b>7.8.3</b> Backward Elimination</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#model-checking"><i class="fa fa-check"></i><b>7.9</b> Model Checking</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#residual-analysis"><i class="fa fa-check"></i><b>7.9.1</b> Residual Analysis</a></li>
<li class="chapter" data-level="7.9.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#preditive-power-accuracy-and-roc-curve"><i class="fa fa-check"></i><b>7.9.2</b> Preditive Power: Accuracy and ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classification-tree-recursive-partitioning"><i class="fa fa-check"></i><b>7.10</b> Classification Tree (Recursive Partitioning)</a></li>
<li class="chapter" data-level="7.11" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#regression-tree"><i class="fa fa-check"></i><b>7.11</b> Regression Tree</a></li>
<li class="chapter" data-level="7.12" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#random-forest"><i class="fa fa-check"></i><b>7.12</b> Random Forest</a></li>
<li class="chapter" data-level="7.13" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#support-vector-machines"><i class="fa fa-check"></i><b>7.13</b> Support Vector Machines</a></li>
<li class="chapter" data-level="7.14" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#neural-networks"><i class="fa fa-check"></i><b>7.14</b> Neural Networks</a></li>
<li class="chapter" data-level="7.15" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#classical-methods"><i class="fa fa-check"></i><b>7.15</b> Classical Methods</a>
<ul>
<li class="chapter" data-level="7.15.1" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#mahalanobis-distance-method"><i class="fa fa-check"></i><b>7.15.1</b> Mahalanobis Distance Method</a></li>
<li class="chapter" data-level="7.15.2" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#bayes-posterior"><i class="fa fa-check"></i><b>7.15.2</b> Bayes Posterior</a></li>
<li class="chapter" data-level="7.15.3" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#fishers-discriminant-analysis"><i class="fa fa-check"></i><b>7.15.3</b> Fisher’s Discriminant Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.16" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#summary"><i class="fa fa-check"></i><b>7.16</b> Summary</a></li>
<li class="chapter" data-level="" data-path="discriminant-analysis-and-classification.html"><a href="discriminant-analysis-and-classification.html#revisit-learning-outcomes"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="clustering-analysis.html"><a href="clustering-analysis.html"><i class="fa fa-check"></i><b>8</b> Clustering Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#learning-outcomes-7"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="8.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#introduction-2"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#clustering-methods"><i class="fa fa-check"></i><b>8.2</b> Clustering Methods</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#hierarchical-method"><i class="fa fa-check"></i><b>8.2.1</b> Hierarchical Method</a></li>
<li class="chapter" data-level="8.2.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#k-means"><i class="fa fa-check"></i><b>8.2.2</b> K-Means</a></li>
<li class="chapter" data-level="8.2.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#model-based-clustering"><i class="fa fa-check"></i><b>8.2.3</b> Model-Based Clustering</a></li>
<li class="chapter" data-level="8.2.4" data-path="clustering-analysis.html"><a href="clustering-analysis.html#pros-and-cons-of-clustering-methods"><i class="fa fa-check"></i><b>8.2.4</b> Pros and Cons of Clustering Methods</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#determine-k-number-of-clusters"><i class="fa fa-check"></i><b>8.3</b> Determine <span class="math inline">\(K\)</span>: Number of Clusters</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#the-elbow-plot-method"><i class="fa fa-check"></i><b>8.3.1</b> The Elbow Plot Method</a></li>
<li class="chapter" data-level="8.3.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#the-silhouette-score"><i class="fa fa-check"></i><b>8.3.2</b> The Silhouette Score</a></li>
<li class="chapter" data-level="8.3.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#gap-statistics"><i class="fa fa-check"></i><b>8.3.3</b> Gap Statistics</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#side-note-on-the-em-algorithm"><i class="fa fa-check"></i>Side-Note on the EM Algorithm</a></li>
<li class="chapter" data-level="" data-path="clustering-analysis.html"><a href="clustering-analysis.html#revisit-learning-outcomes-1"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html"><i class="fa fa-check"></i><b>9</b> Canonical Correlation Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#learning-outcomes-8"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="9.1" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#objective"><i class="fa fa-check"></i><b>9.1</b> Objective</a></li>
<li class="chapter" data-level="9.2" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#obtain-the-canonical-variates-pairs"><i class="fa fa-check"></i><b>9.2</b> Obtain the Canonical Variates Pairs</a></li>
<li class="chapter" data-level="9.3" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#interpretation"><i class="fa fa-check"></i><b>9.3</b> Interpretation</a></li>
<li class="chapter" data-level="9.4" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#testing-mathbfsigma_120"><i class="fa fa-check"></i><b>9.4</b> Testing <span class="math inline">\(\mathbf{\Sigma}_{12}=0\)</span></a></li>
<li class="chapter" data-level="" data-path="canonical-correlation-analysis.html"><a href="canonical-correlation-analysis.html#revisit-learning-outcomes-2"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html"><i class="fa fa-check"></i><b>10</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#learning-outcomes-9"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="10.1" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#objective-1"><i class="fa fa-check"></i><b>10.1</b> Objective</a></li>
<li class="chapter" data-level="10.2" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#methods"><i class="fa fa-check"></i><b>10.2</b> Methods</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#classical-scaling"><i class="fa fa-check"></i><b>10.2.1</b> Classical Scaling</a></li>
<li class="chapter" data-level="10.2.2" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#metric-scaling"><i class="fa fa-check"></i><b>10.2.2</b> Metric Scaling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#example"><i class="fa fa-check"></i><b>10.3</b> Example</a></li>
<li class="chapter" data-level="" data-path="multidimensional-scaling.html"><a href="multidimensional-scaling.html#revisit-learning-outcomes-3"><i class="fa fa-check"></i>Revisit Learning Outcomes</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 372 Open Textbook (Python)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-tests-on-mean-vectors" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Hypothesis Tests on Mean Vectors<a href="hypothesis-tests-on-mean-vectors.html#hypothesis-tests-on-mean-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the univariate case, a one-sample <span class="math inline">\(t\)</span> test is used for the hypothesis test for one population mean, the test statistic is <span class="math inline">\(t=\frac{\bar x-\mu_0}{s/\sqrt{n}}\)</span>. We apply the one-sample <span class="math inline">\(t\)</span> test on the paired differences when comparing two population means with paired samples. When comparing two population means with two independent samples, we use a non-pooled two-sample <span class="math inline">\(t\)</span> test. The test statistic is
<span class="math display">\[
t=\frac{(\bar x_1-\bar x_2)-\Delta_0}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}
\]</span>
If the two population variances <span class="math inline">\(\sigma_1^2, \sigma_2^2\)</span> are the same, we can combine the two samples and the common variance can be estimated as
<span class="math display">\[
s_p^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}
\]</span>
and the test statistic becomes
<span class="math display">\[
t=\frac{(\bar x_1-\bar x_2)-\Delta_0}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}
\]</span>
When comparing more than two population means, we use one-way ANOVA. This note covers how to conduct hypothesis tests on the mean vector for multivariate data.</p>
<div id="learning-outcomes-3" class="section level2 unnumbered hasAnchor">
<h2>Learning Outcomes<a href="hypothesis-tests-on-mean-vectors.html#learning-outcomes-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After finishing this note, students should be able to</p>
<ul>
<li>Explain the main idea of using a chi-square Q-Q plot to assess the multivariate normality assumption.</li>
<li>Assess the multivariate normality through a chi-square Q-Q plot and casual procedures using R.</li>
<li>Conduct a Hotelling’s <span class="math inline">\(T^2\)</span> test on one mean vector based on one sample or paired sample.</li>
<li>Conduct a Hotelling’s <span class="math inline">\(T^2\)</span> test on two mean vectors based on two independent samples.</li>
<li>Conduct a one-way MANOVA test on at least two mean vectors based on at least two independent samples.</li>
<li>Obtain <span class="math inline">\((1-\alpha)\times 100\%\)</span> Bonferroni confidence intervals associated with a certain test if applicable.</li>
</ul>
</div>
<div id="hypothesis-test-for-one-mean-vector" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Hypothesis Test for one Mean Vector<a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-one-mean-vector" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s first review hypothesis tests for one population mean in the univarite case.</p>
<div id="univariate-case" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Univariate Case<a href="hypothesis-tests-on-mean-vectors.html#univariate-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For one population mean in the univariate case, the hypotheses for a two-tailed test are:
<span class="math display">\[
H_0: \mu=\mu_0 \mbox{  versus  } H_a: \mu\ne \mu_0.
\]</span>
By the critical value approach, we reject <span class="math inline">\(H_0: \mu=\mu_0\)</span> at the significance level <span class="math inline">\(\alpha\)</span> if the observed test statistic
<span class="math display">\[
t_o=\frac{\bar x-\mu_0}{\frac{s}{\sqrt{n}}}\ge t_{n-1} (\frac{\alpha}{2}) \mbox{  or   } t_o\le -t_{n-1} (\frac{\alpha}{2})
\]</span>
where <span class="math inline">\(t_{n-1} (\frac{\alpha}{2})\)</span> is the <span class="math inline">\(t\)</span>-score with an area <span class="math inline">\(\frac{\alpha}{2}\)</span> to its right under a <span class="math inline">\(t\)</span> density curve with <span class="math inline">\(df=n-1\)</span>. It is equivalent to reject <span class="math inline">\(H_0\)</span> if
<span class="math display">\[
|t_o|=\frac{|\bar x-\mu_0|}{s/\sqrt{n}}\ge t_{n-1} (\frac{\alpha}{2}) \mbox{ or if  } t_o^2=\frac{(\bar x-\mu_0)^2}{s^2/n} \ge F_{1, n-1}(\alpha).
\]</span>
Note that
<span class="math display">\[
T^2=\frac{(\bar X-\mu)^2}{s^2/n}=\frac{\left(\frac{\bar X-\mu}{\sigma/\sqrt{n}}\right)^2}{s^2/\sigma^2}\sim F_{1, n-1}
\]</span>
because <span class="math inline">\(\left(\frac{\bar X-\mu}{\sigma/\sqrt{n}}\right)^2\sim \chi^2_1\)</span> and <span class="math inline">\(\frac{(n-1)s^2}{\sigma^2}\sim \chi^2_{n-1}\)</span>. The term <span class="math inline">\(T^2=\frac{(\bar X-\mu)^2}{s^2/n}\)</span> can be rewritten as
<span class="math display">\[
T^2=n(\bar X-\mu)(s^2)^{-1}(\bar X-\mu).
\]</span></p>
</div>
<div id="multivariate-case" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Multivariate Case<a href="hypothesis-tests-on-mean-vectors.html#multivariate-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The one-sample t test can be generalized to the multivariate case for one population mean vector.</p>
<div id="one-sample-hotellings-t2-test" class="section level4 unnumbered hasAnchor">
<h4>One-sample Hotelling’s <span class="math inline">\(T^2\)</span> Test<a href="hypothesis-tests-on-mean-vectors.html#one-sample-hotellings-t2-test" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For multivariate cases, each observation has multiple measurements in a vector form. The hypotheses for a two-tailed test are:
<span class="math display">\[
H_0: \boldsymbol{\mu}=\boldsymbol{\mu}_0 \mbox{  versus  } H_a: \boldsymbol{\mu}\ne \boldsymbol{\mu}_0.
\]</span>
Reject <span class="math inline">\(H_0: \boldsymbol{\mu}=\boldsymbol{\mu}_0\)</span> if the test statistic <span class="math inline">\(T^2=n(\mathbf{\bar X}-\boldsymbol{\mu}_0)^T\mathbf{S}^{-1}(\mathbf{\bar X}-\boldsymbol{\mu}_0)\)</span> is too large, where <span class="math inline">\(\mathbf{\bar X}_{p \times 1}\)</span> is the sample mean vector and <span class="math inline">\(\mathbf{S}_{p\times p}\)</span> is the sample covariance matrix respectively based on the observation matrix <span class="math inline">\(\mathbf{X}_{n \times p}\)</span>. The test statistic <span class="math inline">\(T^2\)</span> follows a <em>Hotelling’s</em> <span class="math inline">\(T^2\)</span> distribution with degrees of freedom <span class="math inline">\(p\)</span> and <span class="math inline">\(n-1\)</span>. It can be shown that
<span class="math display">\[
\frac{n-p}{p(n-1)}T^2_{p, n-1}\sim F_{p, n-p}.
\]</span>
Therefore, we reject <span class="math inline">\(H_0: \boldsymbol{\mu}=\boldsymbol{\mu}_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if
<span class="math display">\[
\frac{n(n-p)}{p(n-1)}(\mathbf{\bar x}-\boldsymbol{\mu}_0)^T\mathbf{S}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu}_0)\ge F_{p, n-p}(\alpha)
\]</span>
where <span class="math inline">\(F_{p, n-p}(\alpha)\)</span> is the upper <span class="math inline">\(100\alpha\)</span>th percentile of the <span class="math inline">\(F_{p, n-p}\)</span> distribution.</p>
<p>A <span class="math inline">\((1-\alpha)\times 100 \%\)</span> <em>confidence region</em> for the mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> is the ellipsoid such that
<span class="math display">\[
(\mathbf{\bar x}-\boldsymbol{\mu})^T\mathbf{S}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})\le c^2=\frac{p(n-1)}{n(n-p)} F_{p, n-p}(\alpha).
\]</span>
The ellipsoid is centered at <span class="math inline">\(\mathbf{\bar x}\)</span> and has axes <span class="math inline">\(\pm \sqrt{\frac{p(n-1)}{n(n-p)} F_{p, n-p}(\alpha)}\sqrt{\lambda_i}\mathbf{e}_i\)</span>, where <span class="math inline">\(\lambda_i\)</span> and <span class="math inline">\(\mathbf{e}_i\)</span> are the eigenvalues and corresponding unit eigenvectors of the sample covariance matrix <span class="math inline">\(\mathbf{S}\)</span>. We should reject <span class="math inline">\(H_0: \boldsymbol{\mu}=\boldsymbol{\mu}_0\)</span> if <span class="math inline">\(\boldsymbol{\mu}_0\)</span> is outside the confidence region, i.e., if the distance
<span class="math display">\[
(\mathbf{\bar x}-\boldsymbol{\mu}_0)^T\mathbf{S}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu}_0) &gt;\frac{p(n-1)}{n(n-p)} F_{p, n-p}(\alpha).
\]</span></p>
<p><span class="math inline">\(\textbf{Side note}\)</span>: The derivation of the distribution of <span class="math inline">\(T^2\)</span> is based on the Wishart distribution and its properties, which is outside the scope of this course. Some more details are provided here and are not required for exams.</p>
<ul>
<li>If <span class="math inline">\(\mathbf{y}_1, \mathbf{y}_2, \cdots, \mathbf{y}_n\)</span> are identically independently follow a multivariate normal distribution <span class="math inline">\(N_p(\mathbf{0}, \boldsymbol{\Sigma})\)</span>, then <span class="math inline">\(\mathbf{A}=\sum_{i=1}^n \mathbf{y}_i \mathbf{y}_i^T\sim W_p(n, \boldsymbol{\Sigma})\)</span>, a <span class="math inline">\(p\)</span> dimensional Wishart distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.</li>
<li>If <span class="math inline">\(\mathbf{A} \sim W_p(n, \boldsymbol{\Sigma})\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is a <span class="math inline">\(p\times 1\)</span> random vector independent of <span class="math inline">\(\mathbf{A}\)</span>, then
<span class="math display">\[
\frac{\mathbf{y}^T\boldsymbol{\Sigma}^{-1} \mathbf{y}}{\mathbf{y}^T\mathbf{A}^{-1} \mathbf{y}}\sim \chi_{n-(p-1)}^2 \mbox{    and independent of $\mathbf{y}$.}
\]</span></li>
</ul>
<p>Given the definition of Wishart distribution and its property, the distribution of <span class="math inline">\(T^2\)</span> can be obtained as follows:</p>
<ul>
<li>Suppose <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\)</span> are identically independently follow a multivariate normal distribution <span class="math inline">\(N_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>, then the sample mean vector <span class="math inline">\(\mathbf{\bar x}\)</span> and the sample covariance matrix <span class="math inline">\(\mathbf{S}\)</span> have the following distributions
<span class="math display">\[
\mathbf{\bar x}\sim N_p\left(\boldsymbol{\mu},  \frac{\boldsymbol{\Sigma}}{n}\right), \quad \quad (n-1)\mathbf{S}=\sum_{i=1}^n (\mathbf{x}_i-\mathbf{\bar x})(\mathbf{x}_i-\mathbf{\bar x})^T\sim W_p(n-1, \boldsymbol{\Sigma})
\]</span>
and the sample mean vector <span class="math inline">\(\mathbf{\bar x}\)</span> and the sample covariance matrix <span class="math inline">\(\mathbf{S}\)</span> are independent. This implies random vector <span class="math inline">\(\mathbf{\bar x}-\boldsymbol{\mu}\sim N_p\left(\boldsymbol{0},  \frac{\boldsymbol{\Sigma}}{n}\right)\)</span> and independent of <span class="math inline">\(\mathbf{S}\)</span>.
Since
<span class="math display">\[
\begin{aligned}
U&amp;=(\mathbf{\bar x}-\boldsymbol{\mu})^T\left(\frac{\boldsymbol{\Sigma}}{n}\right)^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})=n(\mathbf{\bar x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})\sim \chi_p^2\\
V&amp;=\frac{(\mathbf{\bar x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})}{(\mathbf{\bar x}-\boldsymbol{\mu})^T[(n-1)\mathbf{S}]^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})}=\frac{(n-1)(\mathbf{\bar x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})}{(\mathbf{\bar x}-\boldsymbol{\mu})^T\mathbf{S}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})}\sim \chi_{(n-1)-(p-1)}^2
\end{aligned}
\]</span>
and <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are independent,
<span class="math display">\[
\frac{U/p}{V/(n-p)}\sim F_{p, n-p}\Longrightarrow \left(\frac{n-p}{p}\right)\left(\frac{n}{n-1}\right)(\mathbf{\bar x}-\boldsymbol{\mu})^T\mathbf{S}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu})=\frac{n-p}{p(n-1)}T^2\sim F_{p, n-p}.
\]</span></li>
</ul>
</div>
<div id="one-sample-hotellings-t2-confidence-interval" class="section level4 unnumbered hasAnchor">
<h4>One-sample Hotelling’s <span class="math inline">\(T^2\)</span> Confidence Interval<a href="hypothesis-tests-on-mean-vectors.html#one-sample-hotellings-t2-confidence-interval" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall the one-way ANOVA F test, we will follow up with a posthoc test to figure out which pairs are significantly different if we reject the null hypothesis that all means are equal. Similarly, for a one-mean Hotelling’s <span class="math inline">\(T^2\)</span> test, if we reject <span class="math inline">\(H_0: \boldsymbol{\mu}=\boldsymbol{\mu}_0\)</span>, we would like to figure out in which variables the means are different.</p>
<ul>
<li>A <span class="math inline">\((1-\alpha)\times 100 \%\)</span> simultaneous confidence interval for the <span class="math inline">\(i\)</span>th variable <span class="math inline">\(X_i\)</span> is given by
<span class="math display">\[
\bar x_i \pm \sqrt{\frac{p(n-1)}{(n-p)} F_{p, n-p}(\alpha)}\sqrt{\frac{s_i^2}{n}}, i=1, 2, \cdots, p
\]</span>
where <span class="math inline">\(\bar x_i\)</span> and <span class="math inline">\(s_i^2\)</span> are the sample mean and sample variance of <span class="math inline">\(X_i\)</span>.</li>
<li>A <span class="math inline">\((1-\alpha)\times 100 \%\)</span> Bonferroni interval\
The problem with the simultaneous confidence intervals is that if we are not interested in all <span class="math inline">\(p\)</span> variables, the simultaneous confidence interval may be too wide, and, hence, too conservative. This is called the {} problem. We can adopt the Bonferroni adjustment to fix this problem. A <span class="math inline">\((1-\alpha)\times 100 \%\)</span> Bonferroni interval is given by
<span class="math display">\[
\bar x_i \pm t_{n-1}(\frac{\alpha}{2p})\sqrt{\frac{s_i^2}{n}}, i=1, 2, \cdots, p,
\]</span>
where <span class="math inline">\(t_{n-1}(\frac{\alpha}{2p})\)</span> is the <span class="math inline">\(\frac{\alpha}{2p}\times 100\)</span>th upper-tailed quantile of the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df=n-1\)</span>.The mean vector differs in <span class="math inline">\(X_i\)</span> if <span class="math inline">\(\mu_{0,i}\)</span> is outside the interval.</li>
</ul>
<p><span class="math inline">\(\textbf{Example: One-sample Hotelling&#39;s } \boldsymbol{T^2} \textbf{ Test}\)</span></p>
<p>Given the data matrix
<span class="math display">\[
\mathbf{X}=
\left[
\begin{array}{cc}
6&amp; 9\\
10&amp; 6\\
8&amp;3
\end{array}
\right]
\]</span></p>
<ul>
<li><p>Find the sample mean vector and covariance matrix.</p></li>
<li><p>Test at the 5% significance level
<span class="math display">\[
H_0: \boldsymbol{\mu}=\left[
\begin{array}{c}
9\\
5
\end{array}
\right]
\text{versus } H_a: \boldsymbol{\mu}\ne\left[
\begin{array}{c}
9\\
5
\end{array}
\right].
\]</span></p></li>
</ul>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="hypothesis-tests-on-mean-vectors.html#cb51-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb51-2"><a href="hypothesis-tests-on-mean-vectors.html#cb51-2" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> f</span>
<span id="cb51-3"><a href="hypothesis-tests-on-mean-vectors.html#cb51-3" tabindex="-1"></a></span>
<span id="cb51-4"><a href="hypothesis-tests-on-mean-vectors.html#cb51-4" tabindex="-1"></a>xmat <span class="op">=</span> np.array([[<span class="dv">6</span>, <span class="dv">9</span>], [<span class="dv">10</span>, <span class="dv">6</span>], [<span class="dv">8</span>, <span class="dv">3</span>]])</span>
<span id="cb51-5"><a href="hypothesis-tests-on-mean-vectors.html#cb51-5" tabindex="-1"></a>mu0 <span class="op">=</span> np.array([<span class="dv">9</span>, <span class="dv">5</span>])</span>
<span id="cb51-6"><a href="hypothesis-tests-on-mean-vectors.html#cb51-6" tabindex="-1"></a></span>
<span id="cb51-7"><a href="hypothesis-tests-on-mean-vectors.html#cb51-7" tabindex="-1"></a><span class="co"># manually in python as there is no one package for all tests</span></span>
<span id="cb51-8"><a href="hypothesis-tests-on-mean-vectors.html#cb51-8" tabindex="-1"></a>n, p <span class="op">=</span> xmat.shape</span>
<span id="cb51-9"><a href="hypothesis-tests-on-mean-vectors.html#cb51-9" tabindex="-1"></a>diff <span class="op">=</span> np.mean(xmat, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">-</span> mu0</span>
<span id="cb51-10"><a href="hypothesis-tests-on-mean-vectors.html#cb51-10" tabindex="-1"></a>inv_cov <span class="op">=</span> np.linalg.inv(np.cov(xmat, rowvar<span class="op">=</span><span class="va">False</span>, ddof<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb51-11"><a href="hypothesis-tests-on-mean-vectors.html#cb51-11" tabindex="-1"></a>t_squared <span class="op">=</span> n <span class="op">*</span> diff <span class="op">@</span> inv_cov <span class="op">@</span> diff <span class="co"># formula for T squared</span></span>
<span id="cb51-12"><a href="hypothesis-tests-on-mean-vectors.html#cb51-12" tabindex="-1"></a>f_stat <span class="op">=</span> (n <span class="op">-</span> p) <span class="op">/</span> (p <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>)) <span class="op">*</span> t_squared <span class="co"># formula for F statistic</span></span>
<span id="cb51-13"><a href="hypothesis-tests-on-mean-vectors.html#cb51-13" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> f.cdf(f_stat, p, n <span class="op">-</span> p) <span class="co"># formula for p-value</span></span>
<span id="cb51-14"><a href="hypothesis-tests-on-mean-vectors.html#cb51-14" tabindex="-1"></a></span>
<span id="cb51-15"><a href="hypothesis-tests-on-mean-vectors.html#cb51-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;T² statistic: </span><span class="sc">{</span>t_squared<span class="sc">:.5f}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>## T² statistic: 0.77778</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="hypothesis-tests-on-mean-vectors.html#cb53-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;F statistic: </span><span class="sc">{</span>f_stat<span class="sc">:.5f}</span><span class="ss"> (df1 = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">, df2 = </span><span class="sc">{</span>n<span class="op">-</span>p<span class="sc">}</span><span class="ss">)&#39;</span>)</span></code></pre></div>
<pre><code>## F statistic: 0.19444 (df1 = 2, df2 = 1)</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="hypothesis-tests-on-mean-vectors.html#cb55-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<pre><code>## p-value: 0.8485</code></pre>
</div>
</div>
<div id="evaluating-multivariate-normality" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Evaluating Multivariate Normality<a href="hypothesis-tests-on-mean-vectors.html#evaluating-multivariate-normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the assumptions for a one-sample <span class="math inline">\(t\)</span> test in the univariate case is that we either have a normal population or a large sample. A normal probability plot is used to check the normality assumption. If the distribution of the data is roughly normal, the points will roughly fall on a straight line. Deviations from a straight line indicate that the underlying distribution is not normal.</p>
<p>The rationale behind the normal probability plot is as follows. If the data follow a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, their <span class="math inline">\(z\)</span>-scores given by
<span class="math display">\[
z_i=\frac{y_i-\mu}{\sigma}=-\frac{\mu}{\sigma}+\frac{1}{\sigma}y_i
\]</span>
should follow a standard normal distribution. If we plot the normal scores <span class="math inline">\(z_i\)</span> versus the observed values <span class="math inline">\(y_i\)</span>, the data are roughly on a straight line with intercept <span class="math inline">\(-\frac{\mu}{\sigma}\)</span> and slope <span class="math inline">\(\frac{1}{\sigma}\)</span>. Therefore, if the data are from a normal population, plotting the normal scores (theoretical quantiles) obtained from Table III in the appendix of the recommended textbook (Weiss) versus the observations (observed quantiles) gives a straight line. The normal probability plot is also called the normal Q-Q plot; “Q” stands for “quantile”. The following example shows the steps to construct a normal probability plot.</p>
<p></p>
<p>Suppose the data are 75, 80, 90, 85, 75, and 40.</p>
<p>Here is the code for a normal Q-Q plot</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="hypothesis-tests-on-mean-vectors.html#cb57-1" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> stats</span>
<span id="cb57-2"><a href="hypothesis-tests-on-mean-vectors.html#cb57-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb57-3"><a href="hypothesis-tests-on-mean-vectors.html#cb57-3" tabindex="-1"></a></span>
<span id="cb57-4"><a href="hypothesis-tests-on-mean-vectors.html#cb57-4" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">75</span>, <span class="dv">80</span>, <span class="dv">90</span>, <span class="dv">85</span>, <span class="dv">75</span>, <span class="dv">40</span>])</span>
<span id="cb57-5"><a href="hypothesis-tests-on-mean-vectors.html#cb57-5" tabindex="-1"></a></span>
<span id="cb57-6"><a href="hypothesis-tests-on-mean-vectors.html#cb57-6" tabindex="-1"></a>stats.probplot(x, dist<span class="op">=</span><span class="st">&quot;norm&quot;</span>, plot<span class="op">=</span>plt)</span></code></pre></div>
<pre><code>## ((array([-1.23132171, -0.63003387, -0.19819716,  0.19819716,  0.63003387,
##         1.23132171]), array([40, 75, 75, 80, 85, 90])), (np.float64(17.634241917611398), np.float64(74.16666666666667), np.float64(0.8792020358303791)))</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="hypothesis-tests-on-mean-vectors.html#cb59-1" tabindex="-1"></a>plt.title(<span class="st">&quot;Normal QQ Plot of Grade&quot;</span>)</span>
<span id="cb59-2"><a href="hypothesis-tests-on-mean-vectors.html#cb59-2" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="Plots/qqplotgrade-1.png" width="70%" /></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="hypothesis-tests-on-mean-vectors.html#cb60-1" tabindex="-1"></a>plt.close()</span></code></pre></div>
<p>The six points do not fall in a straight line; the data do not seem to come from a normal distribution. The point on the lower left corner might be an outlier. If we remove this potential outlier, the other five points roughly fall on a straight line.</p>
<pre><code>## (np.float64(-0.5), np.float64(767.5), np.float64(811.5), np.float64(-0.5))</code></pre>
<p><img src="Plots/tblIII-3.png" width="75%" style="display: block; margin: auto;" /></p>
<p>The following section explains how the normal scores given in Table III were calculated.</p>
<ul>
<li>Sort the observations <span class="math inline">\(x_1, x_2 \cdots, x_n\)</span> from the smallest to the largest, we have <span class="math inline">\(x_{(1)}, x_{(2)}, \cdots, x_{(n)}\)</span>, where <span class="math inline">\(x_{(1)}\)</span> denotes the smallest observation, <span class="math inline">\(x_{(2)}\)</span> the second smallest and <span class="math inline">\(x_{(n)}\)</span> the largest value.</li>
<li>Let
<span class="math display">\[
a_i=\frac{i-\frac{3}{8}}{n+\frac{1}{4}}, i=1, 2, \ldots, n.
\]</span>
As we can see, <span class="math inline">\(a_i\)</span> is roughly the percentage of values that lie below the <span class="math inline">\(i\)</span>th smallest, the fractions <span class="math inline">\(\frac{3}{8}\)</span> and <span class="math inline">\(\frac{1}{4}\)</span> is for continuity correction.</li>
<li>Using Table II to find the normal scores <span class="math inline">\(z_1, z_2, \cdots, z_n\)</span>, such that
<span class="math display">\[
P(Z\le z_i)=\mbox{Area}(Z\le z_i)=a_i, i=1, 2, \cdots, n.
\]</span></li>
</ul>
<p>Use the previous example with six grades, the normal scores for <span class="math inline">\(n=6\)</span> are:</p>
<span class="math display">\[\begin{array}{c|c|c|c}
\hline
\text{Grade} (x_{(i)}) &amp;i&amp;a_i=\frac{i-\frac{3}{8}}{n+\frac{1}{4}}&amp;\text{Normal score} (z_i)\\
\hline
40&amp;1&amp;(1-3/8)/(6+1/4)=0.10&amp;-1.28\\
75&amp;2&amp;(2-3/8)/(6+1/4)=0.26&amp;-0.64\\
75&amp;3&amp;(3-3/8)/(6+1/4)=0.42&amp;-0.20\\
80&amp;4&amp;(4-3/8)/(6+1/4)=0.58&amp;0.20\\
85&amp;5&amp;(5-3/8)/(6+1/4)=0.74&amp;0.64\\
90&amp;6&amp;(6-3/8)/(6+1/4)=0.90&amp;1.28\\
\hline
\end{array}\]</span>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="hypothesis-tests-on-mean-vectors.html#cb62-1" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb62-2"><a href="hypothesis-tests-on-mean-vectors.html#cb62-2" tabindex="-1"></a></span>
<span id="cb62-3"><a href="hypothesis-tests-on-mean-vectors.html#cb62-3" tabindex="-1"></a><span class="co"># First example: n = 6</span></span>
<span id="cb62-4"><a href="hypothesis-tests-on-mean-vectors.html#cb62-4" tabindex="-1"></a>i <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">7</span>)</span>
<span id="cb62-5"><a href="hypothesis-tests-on-mean-vectors.html#cb62-5" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(i)</span>
<span id="cb62-6"><a href="hypothesis-tests-on-mean-vectors.html#cb62-6" tabindex="-1"></a>a <span class="op">=</span> (i <span class="op">-</span> <span class="dv">3</span><span class="op">/</span><span class="dv">8</span>) <span class="op">/</span> (n <span class="op">+</span> <span class="fl">0.25</span>)</span>
<span id="cb62-7"><a href="hypothesis-tests-on-mean-vectors.html#cb62-7" tabindex="-1"></a>z <span class="op">=</span> norm.ppf(a)</span>
<span id="cb62-8"><a href="hypothesis-tests-on-mean-vectors.html#cb62-8" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(z, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [-1.28 -0.64 -0.2   0.2   0.64  1.28]</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="hypothesis-tests-on-mean-vectors.html#cb64-1" tabindex="-1"></a><span class="co"># Second example: n = 10</span></span>
<span id="cb64-2"><a href="hypothesis-tests-on-mean-vectors.html#cb64-2" tabindex="-1"></a>i2 <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">11</span>)</span>
<span id="cb64-3"><a href="hypothesis-tests-on-mean-vectors.html#cb64-3" tabindex="-1"></a>n2 <span class="op">=</span> <span class="bu">len</span>(i2)</span>
<span id="cb64-4"><a href="hypothesis-tests-on-mean-vectors.html#cb64-4" tabindex="-1"></a>a2 <span class="op">=</span> (i2 <span class="op">-</span> <span class="dv">3</span><span class="op">/</span><span class="dv">8</span>) <span class="op">/</span> (n2 <span class="op">+</span> <span class="fl">0.25</span>)</span>
<span id="cb64-5"><a href="hypothesis-tests-on-mean-vectors.html#cb64-5" tabindex="-1"></a>z2 <span class="op">=</span> norm.ppf(a2)</span>
<span id="cb64-6"><a href="hypothesis-tests-on-mean-vectors.html#cb64-6" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(z2, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [-1.55 -1.   -0.66 -0.38 -0.12  0.12  0.38  0.66  1.    1.55]</code></pre>
<p><span class="math inline">\(\textbf{Note: }\)</span></p>
<ol style="list-style-type: decimal">
<li>the normal probability plots in the Stat 151 textbook are generated by Minitab, which plots the normal scores versus the sorted observations. It does not matter whether we plot the observed data in the <span class="math inline">\(x\)</span>-axis or in the <span class="math inline">\(y\)</span>-axis. The points will fall on a straight line if the data are from a normal population.</li>
<li>Different formulas can be used to calculate <span class="math inline">\(a_i\)</span>. Another more popular choice is <span class="math inline">\(a_i=\frac{i-0.5}{n}\)</span>, which is used in <span class="math inline">\(\textsf{R.}\)</span></li>
</ol>
<p>A similar idea can be applied to the multivariate case. Suppose <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_n\)</span> is a simple random sample of size <span class="math inline">\(n\)</span> from a multivariate normal distribution <span class="math inline">\(N_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>, then <span class="math inline">\((\mathbf{x}_i-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}_i-\boldsymbol{\mu})\sim \chi_p^2\)</span>. If the observations follow a multivariate normal distribution <span class="math inline">\(N_p(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>, <span class="math inline">\((\mathbf{x}_i-\mathbf{\bar x})^T\mathbf{S}^{-1}(\mathbf{x}_i-\mathbf{\bar x})\)</span> should also roughly follows a <span class="math inline">\(\chi_p^2\)</span> distribution. Therefore, we could use the theoretical quantiles from a <span class="math inline">\(\chi_p^2\)</span> distribution to test multivariate normality. The steps are as follows:</p>
<ol style="list-style-type: decimal">
<li>Calculate the sample mean vector <span class="math inline">\(\mathbf{\bar x}\)</span> and the sample covariance matrix <span class="math inline">\(\mathbf{S}\)</span>.</li>
<li>Calculate the Mahalanobis distance from each observation <span class="math inline">\(\mathbf{x}_i\)</span> to the sample mean <span class="math inline">\(\mathbf{\bar x}\)</span>, denoted as <span class="math inline">\(d_i^2\)</span>.</li>
<li>Sort the distances <span class="math inline">\(d_i^2\)</span> from the smallest to the largest, denoted as <span class="math inline">\(d_{(1)}^2, d_{(2)}^2, \cdots, d_{(n)}^2\)</span>. These are the observed quantiles. Obtain the theoretical quantiles from a <span class="math inline">\(\chi_p^2\)</span> distribution, denoted as <span class="math inline">\(q_{(i)}=\chi_p^2(1-\frac{i-0.5}{n})\)</span>, i.e., the <span class="math inline">\(\frac{i-0.5}{n}\times 100\)</span>th percentile.</li>
<li>Draw a scatter plot of the observed quantiles <span class="math inline">\(d_{(i)}^2\)</span> versus the theoretical quantiles <span class="math inline">\(q_{(i)}\)</span>. If the data are from a multivariate normal distribution, the data points should roughly fall on a 45-degree straight line.</li>
</ol>
<p>Figure <a href="hypothesis-tests-on-mean-vectors.html#fig:chiqqplot">4.1</a> is the chi-square probability plot on the 50 iris (setosa) flowers and the table to its right compares the Mahanalobis distance (observed quantiles) and the theoretical quantiles obtained from a <span class="math inline">\(\chi_4^2\)</span> distribution. The sample mean vector is <span class="math inline">\(\mathbf{\bar x}=[5.006, 3.428, 1.462, 0.246]^T\)</span>. Except for the last ten observations, most of the points are roughly on the 45-degree line. There is no strong evidence against the null hypothesis: data follow a chi-square distribution.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="hypothesis-tests-on-mean-vectors.html#cb66-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb66-2"><a href="hypothesis-tests-on-mean-vectors.html#cb66-2" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2</span>
<span id="cb66-3"><a href="hypothesis-tests-on-mean-vectors.html#cb66-3" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> mahalanobis</span>
<span id="cb66-4"><a href="hypothesis-tests-on-mean-vectors.html#cb66-4" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb66-5"><a href="hypothesis-tests-on-mean-vectors.html#cb66-5" tabindex="-1"></a></span>
<span id="cb66-6"><a href="hypothesis-tests-on-mean-vectors.html#cb66-6" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb66-7"><a href="hypothesis-tests-on-mean-vectors.html#cb66-7" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(iris.data, columns<span class="op">=</span>iris.feature_names)</span>
<span id="cb66-8"><a href="hypothesis-tests-on-mean-vectors.html#cb66-8" tabindex="-1"></a></span>
<span id="cb66-9"><a href="hypothesis-tests-on-mean-vectors.html#cb66-9" tabindex="-1"></a><span class="co"># subsetting setosa</span></span>
<span id="cb66-10"><a href="hypothesis-tests-on-mean-vectors.html#cb66-10" tabindex="-1"></a>x <span class="op">=</span> df.iloc[:<span class="dv">50</span>, :].values</span>
<span id="cb66-11"><a href="hypothesis-tests-on-mean-vectors.html#cb66-11" tabindex="-1"></a>n, p <span class="op">=</span> x.shape</span>
<span id="cb66-12"><a href="hypothesis-tests-on-mean-vectors.html#cb66-12" tabindex="-1"></a></span>
<span id="cb66-13"><a href="hypothesis-tests-on-mean-vectors.html#cb66-13" tabindex="-1"></a>mean_vec <span class="op">=</span> np.mean(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb66-14"><a href="hypothesis-tests-on-mean-vectors.html#cb66-14" tabindex="-1"></a>cov_mat <span class="op">=</span> np.cov(x, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb66-15"><a href="hypothesis-tests-on-mean-vectors.html#cb66-15" tabindex="-1"></a>cov_inv <span class="op">=</span> np.linalg.inv(cov_mat)</span>
<span id="cb66-16"><a href="hypothesis-tests-on-mean-vectors.html#cb66-16" tabindex="-1"></a></span>
<span id="cb66-17"><a href="hypothesis-tests-on-mean-vectors.html#cb66-17" tabindex="-1"></a><span class="co"># mahalanobis distances</span></span>
<span id="cb66-18"><a href="hypothesis-tests-on-mean-vectors.html#cb66-18" tabindex="-1"></a>D2 <span class="op">=</span> np.array([mahalanobis(obs, mean_vec, cov_inv)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> obs <span class="kw">in</span> x])</span>
<span id="cb66-19"><a href="hypothesis-tests-on-mean-vectors.html#cb66-19" tabindex="-1"></a>D2_sorted <span class="op">=</span> np.sort(D2)</span>
<span id="cb66-20"><a href="hypothesis-tests-on-mean-vectors.html#cb66-20" tabindex="-1"></a>q_vec <span class="op">=</span> (np.arange(<span class="dv">1</span>, n<span class="op">+</span><span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span>) <span class="op">/</span> n</span>
<span id="cb66-21"><a href="hypothesis-tests-on-mean-vectors.html#cb66-21" tabindex="-1"></a>q_theo <span class="op">=</span> chi2.ppf(q_vec, df<span class="op">=</span>p)</span>
<span id="cb66-22"><a href="hypothesis-tests-on-mean-vectors.html#cb66-22" tabindex="-1"></a></span>
<span id="cb66-23"><a href="hypothesis-tests-on-mean-vectors.html#cb66-23" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb66-24"><a href="hypothesis-tests-on-mean-vectors.html#cb66-24" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb66-25"><a href="hypothesis-tests-on-mean-vectors.html#cb66-25" tabindex="-1"></a>plt.plot(q_theo, D2_sorted, <span class="st">&#39;ko&#39;</span>, markersize<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb66-26"><a href="hypothesis-tests-on-mean-vectors.html#cb66-26" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="bu">max</span>(q_theo)], [<span class="dv">0</span>, <span class="bu">max</span>(q_theo)], <span class="st">&#39;r-&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb66-27"><a href="hypothesis-tests-on-mean-vectors.html#cb66-27" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Theoretical Quantile&quot;</span>)</span>
<span id="cb66-28"><a href="hypothesis-tests-on-mean-vectors.html#cb66-28" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Observed Quantile&quot;</span>)</span>
<span id="cb66-29"><a href="hypothesis-tests-on-mean-vectors.html#cb66-29" tabindex="-1"></a>plt.title(<span class="st">&quot;Chi-square Q-Q Plot&quot;</span>)</span>
<span id="cb66-30"><a href="hypothesis-tests-on-mean-vectors.html#cb66-30" tabindex="-1"></a>plt.grid(<span class="va">True</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb66-31"><a href="hypothesis-tests-on-mean-vectors.html#cb66-31" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb66-32"><a href="hypothesis-tests-on-mean-vectors.html#cb66-32" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chiqqplot"></span>
<img src="Plots/chiqqplot-5.png" alt="Chi-squre Q-Q Plot for Setosa" width="60%" />
<p class="caption">
Figure 4.1: Chi-squre Q-Q Plot for Setosa
</p>
</div>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="hypothesis-tests-on-mean-vectors.html#cb67-1" tabindex="-1"></a>plt.close()</span></code></pre></div>
<p>We can apply the Kolmogorov-Smirnov (KS) test to check whether the Mahanalobis distances follow a chi-square distribution with <span class="math inline">\(df=4\)</span>. The Kolmogorov-Smirnov is a non-parametric statistical test used to determine if a sample of data follows a specified probability distribution or if two samples come from the same underlying distribution. The main idea behind the KS test is to compare the empirical cumulative distribution function (ECDF) of the sample data to the cumulative distribution function (CDF) of the target distribution. Since the p-value of the test is 0.4337, there is no strong evidence against the null hypothesis: data follow a chi-square distribution.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="hypothesis-tests-on-mean-vectors.html#cb68-1" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> kstest</span>
<span id="cb68-2"><a href="hypothesis-tests-on-mean-vectors.html#cb68-2" tabindex="-1"></a></span>
<span id="cb68-3"><a href="hypothesis-tests-on-mean-vectors.html#cb68-3" tabindex="-1"></a>ks_stat, p_value <span class="op">=</span> kstest(D2, cdf<span class="op">=</span><span class="st">&#39;chi2&#39;</span>, args<span class="op">=</span>(<span class="dv">4</span>,))</span>
<span id="cb68-4"><a href="hypothesis-tests-on-mean-vectors.html#cb68-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;KS statistic (D): </span><span class="sc">{</span>ks_stat<span class="sc">:.5f}</span><span class="ss">, p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## KS statistic (D): 0.12001, p-value: 0.4337</code></pre>
<p>The following table compares the observed and theoretical quantiles.</p>
<span class="math display">\[\begin{array}{c|c|c|c|c|c}
\hline
\text{Sepal}    &amp;   \text{Sepal} &amp;  \text{Petal}&amp;   \text{Petal}&amp;   \text{Observed} &amp;   \text{Theoretical}\\
\text{Length}&amp;\text{Width}&amp;\text{Length}&amp;\text{Width}&amp;\text{Quantiles} d_{(i)}^2 &amp;\text{Quantiles} q_{(i)}\\
\hline
5   &amp;   3.4 &amp;   1.5 &amp;   0.2 &amp;   0.343   &amp;   0.297   \\
5.1 &amp;   3.5 &amp;   1.4 &amp;   0.2 &amp;   0.449   &amp;   0.535   \\
5   &amp;   3.3 &amp;   1.4 &amp;   0.2 &amp;   0.495   &amp;   0.711   \\
5.1 &amp;   3.4 &amp;   1.5 &amp;   0.2 &amp;   0.589   &amp;   0.862   \\
5.1 &amp;   3.5 &amp;   1.4 &amp;   0.3 &amp;   0.636   &amp;   0.999   \\
5   &amp;   3.6 &amp;   1.4 &amp;   0.2 &amp;   0.762   &amp;   1.127   \\
5.2 &amp;   3.5 &amp;   1.5 &amp;   0.2 &amp;   0.829   &amp;   1.249   \\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp;\vdots\\
5.2 &amp;   4.1 &amp;   1.5 &amp;   0.1 &amp;   7.699   &amp;   7.114   \\
5.1 &amp;   3.8 &amp;   1.9 &amp;   0.4 &amp;   8.601   &amp;   7.539   \\
4.8 &amp;   3.4 &amp;   1.9 &amp;   0.2 &amp;   9.748   &amp;   8.043   \\
5.8 &amp;   4   &amp;   1.2 &amp;   0.2 &amp;   10.222  &amp;   8.666   \\
4.6 &amp;   3.6 &amp;   1   &amp;   0.2 &amp;   11.044  &amp;   9.488   \\
5   &amp;   3.5 &amp;   1.6 &amp;   0.6 &amp;   12.310  &amp;   10.712  \\
4.5 &amp;   2.3 &amp;   1.3 &amp;   0.3 &amp;   12.328  &amp;   13.277  \\
\hline
\end{array}\]</span>
<p>We can also employ the following diagnostic procedures to assess the multivariate normal assumption in a more casual way.</p>
<ul>
<li>Produce a normal probability plot (Q-Q plot ) for each variable. What we should look for is whether the points are roughly on a straight line.</li>
<li>Produce scatter plots for each pair of variables. Under multivariate normality, we should see an elliptical cloud of points.</li>
<li>Produce a three-dimensional rotating scatter plot. Again, we should see an elliptical cloud of points.</li>
</ul>
<p>Figure <a href="hypothesis-tests-on-mean-vectors.html#fig:qqplotSetosa">4.2</a> shows a normal Q-Q plot for each of the four variables of the Setosa Iris data. Except for Petal Width which has only six distinct values, all other three variables roughly follow normal distributions.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="hypothesis-tests-on-mean-vectors.html#cb70-1" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb70-2"><a href="hypothesis-tests-on-mean-vectors.html#cb70-2" tabindex="-1"></a></span>
<span id="cb70-3"><a href="hypothesis-tests-on-mean-vectors.html#cb70-3" tabindex="-1"></a>iris_df <span class="op">=</span> pd.DataFrame(iris.data, columns<span class="op">=</span>iris.feature_names)</span>
<span id="cb70-4"><a href="hypothesis-tests-on-mean-vectors.html#cb70-4" tabindex="-1"></a>iris_df[<span class="st">&#39;species&#39;</span>] <span class="op">=</span> iris.target</span>
<span id="cb70-5"><a href="hypothesis-tests-on-mean-vectors.html#cb70-5" tabindex="-1"></a></span>
<span id="cb70-6"><a href="hypothesis-tests-on-mean-vectors.html#cb70-6" tabindex="-1"></a><span class="co"># Filter for Setosa (species = 0)</span></span>
<span id="cb70-7"><a href="hypothesis-tests-on-mean-vectors.html#cb70-7" tabindex="-1"></a>setosa_df <span class="op">=</span> iris_df[iris_df[<span class="st">&#39;species&#39;</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb70-8"><a href="hypothesis-tests-on-mean-vectors.html#cb70-8" tabindex="-1"></a></span>
<span id="cb70-9"><a href="hypothesis-tests-on-mean-vectors.html#cb70-9" tabindex="-1"></a><span class="co"># Plot Q-Q plots for each variable</span></span>
<span id="cb70-10"><a href="hypothesis-tests-on-mean-vectors.html#cb70-10" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb70-11"><a href="hypothesis-tests-on-mean-vectors.html#cb70-11" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb70-12"><a href="hypothesis-tests-on-mean-vectors.html#cb70-12" tabindex="-1"></a>columns <span class="op">=</span> setosa_df.columns[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb70-13"><a href="hypothesis-tests-on-mean-vectors.html#cb70-13" tabindex="-1"></a></span>
<span id="cb70-14"><a href="hypothesis-tests-on-mean-vectors.html#cb70-14" tabindex="-1"></a><span class="cf">for</span> i, col <span class="kw">in</span> <span class="bu">enumerate</span>(columns):</span>
<span id="cb70-15"><a href="hypothesis-tests-on-mean-vectors.html#cb70-15" tabindex="-1"></a>    stats.probplot(setosa_df[col], dist<span class="op">=</span><span class="st">&quot;norm&quot;</span>, plot<span class="op">=</span>axes[i])</span>
<span id="cb70-16"><a href="hypothesis-tests-on-mean-vectors.html#cb70-16" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f&quot;QQ Plot for </span><span class="sc">{</span>col<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb70-17"><a href="hypothesis-tests-on-mean-vectors.html#cb70-17" tabindex="-1"></a></span>
<span id="cb70-18"><a href="hypothesis-tests-on-mean-vectors.html#cb70-18" tabindex="-1"></a>plt.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.6</span>, wspace<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># spacing</span></span>
<span id="cb70-19"><a href="hypothesis-tests-on-mean-vectors.html#cb70-19" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qqplotSetosa"></span>
<img src="Plots/qqplotSetosa-7.png" alt="Normal Q-Q Plot for Setosa" width="90%" />
<p class="caption">
Figure 4.2: Normal Q-Q Plot for Setosa
</p>
</div>
<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="hypothesis-tests-on-mean-vectors.html#cb71-1" tabindex="-1"></a>plt.close()</span></code></pre></div>
<p>Figure <a href="hypothesis-tests-on-mean-vectors.html#fig:scatterSetosa">4.3</a> is the scatter plots matrix of those 50 Setosa iris flowers. Sepal Length and Sepal Width form an elliptical cloud. Sepal Length and Petal Length, Sepal Width and Petal Length form a roughly elliptical cloud. Due to the sparse values of Petal Width, it does not form elliptical cloud of points with any other variables.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="hypothesis-tests-on-mean-vectors.html#cb72-1" tabindex="-1"></a>setosa_df.columns <span class="op">=</span> [<span class="st">&#39;Sepal Length&#39;</span>, <span class="st">&#39;Sepal Width&#39;</span>, <span class="st">&#39;Petal Length&#39;</span>, <span class="st">&#39;Petal Width&#39;</span>, <span class="st">&#39;Species&#39;</span>]</span>
<span id="cb72-2"><a href="hypothesis-tests-on-mean-vectors.html#cb72-2" tabindex="-1"></a>setosa_plot_df <span class="op">=</span> setosa_df.drop(columns<span class="op">=</span><span class="st">&#39;Species&#39;</span>)</span>
<span id="cb72-3"><a href="hypothesis-tests-on-mean-vectors.html#cb72-3" tabindex="-1"></a>sns.pairplot(setosa_plot_df, diag_kind<span class="op">=</span><span class="st">&quot;hist&quot;</span>, corner<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterSetosa"></span>
<img src="Plots/scatterSetosa-9.png" alt="Scatter Plot Matrix for Setosa" width="90%" />
<p class="caption">
Figure 4.3: Scatter Plot Matrix for Setosa
</p>
</div>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="hypothesis-tests-on-mean-vectors.html#cb73-1" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterSetosa"></span>
<img src="Plots/scatterSetosa-10.png" alt="Scatter Plot Matrix for Setosa" width="90%" />
<p class="caption">
Figure 4.3: Scatter Plot Matrix for Setosa
</p>
</div>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="hypothesis-tests-on-mean-vectors.html#cb74-1" tabindex="-1"></a>plt.close()</span></code></pre></div>
<p><span class="math inline">\(\textbf{Example: Inference on One Mean Vector with Admission Data}\)</span></p>
<p>The admission data set contains 397 graduate school admissions decisions, among which <span class="math inline">\(n_1=271\)</span> were not admitted and <span class="math inline">\(n_2=126\)</span> were admitted. The explanatory variables are the student’s GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution (values from 1 to 4 with 1 having the highest prestige). The response variable, admit/don’t admit, is a binary variable with 1=admitted and 0=not admitted.</p>
<p>Let’s focus on the admitted students with the first two quantitative explanatory variables, GRE and GPA. This creates a one-sample scenario. Given that the sample mean vector <span class="math inline">\(\mathbf{\bar x}=[618.571,  3.489]^T\)</span> and the sample covariance matrix and its inverse
<span class="math display">\[
\mathbf{S}=\left[
\begin{array}{cc}
11937.143&amp; 9.452\\
9.452&amp; 0.138
\end{array}
\right]; \quad \quad
\mathbf{S}^{-1}=\left[
\begin{array}{cc}
8.857592\times 10^{-5} &amp; -0.006066809\\
-0.006066809 &amp; 7.661909264
\end{array}
\right]
\]</span></p>
<p>We could check whether the two variables GRE and GPA are bivariate normally distributed based on the following four graphs.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="hypothesis-tests-on-mean-vectors.html#cb75-1" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> probplot</span>
<span id="cb75-2"><a href="hypothesis-tests-on-mean-vectors.html#cb75-2" tabindex="-1"></a></span>
<span id="cb75-3"><a href="hypothesis-tests-on-mean-vectors.html#cb75-3" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;data/admission.csv&quot;</span>)</span>
<span id="cb75-4"><a href="hypothesis-tests-on-mean-vectors.html#cb75-4" tabindex="-1"></a>df <span class="op">=</span> df.dropna()</span>
<span id="cb75-5"><a href="hypothesis-tests-on-mean-vectors.html#cb75-5" tabindex="-1"></a>xx <span class="op">=</span> df[[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]].to_numpy()</span>
<span id="cb75-6"><a href="hypothesis-tests-on-mean-vectors.html#cb75-6" tabindex="-1"></a>n, p <span class="op">=</span> xx.shape</span>
<span id="cb75-7"><a href="hypothesis-tests-on-mean-vectors.html#cb75-7" tabindex="-1"></a></span>
<span id="cb75-8"><a href="hypothesis-tests-on-mean-vectors.html#cb75-8" tabindex="-1"></a>mean_vec <span class="op">=</span> np.mean(xx, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb75-9"><a href="hypothesis-tests-on-mean-vectors.html#cb75-9" tabindex="-1"></a>cov_mat <span class="op">=</span> np.cov(xx.T)</span>
<span id="cb75-10"><a href="hypothesis-tests-on-mean-vectors.html#cb75-10" tabindex="-1"></a>inv_covmat <span class="op">=</span> np.linalg.inv(cov_mat)</span>
<span id="cb75-11"><a href="hypothesis-tests-on-mean-vectors.html#cb75-11" tabindex="-1"></a>D2 <span class="op">=</span> np.array([mahalanobis(x, mean_vec, inv_covmat)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> x <span class="kw">in</span> xx])</span>
<span id="cb75-12"><a href="hypothesis-tests-on-mean-vectors.html#cb75-12" tabindex="-1"></a></span>
<span id="cb75-13"><a href="hypothesis-tests-on-mean-vectors.html#cb75-13" tabindex="-1"></a><span class="co"># theoretical chi-square quantiles</span></span>
<span id="cb75-14"><a href="hypothesis-tests-on-mean-vectors.html#cb75-14" tabindex="-1"></a>qvec <span class="op">=</span> ((np.arange(<span class="dv">1</span>, n <span class="op">+</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span>) <span class="op">/</span> n)</span>
<span id="cb75-15"><a href="hypothesis-tests-on-mean-vectors.html#cb75-15" tabindex="-1"></a>q_theo <span class="op">=</span> chi2.ppf(qvec, df<span class="op">=</span>p)</span>
<span id="cb75-16"><a href="hypothesis-tests-on-mean-vectors.html#cb75-16" tabindex="-1"></a>q_obs <span class="op">=</span> np.sort(D2)</span>
<span id="cb75-17"><a href="hypothesis-tests-on-mean-vectors.html#cb75-17" tabindex="-1"></a></span>
<span id="cb75-18"><a href="hypothesis-tests-on-mean-vectors.html#cb75-18" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb75-19"><a href="hypothesis-tests-on-mean-vectors.html#cb75-19" tabindex="-1"></a></span>
<span id="cb75-20"><a href="hypothesis-tests-on-mean-vectors.html#cb75-20" tabindex="-1"></a><span class="co"># Scatter plot of GRE vs GPA</span></span>
<span id="cb75-21"><a href="hypothesis-tests-on-mean-vectors.html#cb75-21" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].scatter(df[<span class="st">&#39;gre&#39;</span>], df[<span class="st">&#39;gpa&#39;</span>], c<span class="op">=</span><span class="st">&#39;black&#39;</span>)</span>
<span id="cb75-22"><a href="hypothesis-tests-on-mean-vectors.html#cb75-22" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">&quot;Scatter Plot of GPA vs GRE&quot;</span>)</span>
<span id="cb75-23"><a href="hypothesis-tests-on-mean-vectors.html#cb75-23" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_xlabel(<span class="st">&quot;GRE&quot;</span>)</span>
<span id="cb75-24"><a href="hypothesis-tests-on-mean-vectors.html#cb75-24" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">&quot;GPA&quot;</span>)</span>
<span id="cb75-25"><a href="hypothesis-tests-on-mean-vectors.html#cb75-25" tabindex="-1"></a></span>
<span id="cb75-26"><a href="hypothesis-tests-on-mean-vectors.html#cb75-26" tabindex="-1"></a><span class="co"># QQ plot for GRE</span></span>
<span id="cb75-27"><a href="hypothesis-tests-on-mean-vectors.html#cb75-27" tabindex="-1"></a>probplot(df[<span class="st">&#39;gre&#39;</span>], dist<span class="op">=</span><span class="st">&quot;norm&quot;</span>, plot<span class="op">=</span>axes[<span class="dv">0</span>, <span class="dv">1</span>])</span></code></pre></div>
<pre><code>## ((array([-2.9210196 , -2.63280541, -2.47026126, -2.35476152, -2.26410345,
##        -2.18894403, -2.12443167, -2.06771086, -2.0169534 , -1.97091559,
##        -1.9287115 , -1.88968692, -1.85334448, -1.81929684, -1.78723626,
##        -1.75691401, -1.72812608, -1.70070296, -1.67450224, -1.64940305,
##        -1.6253019 , -1.60210948, -1.57974813, -1.55814991, -1.53725502,
##        -1.51701052, -1.49736934, -1.47828938, -1.45973291, -1.44166589,
##        -1.42405757, -1.40688004, -1.39010789, -1.37371791, -1.35768887,
##        -1.34200126, -1.32663715, -1.31157996, -1.2968144 , -1.28232629,
##        -1.26810245, -1.25413065, -1.24039947, -1.22689826, -1.21361706,
##        -1.20054655, -1.18767798, -1.17500315, -1.16251432, -1.15020422,
##        -1.138066  , -1.12609318, -1.11427965, -1.10261961, -1.09110758,
##        -1.07973837, -1.06850703, -1.05740889, -1.04643948, -1.03559457,
##        -1.0248701 , -1.01426223, -1.00376729, -0.99338175, -0.98310227,
##        -0.97292563, -0.96284878, -0.95286875, -0.94298274, -0.93318804,
##        -0.92348206, -0.91386231, -0.90432639, -0.89487201, -0.88549694,
##        -0.87619907, -0.86697633, -0.85782675, -0.84874843, -0.83973953,
##        -0.83079827, -0.82192294, -0.81311188, -0.80436351, -0.79567627,
##        -0.78704866, -0.77847925, -0.76996662, -0.76150943, -0.75310636,
##        -0.74475613, -0.73645751, -0.7282093 , -0.72001034, -0.7118595 ,
##        -0.70375568, -0.69569781, -0.68768487, -0.67971583, -0.67178974,
##        -0.66390562, -0.65606255, -0.64825964, -0.640496  , -0.63277078,
##        -0.62508313, -0.61743225, -0.60981734, -0.60223763, -0.59469237,
##        -0.58718081, -0.57970224, -0.57225594, -0.56484125, -0.55745748,
##        -0.55010398, -0.5427801 , -0.53548523, -0.52821874, -0.52098003,
##        -0.51376852, -0.50658363, -0.4994248 , -0.49229147, -0.48518311,
##        -0.47809918, -0.47103915, -0.46400253, -0.45698881, -0.4499975 ,
##        -0.44302812, -0.43608018, -0.42915324, -0.42224683, -0.4153605 ,
##        -0.40849381, -0.40164632, -0.39481762, -0.38800728, -0.38121489,
##        -0.37444004, -0.36768234, -0.36094138, -0.35421679, -0.34750817,
##        -0.34081516, -0.33413738, -0.32747447, -0.32082607, -0.31419181,
##        -0.30757136, -0.30096436, -0.29437047, -0.28778936, -0.28122069,
##        -0.27466412, -0.26811935, -0.26158604, -0.25506388, -0.24855255,
##        -0.24205174, -0.23556114, -0.22908045, -0.22260937, -0.2161476 ,
##        -0.20969484, -0.2032508 , -0.19681519, -0.19038772, -0.1839681 ,
##        -0.17755606, -0.17115131, -0.16475358, -0.15836258, -0.15197804,
##        -0.14559969, -0.13922726, -0.13286048, -0.12649907, -0.12014279,
##        -0.11379135, -0.1074445 , -0.10110198, -0.09476352, -0.08842887,
##        -0.08209776, -0.07576994, -0.06944516, -0.06312315, -0.05680366,
##        -0.05048644, -0.04417124, -0.03785779, -0.03154586, -0.02523518,
##        -0.01892551, -0.01261659, -0.00630817,  0.        ,  0.00630817,
##         0.01261659,  0.01892551,  0.02523518,  0.03154586,  0.03785779,
##         0.04417124,  0.05048644,  0.05680366,  0.06312315,  0.06944516,
##         0.07576994,  0.08209776,  0.08842887,  0.09476352,  0.10110198,
##         0.1074445 ,  0.11379135,  0.12014279,  0.12649907,  0.13286048,
##         0.13922726,  0.14559969,  0.15197804,  0.15836258,  0.16475358,
##         0.17115131,  0.17755606,  0.1839681 ,  0.19038772,  0.19681519,
##         0.2032508 ,  0.20969484,  0.2161476 ,  0.22260937,  0.22908045,
##         0.23556114,  0.24205174,  0.24855255,  0.25506388,  0.26158604,
##         0.26811935,  0.27466412,  0.28122069,  0.28778936,  0.29437047,
##         0.30096436,  0.30757136,  0.31419181,  0.32082607,  0.32747447,
##         0.33413738,  0.34081516,  0.34750817,  0.35421679,  0.36094138,
##         0.36768234,  0.37444004,  0.38121489,  0.38800728,  0.39481762,
##         0.40164632,  0.40849381,  0.4153605 ,  0.42224683,  0.42915324,
##         0.43608018,  0.44302812,  0.4499975 ,  0.45698881,  0.46400253,
##         0.47103915,  0.47809918,  0.48518311,  0.49229147,  0.4994248 ,
##         0.50658363,  0.51376852,  0.52098003,  0.52821874,  0.53548523,
##         0.5427801 ,  0.55010398,  0.55745748,  0.56484125,  0.57225594,
##         0.57970224,  0.58718081,  0.59469237,  0.60223763,  0.60981734,
##         0.61743225,  0.62508313,  0.63277078,  0.640496  ,  0.64825964,
##         0.65606255,  0.66390562,  0.67178974,  0.67971583,  0.68768487,
##         0.69569781,  0.70375568,  0.7118595 ,  0.72001034,  0.7282093 ,
##         0.73645751,  0.74475613,  0.75310636,  0.76150943,  0.76996662,
##         0.77847925,  0.78704866,  0.79567627,  0.80436351,  0.81311188,
##         0.82192294,  0.83079827,  0.83973953,  0.84874843,  0.85782675,
##         0.86697633,  0.87619907,  0.88549694,  0.89487201,  0.90432639,
##         0.91386231,  0.92348206,  0.93318804,  0.94298274,  0.95286875,
##         0.96284878,  0.97292563,  0.98310227,  0.99338175,  1.00376729,
##         1.01426223,  1.0248701 ,  1.03559457,  1.04643948,  1.05740889,
##         1.06850703,  1.07973837,  1.09110758,  1.10261961,  1.11427965,
##         1.12609318,  1.138066  ,  1.15020422,  1.16251432,  1.17500315,
##         1.18767798,  1.20054655,  1.21361706,  1.22689826,  1.24039947,
##         1.25413065,  1.26810245,  1.28232629,  1.2968144 ,  1.31157996,
##         1.32663715,  1.34200126,  1.35768887,  1.37371791,  1.39010789,
##         1.40688004,  1.42405757,  1.44166589,  1.45973291,  1.47828938,
##         1.49736934,  1.51701052,  1.53725502,  1.55814991,  1.57974813,
##         1.60210948,  1.6253019 ,  1.64940305,  1.67450224,  1.70070296,
##         1.72812608,  1.75691401,  1.78723626,  1.81929684,  1.85334448,
##         1.88968692,  1.9287115 ,  1.97091559,  2.0169534 ,  2.06771086,
##         2.12443167,  2.18894403,  2.26410345,  2.35476152,  2.47026126,
##         2.63280541,  2.9210196 ]), array([220., 300., 300., 300., 340., 340., 340., 340., 360., 360., 360.,
##        360., 380., 380., 380., 380., 380., 380., 380., 380., 400., 400.,
##        400., 400., 400., 400., 400., 400., 400., 400., 400., 420., 420.,
##        420., 420., 420., 420., 420., 440., 440., 440., 440., 440., 440.,
##        440., 440., 440., 440., 460., 460., 460., 460., 460., 460., 460.,
##        460., 460., 460., 460., 460., 460., 480., 480., 480., 480., 480.,
##        480., 480., 480., 480., 480., 480., 480., 480., 480., 480., 480.,
##        500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 500.,
##        500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 520.,
##        520., 520., 520., 520., 520., 520., 520., 520., 520., 520., 520.,
##        520., 520., 520., 520., 520., 520., 520., 520., 520., 520., 520.,
##        520., 540., 540., 540., 540., 540., 540., 540., 540., 540., 540.,
##        540., 540., 540., 540., 540., 540., 540., 540., 540., 540., 540.,
##        540., 540., 540., 540., 540., 540., 560., 560., 560., 560., 560.,
##        560., 560., 560., 560., 560., 560., 560., 560., 560., 560., 560.,
##        560., 560., 560., 560., 560., 560., 560., 560., 580., 580., 580.,
##        580., 580., 580., 580., 580., 580., 580., 580., 580., 580., 580.,
##        580., 580., 580., 580., 580., 580., 580., 580., 580., 580., 580.,
##        580., 580., 580., 600., 600., 600., 600., 600., 600., 600., 600.,
##        600., 600., 600., 600., 600., 600., 600., 600., 600., 600., 600.,
##        600., 600., 600., 600., 620., 620., 620., 620., 620., 620., 620.,
##        620., 620., 620., 620., 620., 620., 620., 620., 620., 620., 620.,
##        620., 620., 620., 620., 620., 620., 620., 620., 620., 620., 620.,
##        620., 640., 640., 640., 640., 640., 640., 640., 640., 640., 640.,
##        640., 640., 640., 640., 640., 640., 640., 640., 640., 640., 640.,
##        660., 660., 660., 660., 660., 660., 660., 660., 660., 660., 660.,
##        660., 660., 660., 660., 660., 660., 660., 660., 660., 660., 660.,
##        660., 680., 680., 680., 680., 680., 680., 680., 680., 680., 680.,
##        680., 680., 680., 680., 680., 680., 680., 680., 680., 680., 700.,
##        700., 700., 700., 700., 700., 700., 700., 700., 700., 700., 700.,
##        700., 700., 700., 700., 700., 700., 700., 700., 700., 700., 720.,
##        720., 720., 720., 720., 720., 720., 720., 720., 720., 720., 740.,
##        740., 740., 740., 740., 740., 740., 740., 740., 740., 740., 760.,
##        760., 760., 760., 760., 780., 780., 780., 780., 780., 800., 800.,
##        800., 800., 800., 800., 800., 800., 800., 800., 800., 800., 800.,
##        800., 800., 800., 800., 800., 800., 800., 800., 800., 800., 800.,
##        800.])), (np.float64(115.55472653820884), np.float64(587.8589420654912), np.float64(0.9936469932510308)))</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="hypothesis-tests-on-mean-vectors.html#cb77-1" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">&quot;Normal Q-Q Plot for GRE&quot;</span>)</span>
<span id="cb77-2"><a href="hypothesis-tests-on-mean-vectors.html#cb77-2" tabindex="-1"></a></span>
<span id="cb77-3"><a href="hypothesis-tests-on-mean-vectors.html#cb77-3" tabindex="-1"></a><span class="co"># QQ plot for GPA</span></span>
<span id="cb77-4"><a href="hypothesis-tests-on-mean-vectors.html#cb77-4" tabindex="-1"></a>probplot(df[<span class="st">&#39;gpa&#39;</span>], dist<span class="op">=</span><span class="st">&quot;norm&quot;</span>, plot<span class="op">=</span>axes[<span class="dv">1</span>, <span class="dv">0</span>])</span></code></pre></div>
<pre><code>## ((array([-2.9210196 , -2.63280541, -2.47026126, -2.35476152, -2.26410345,
##        -2.18894403, -2.12443167, -2.06771086, -2.0169534 , -1.97091559,
##        -1.9287115 , -1.88968692, -1.85334448, -1.81929684, -1.78723626,
##        -1.75691401, -1.72812608, -1.70070296, -1.67450224, -1.64940305,
##        -1.6253019 , -1.60210948, -1.57974813, -1.55814991, -1.53725502,
##        -1.51701052, -1.49736934, -1.47828938, -1.45973291, -1.44166589,
##        -1.42405757, -1.40688004, -1.39010789, -1.37371791, -1.35768887,
##        -1.34200126, -1.32663715, -1.31157996, -1.2968144 , -1.28232629,
##        -1.26810245, -1.25413065, -1.24039947, -1.22689826, -1.21361706,
##        -1.20054655, -1.18767798, -1.17500315, -1.16251432, -1.15020422,
##        -1.138066  , -1.12609318, -1.11427965, -1.10261961, -1.09110758,
##        -1.07973837, -1.06850703, -1.05740889, -1.04643948, -1.03559457,
##        -1.0248701 , -1.01426223, -1.00376729, -0.99338175, -0.98310227,
##        -0.97292563, -0.96284878, -0.95286875, -0.94298274, -0.93318804,
##        -0.92348206, -0.91386231, -0.90432639, -0.89487201, -0.88549694,
##        -0.87619907, -0.86697633, -0.85782675, -0.84874843, -0.83973953,
##        -0.83079827, -0.82192294, -0.81311188, -0.80436351, -0.79567627,
##        -0.78704866, -0.77847925, -0.76996662, -0.76150943, -0.75310636,
##        -0.74475613, -0.73645751, -0.7282093 , -0.72001034, -0.7118595 ,
##        -0.70375568, -0.69569781, -0.68768487, -0.67971583, -0.67178974,
##        -0.66390562, -0.65606255, -0.64825964, -0.640496  , -0.63277078,
##        -0.62508313, -0.61743225, -0.60981734, -0.60223763, -0.59469237,
##        -0.58718081, -0.57970224, -0.57225594, -0.56484125, -0.55745748,
##        -0.55010398, -0.5427801 , -0.53548523, -0.52821874, -0.52098003,
##        -0.51376852, -0.50658363, -0.4994248 , -0.49229147, -0.48518311,
##        -0.47809918, -0.47103915, -0.46400253, -0.45698881, -0.4499975 ,
##        -0.44302812, -0.43608018, -0.42915324, -0.42224683, -0.4153605 ,
##        -0.40849381, -0.40164632, -0.39481762, -0.38800728, -0.38121489,
##        -0.37444004, -0.36768234, -0.36094138, -0.35421679, -0.34750817,
##        -0.34081516, -0.33413738, -0.32747447, -0.32082607, -0.31419181,
##        -0.30757136, -0.30096436, -0.29437047, -0.28778936, -0.28122069,
##        -0.27466412, -0.26811935, -0.26158604, -0.25506388, -0.24855255,
##        -0.24205174, -0.23556114, -0.22908045, -0.22260937, -0.2161476 ,
##        -0.20969484, -0.2032508 , -0.19681519, -0.19038772, -0.1839681 ,
##        -0.17755606, -0.17115131, -0.16475358, -0.15836258, -0.15197804,
##        -0.14559969, -0.13922726, -0.13286048, -0.12649907, -0.12014279,
##        -0.11379135, -0.1074445 , -0.10110198, -0.09476352, -0.08842887,
##        -0.08209776, -0.07576994, -0.06944516, -0.06312315, -0.05680366,
##        -0.05048644, -0.04417124, -0.03785779, -0.03154586, -0.02523518,
##        -0.01892551, -0.01261659, -0.00630817,  0.        ,  0.00630817,
##         0.01261659,  0.01892551,  0.02523518,  0.03154586,  0.03785779,
##         0.04417124,  0.05048644,  0.05680366,  0.06312315,  0.06944516,
##         0.07576994,  0.08209776,  0.08842887,  0.09476352,  0.10110198,
##         0.1074445 ,  0.11379135,  0.12014279,  0.12649907,  0.13286048,
##         0.13922726,  0.14559969,  0.15197804,  0.15836258,  0.16475358,
##         0.17115131,  0.17755606,  0.1839681 ,  0.19038772,  0.19681519,
##         0.2032508 ,  0.20969484,  0.2161476 ,  0.22260937,  0.22908045,
##         0.23556114,  0.24205174,  0.24855255,  0.25506388,  0.26158604,
##         0.26811935,  0.27466412,  0.28122069,  0.28778936,  0.29437047,
##         0.30096436,  0.30757136,  0.31419181,  0.32082607,  0.32747447,
##         0.33413738,  0.34081516,  0.34750817,  0.35421679,  0.36094138,
##         0.36768234,  0.37444004,  0.38121489,  0.38800728,  0.39481762,
##         0.40164632,  0.40849381,  0.4153605 ,  0.42224683,  0.42915324,
##         0.43608018,  0.44302812,  0.4499975 ,  0.45698881,  0.46400253,
##         0.47103915,  0.47809918,  0.48518311,  0.49229147,  0.4994248 ,
##         0.50658363,  0.51376852,  0.52098003,  0.52821874,  0.53548523,
##         0.5427801 ,  0.55010398,  0.55745748,  0.56484125,  0.57225594,
##         0.57970224,  0.58718081,  0.59469237,  0.60223763,  0.60981734,
##         0.61743225,  0.62508313,  0.63277078,  0.640496  ,  0.64825964,
##         0.65606255,  0.66390562,  0.67178974,  0.67971583,  0.68768487,
##         0.69569781,  0.70375568,  0.7118595 ,  0.72001034,  0.7282093 ,
##         0.73645751,  0.74475613,  0.75310636,  0.76150943,  0.76996662,
##         0.77847925,  0.78704866,  0.79567627,  0.80436351,  0.81311188,
##         0.82192294,  0.83079827,  0.83973953,  0.84874843,  0.85782675,
##         0.86697633,  0.87619907,  0.88549694,  0.89487201,  0.90432639,
##         0.91386231,  0.92348206,  0.93318804,  0.94298274,  0.95286875,
##         0.96284878,  0.97292563,  0.98310227,  0.99338175,  1.00376729,
##         1.01426223,  1.0248701 ,  1.03559457,  1.04643948,  1.05740889,
##         1.06850703,  1.07973837,  1.09110758,  1.10261961,  1.11427965,
##         1.12609318,  1.138066  ,  1.15020422,  1.16251432,  1.17500315,
##         1.18767798,  1.20054655,  1.21361706,  1.22689826,  1.24039947,
##         1.25413065,  1.26810245,  1.28232629,  1.2968144 ,  1.31157996,
##         1.32663715,  1.34200126,  1.35768887,  1.37371791,  1.39010789,
##         1.40688004,  1.42405757,  1.44166589,  1.45973291,  1.47828938,
##         1.49736934,  1.51701052,  1.53725502,  1.55814991,  1.57974813,
##         1.60210948,  1.6253019 ,  1.64940305,  1.67450224,  1.70070296,
##         1.72812608,  1.75691401,  1.78723626,  1.81929684,  1.85334448,
##         1.88968692,  1.9287115 ,  1.97091559,  2.0169534 ,  2.06771086,
##         2.12443167,  2.18894403,  2.26410345,  2.35476152,  2.47026126,
##         2.63280541,  2.9210196 ]), array([2.26, 2.42, 2.42, 2.48, 2.52, 2.55, 2.56, 2.62, 2.62, 2.63, 2.65,
##        2.67, 2.67, 2.68, 2.69, 2.7 , 2.7 , 2.71, 2.71, 2.73, 2.76, 2.78,
##        2.78, 2.79, 2.79, 2.81, 2.81, 2.81, 2.82, 2.82, 2.83, 2.84, 2.85,
##        2.85, 2.86, 2.86, 2.88, 2.9 , 2.9 , 2.9 , 2.9 , 2.91, 2.91, 2.91,
##        2.92, 2.92, 2.93, 2.93, 2.93, 2.93, 2.93, 2.94, 2.94, 2.94, 2.95,
##        2.96, 2.96, 2.97, 2.97, 2.98, 2.98, 2.98, 2.98, 2.98, 2.98, 3.  ,
##        3.  , 3.  , 3.  , 3.01, 3.01, 3.02, 3.02, 3.02, 3.02, 3.03, 3.04,
##        3.04, 3.05, 3.05, 3.05, 3.06, 3.07, 3.07, 3.07, 3.07, 3.08, 3.08,
##        3.08, 3.08, 3.09, 3.1 , 3.11, 3.12, 3.12, 3.12, 3.12, 3.13, 3.13,
##        3.13, 3.13, 3.13, 3.14, 3.14, 3.14, 3.14, 3.15, 3.15, 3.15, 3.15,
##        3.15, 3.15, 3.15, 3.16, 3.16, 3.17, 3.17, 3.17, 3.17, 3.17, 3.18,
##        3.19, 3.19, 3.19, 3.19, 3.19, 3.2 , 3.2 , 3.21, 3.22, 3.22, 3.22,
##        3.22, 3.22, 3.23, 3.23, 3.23, 3.24, 3.24, 3.25, 3.25, 3.27, 3.27,
##        3.27, 3.28, 3.28, 3.28, 3.28, 3.29, 3.29, 3.3 , 3.3 , 3.3 , 3.3 ,
##        3.31, 3.31, 3.31, 3.31, 3.31, 3.31, 3.31, 3.31, 3.32, 3.32, 3.32,
##        3.32, 3.33, 3.33, 3.33, 3.33, 3.33, 3.34, 3.34, 3.34, 3.34, 3.34,
##        3.35, 3.35, 3.35, 3.35, 3.35, 3.35, 3.35, 3.36, 3.36, 3.36, 3.36,
##        3.37, 3.37, 3.37, 3.38, 3.38, 3.38, 3.38, 3.38, 3.39, 3.39, 3.39,
##        3.4 , 3.4 , 3.4 , 3.4 , 3.4 , 3.4 , 3.4 , 3.41, 3.42, 3.43, 3.43,
##        3.43, 3.43, 3.43, 3.44, 3.44, 3.44, 3.45, 3.45, 3.45, 3.45, 3.45,
##        3.45, 3.45, 3.46, 3.46, 3.46, 3.46, 3.46, 3.47, 3.47, 3.47, 3.48,
##        3.48, 3.48, 3.49, 3.49, 3.49, 3.49, 3.5 , 3.5 , 3.5 , 3.5 , 3.51,
##        3.51, 3.51, 3.51, 3.51, 3.52, 3.52, 3.52, 3.52, 3.53, 3.53, 3.54,
##        3.54, 3.54, 3.55, 3.56, 3.56, 3.56, 3.57, 3.57, 3.57, 3.58, 3.58,
##        3.58, 3.58, 3.58, 3.59, 3.59, 3.59, 3.59, 3.59, 3.6 , 3.6 , 3.6 ,
##        3.61, 3.61, 3.61, 3.62, 3.62, 3.63, 3.63, 3.63, 3.63, 3.63, 3.63,
##        3.64, 3.64, 3.64, 3.64, 3.64, 3.65, 3.65, 3.65, 3.65, 3.66, 3.67,
##        3.67, 3.67, 3.67, 3.69, 3.69, 3.69, 3.7 , 3.7 , 3.7 , 3.71, 3.71,
##        3.72, 3.73, 3.73, 3.74, 3.74, 3.74, 3.74, 3.75, 3.75, 3.76, 3.76,
##        3.77, 3.77, 3.77, 3.77, 3.77, 3.78, 3.78, 3.78, 3.78, 3.8 , 3.8 ,
##        3.81, 3.81, 3.81, 3.82, 3.83, 3.84, 3.84, 3.85, 3.86, 3.86, 3.87,
##        3.88, 3.88, 3.88, 3.89, 3.89, 3.89, 3.9 , 3.9 , 3.9 , 3.91, 3.92,
##        3.92, 3.93, 3.94, 3.94, 3.94, 3.94, 3.94, 3.95, 3.95, 3.95, 3.95,
##        3.95, 3.97, 3.98, 3.99, 3.99, 3.99, 4.  , 4.  , 4.  , 4.  , 4.  ,
##        4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  ,
##        4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  , 4.  ,
##        4.  ])), (np.float64(0.37814377982297576), np.float64(3.3922418136020154), np.float64(0.9896460740588751)))</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="hypothesis-tests-on-mean-vectors.html#cb79-1" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">&quot;Normal Q-Q Plot for GPA&quot;</span>)</span>
<span id="cb79-2"><a href="hypothesis-tests-on-mean-vectors.html#cb79-2" tabindex="-1"></a></span>
<span id="cb79-3"><a href="hypothesis-tests-on-mean-vectors.html#cb79-3" tabindex="-1"></a><span class="co"># Chi-square Q-Q Plot</span></span>
<span id="cb79-4"><a href="hypothesis-tests-on-mean-vectors.html#cb79-4" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].plot(q_theo, q_obs, <span class="st">&#39;ko&#39;</span>)</span>
<span id="cb79-5"><a href="hypothesis-tests-on-mean-vectors.html#cb79-5" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].plot(q_theo, q_theo, <span class="st">&#39;r-&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb79-6"><a href="hypothesis-tests-on-mean-vectors.html#cb79-6" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">&quot;Chi-square Q-Q Plot for Admission Data&quot;</span>)</span>
<span id="cb79-7"><a href="hypothesis-tests-on-mean-vectors.html#cb79-7" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">&quot;Theoretical Quantile&quot;</span>)</span>
<span id="cb79-8"><a href="hypothesis-tests-on-mean-vectors.html#cb79-8" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">&quot;Observed Quantile&quot;</span>)</span>
<span id="cb79-9"><a href="hypothesis-tests-on-mean-vectors.html#cb79-9" tabindex="-1"></a></span>
<span id="cb79-10"><a href="hypothesis-tests-on-mean-vectors.html#cb79-10" tabindex="-1"></a>plt.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.6</span>, wspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb79-11"><a href="hypothesis-tests-on-mean-vectors.html#cb79-11" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="Plots/distancefunctions-13.png" width="90%" /></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="hypothesis-tests-on-mean-vectors.html#cb80-1" tabindex="-1"></a>plt.close()</span></code></pre></div>
<ul>
<li>Based on graphs above, address whether the bivariate normality assumption is satisfied.
</li>
<li>Test at the 5% significance level
<span class="math display">\[
H_0: \boldsymbol{\mu}=\left[
\begin{array}{c}
600\\
3.4
\end{array}
\right]
\text{ versus } H_a: \boldsymbol{\mu}\ne\left[
\begin{array}{c}
600\\
3.4
\end{array}
\right].
\]</span>
Use <span class="math inline">\(T^2=(\mathbf{\bar x}-\boldsymbol{\mu}_0)^T\mathbf{S}^{-1}(\mathbf{\bar x}-\boldsymbol{\mu}_0)=0.07137872\)</span>.
</li>
<li>Shall we follow up with a post hoc test at 5% significance level?
</li>
<li>Obtain a 95% confidence interval for GRE and GPA respectively using the Bonferroni method.
</li>
<li>Obtain a 95% simultaneous confidence interval for GRE and GPA respectively. Compare the intervals with Bonferroni intervals above.
</li>
</ul>
<p>We can use to confirm the Bonferroni interval and the simultaneous confidence interval.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="hypothesis-tests-on-mean-vectors.html#cb81-1" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> t, f</span>
<span id="cb81-2"><a href="hypothesis-tests-on-mean-vectors.html#cb81-2" tabindex="-1"></a></span>
<span id="cb81-3"><a href="hypothesis-tests-on-mean-vectors.html#cb81-3" tabindex="-1"></a><span class="kw">def</span> bonferroni_intervals(x, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb81-4"><a href="hypothesis-tests-on-mean-vectors.html#cb81-4" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb81-5"><a href="hypothesis-tests-on-mean-vectors.html#cb81-5" tabindex="-1"></a><span class="co">    Bonferroni corrected confidence intervals for each variable in x.</span></span>
<span id="cb81-6"><a href="hypothesis-tests-on-mean-vectors.html#cb81-6" tabindex="-1"></a><span class="co">    x: n by p numpy array</span></span>
<span id="cb81-7"><a href="hypothesis-tests-on-mean-vectors.html#cb81-7" tabindex="-1"></a><span class="co">    alpha: significance level</span></span>
<span id="cb81-8"><a href="hypothesis-tests-on-mean-vectors.html#cb81-8" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb81-9"><a href="hypothesis-tests-on-mean-vectors.html#cb81-9" tabindex="-1"></a>    n, p <span class="op">=</span> x.shape</span>
<span id="cb81-10"><a href="hypothesis-tests-on-mean-vectors.html#cb81-10" tabindex="-1"></a>    mvec <span class="op">=</span> np.mean(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb81-11"><a href="hypothesis-tests-on-mean-vectors.html#cb81-11" tabindex="-1"></a>    svec <span class="op">=</span> np.std(x, axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb81-12"><a href="hypothesis-tests-on-mean-vectors.html#cb81-12" tabindex="-1"></a>    tscore <span class="op">=</span> t.ppf(<span class="dv">1</span> <span class="op">-</span> alpha<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>p), df<span class="op">=</span>n<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb81-13"><a href="hypothesis-tests-on-mean-vectors.html#cb81-13" tabindex="-1"></a>    lvec <span class="op">=</span> mvec <span class="op">-</span> tscore <span class="op">*</span> svec <span class="op">/</span> np.sqrt(n)</span>
<span id="cb81-14"><a href="hypothesis-tests-on-mean-vectors.html#cb81-14" tabindex="-1"></a>    uvec <span class="op">=</span> mvec <span class="op">+</span> tscore <span class="op">*</span> svec <span class="op">/</span> np.sqrt(n)</span>
<span id="cb81-15"><a href="hypothesis-tests-on-mean-vectors.html#cb81-15" tabindex="-1"></a>    <span class="cf">return</span> np.column_stack((lvec, uvec))</span>
<span id="cb81-16"><a href="hypothesis-tests-on-mean-vectors.html#cb81-16" tabindex="-1"></a></span>
<span id="cb81-17"><a href="hypothesis-tests-on-mean-vectors.html#cb81-17" tabindex="-1"></a><span class="kw">def</span> simultaneous_intervals(x, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb81-18"><a href="hypothesis-tests-on-mean-vectors.html#cb81-18" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb81-19"><a href="hypothesis-tests-on-mean-vectors.html#cb81-19" tabindex="-1"></a><span class="co">    Simultaneous confidence intervals for each variable in x.</span></span>
<span id="cb81-20"><a href="hypothesis-tests-on-mean-vectors.html#cb81-20" tabindex="-1"></a><span class="co">    x: n by p numpy array</span></span>
<span id="cb81-21"><a href="hypothesis-tests-on-mean-vectors.html#cb81-21" tabindex="-1"></a><span class="co">    alpha: significance level</span></span>
<span id="cb81-22"><a href="hypothesis-tests-on-mean-vectors.html#cb81-22" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb81-23"><a href="hypothesis-tests-on-mean-vectors.html#cb81-23" tabindex="-1"></a>    n, p <span class="op">=</span> x.shape</span>
<span id="cb81-24"><a href="hypothesis-tests-on-mean-vectors.html#cb81-24" tabindex="-1"></a>    mvec <span class="op">=</span> np.mean(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb81-25"><a href="hypothesis-tests-on-mean-vectors.html#cb81-25" tabindex="-1"></a>    svec <span class="op">=</span> np.std(x, axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb81-26"><a href="hypothesis-tests-on-mean-vectors.html#cb81-26" tabindex="-1"></a>    fscore <span class="op">=</span> f.ppf(<span class="dv">1</span> <span class="op">-</span> alpha, dfn<span class="op">=</span>p, dfd<span class="op">=</span>n<span class="op">-</span>p)</span>
<span id="cb81-27"><a href="hypothesis-tests-on-mean-vectors.html#cb81-27" tabindex="-1"></a>    multiplier <span class="op">=</span> np.sqrt(fscore <span class="op">*</span> p <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (n <span class="op">-</span> p))</span>
<span id="cb81-28"><a href="hypothesis-tests-on-mean-vectors.html#cb81-28" tabindex="-1"></a>    lvec <span class="op">=</span> mvec <span class="op">-</span> multiplier <span class="op">*</span> svec <span class="op">/</span> np.sqrt(n)</span>
<span id="cb81-29"><a href="hypothesis-tests-on-mean-vectors.html#cb81-29" tabindex="-1"></a>    uvec <span class="op">=</span> mvec <span class="op">+</span> multiplier <span class="op">*</span> svec <span class="op">/</span> np.sqrt(n)</span>
<span id="cb81-30"><a href="hypothesis-tests-on-mean-vectors.html#cb81-30" tabindex="-1"></a>    <span class="cf">return</span> np.column_stack((lvec, uvec))</span>
<span id="cb81-31"><a href="hypothesis-tests-on-mean-vectors.html#cb81-31" tabindex="-1"></a></span>
<span id="cb81-32"><a href="hypothesis-tests-on-mean-vectors.html#cb81-32" tabindex="-1"></a>np.random.seed(<span class="dv">4061</span>)</span>
<span id="cb81-33"><a href="hypothesis-tests-on-mean-vectors.html#cb81-33" tabindex="-1"></a>axx_df <span class="op">=</span> df.loc[df[<span class="st">&#39;admit&#39;</span>] <span class="op">==</span> <span class="dv">1</span>, [<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]]</span>
<span id="cb81-34"><a href="hypothesis-tests-on-mean-vectors.html#cb81-34" tabindex="-1"></a>axx <span class="op">=</span> axx_df.to_numpy()</span>
<span id="cb81-35"><a href="hypothesis-tests-on-mean-vectors.html#cb81-35" tabindex="-1"></a></span>
<span id="cb81-36"><a href="hypothesis-tests-on-mean-vectors.html#cb81-36" tabindex="-1"></a>cib <span class="op">=</span> bonferroni_intervals(axx, <span class="fl">0.05</span>)</span>
<span id="cb81-37"><a href="hypothesis-tests-on-mean-vectors.html#cb81-37" tabindex="-1"></a>cis <span class="op">=</span> simultaneous_intervals(axx, <span class="fl">0.05</span>)</span>
<span id="cb81-38"><a href="hypothesis-tests-on-mean-vectors.html#cb81-38" tabindex="-1"></a></span>
<span id="cb81-39"><a href="hypothesis-tests-on-mean-vectors.html#cb81-39" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bonferroni intervals:</span><span class="ch">\n</span><span class="st">&quot;</span>, cib)</span></code></pre></div>
<pre><code>## Bonferroni intervals:
##  [[596.4890261  640.65383104]
##  [  3.41408974   3.56432296]]</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="hypothesis-tests-on-mean-vectors.html#cb83-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Simultaneous intervals:</span><span class="ch">\n</span><span class="st">&quot;</span>, cis)</span></code></pre></div>
<pre><code>## Simultaneous intervals:
##  [[594.35875229 642.78410486]
##  [  3.40684329   3.57156941]]</code></pre>
<p>We can construct simultaneous confidence region, simultaneous confidence interval, and Bonferroni interval to compare the two intervals. The confidence region is an ellipsoid centered at the mean vector. Since the hypothesized value <span class="math inline">\(\boldsymbol{\mu}_0=[600, 3.4]^T\)</span> is outside the confidence region, we should reject the null hypothesis.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="hypothesis-tests-on-mean-vectors.html#cb85-1" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> inv, eigvals</span>
<span id="cb85-2"><a href="hypothesis-tests-on-mean-vectors.html#cb85-2" tabindex="-1"></a></span>
<span id="cb85-3"><a href="hypothesis-tests-on-mean-vectors.html#cb85-3" tabindex="-1"></a><span class="co"># Manual ellipse in python, unlike R</span></span>
<span id="cb85-4"><a href="hypothesis-tests-on-mean-vectors.html#cb85-4" tabindex="-1"></a><span class="kw">def</span> confidence_ellipse(cov, mean, n, p, alpha<span class="op">=</span><span class="fl">0.05</span>, points<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb85-5"><a href="hypothesis-tests-on-mean-vectors.html#cb85-5" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb85-6"><a href="hypothesis-tests-on-mean-vectors.html#cb85-6" tabindex="-1"></a><span class="co">    Generate points on the confidence ellipse.</span></span>
<span id="cb85-7"><a href="hypothesis-tests-on-mean-vectors.html#cb85-7" tabindex="-1"></a><span class="co">    cov: covariance matrix (p x p)</span></span>
<span id="cb85-8"><a href="hypothesis-tests-on-mean-vectors.html#cb85-8" tabindex="-1"></a><span class="co">    mean: mean vector (length p)</span></span>
<span id="cb85-9"><a href="hypothesis-tests-on-mean-vectors.html#cb85-9" tabindex="-1"></a><span class="co">    n: sample size</span></span>
<span id="cb85-10"><a href="hypothesis-tests-on-mean-vectors.html#cb85-10" tabindex="-1"></a><span class="co">    p: dimension</span></span>
<span id="cb85-11"><a href="hypothesis-tests-on-mean-vectors.html#cb85-11" tabindex="-1"></a><span class="co">    alpha: significance level</span></span>
<span id="cb85-12"><a href="hypothesis-tests-on-mean-vectors.html#cb85-12" tabindex="-1"></a><span class="co">    points: number of points on ellipse</span></span>
<span id="cb85-13"><a href="hypothesis-tests-on-mean-vectors.html#cb85-13" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb85-14"><a href="hypothesis-tests-on-mean-vectors.html#cb85-14" tabindex="-1"></a><span class="co">    Returns: array of shape (points, p) with ellipse coordinates</span></span>
<span id="cb85-15"><a href="hypothesis-tests-on-mean-vectors.html#cb85-15" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb85-16"><a href="hypothesis-tests-on-mean-vectors.html#cb85-16" tabindex="-1"></a>    f_score <span class="op">=</span> f.ppf(<span class="dv">1</span> <span class="op">-</span> alpha, dfn<span class="op">=</span>p, dfd<span class="op">=</span>n<span class="op">-</span>p)</span>
<span id="cb85-17"><a href="hypothesis-tests-on-mean-vectors.html#cb85-17" tabindex="-1"></a>    radius <span class="op">=</span> np.sqrt(p <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (n <span class="op">*</span> (n <span class="op">-</span> p)) <span class="op">*</span> f_score)</span>
<span id="cb85-18"><a href="hypothesis-tests-on-mean-vectors.html#cb85-18" tabindex="-1"></a></span>
<span id="cb85-19"><a href="hypothesis-tests-on-mean-vectors.html#cb85-19" tabindex="-1"></a>    vals, vecs <span class="op">=</span> np.linalg.eigh(cov)</span>
<span id="cb85-20"><a href="hypothesis-tests-on-mean-vectors.html#cb85-20" tabindex="-1"></a>    order <span class="op">=</span> vals.argsort()[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb85-21"><a href="hypothesis-tests-on-mean-vectors.html#cb85-21" tabindex="-1"></a>    vals <span class="op">=</span> vals[order]</span>
<span id="cb85-22"><a href="hypothesis-tests-on-mean-vectors.html#cb85-22" tabindex="-1"></a>    vecs <span class="op">=</span> vecs[:, order]</span>
<span id="cb85-23"><a href="hypothesis-tests-on-mean-vectors.html#cb85-23" tabindex="-1"></a></span>
<span id="cb85-24"><a href="hypothesis-tests-on-mean-vectors.html#cb85-24" tabindex="-1"></a>    theta <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> np.pi, points)</span>
<span id="cb85-25"><a href="hypothesis-tests-on-mean-vectors.html#cb85-25" tabindex="-1"></a>    circle <span class="op">=</span> np.array([np.cos(theta), np.sin(theta)])  <span class="co"># unit circle points</span></span>
<span id="cb85-26"><a href="hypothesis-tests-on-mean-vectors.html#cb85-26" tabindex="-1"></a></span>
<span id="cb85-27"><a href="hypothesis-tests-on-mean-vectors.html#cb85-27" tabindex="-1"></a>    axis_lengths <span class="op">=</span> radius <span class="op">*</span> np.sqrt(vals)</span>
<span id="cb85-28"><a href="hypothesis-tests-on-mean-vectors.html#cb85-28" tabindex="-1"></a>    ellipse <span class="op">=</span> (vecs <span class="op">@</span> np.diag(axis_lengths)) <span class="op">@</span> circle</span>
<span id="cb85-29"><a href="hypothesis-tests-on-mean-vectors.html#cb85-29" tabindex="-1"></a>    ellipse <span class="op">=</span> ellipse.T <span class="op">+</span> mean</span>
<span id="cb85-30"><a href="hypothesis-tests-on-mean-vectors.html#cb85-30" tabindex="-1"></a></span>
<span id="cb85-31"><a href="hypothesis-tests-on-mean-vectors.html#cb85-31" tabindex="-1"></a>    <span class="cf">return</span> ellipse</span>
<span id="cb85-32"><a href="hypothesis-tests-on-mean-vectors.html#cb85-32" tabindex="-1"></a></span>
<span id="cb85-33"><a href="hypothesis-tests-on-mean-vectors.html#cb85-33" tabindex="-1"></a>n, p <span class="op">=</span> axx.shape</span>
<span id="cb85-34"><a href="hypothesis-tests-on-mean-vectors.html#cb85-34" tabindex="-1"></a>mean_vec <span class="op">=</span> np.mean(axx, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb85-35"><a href="hypothesis-tests-on-mean-vectors.html#cb85-35" tabindex="-1"></a>cov_mat <span class="op">=</span> np.cov(axx, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb85-36"><a href="hypothesis-tests-on-mean-vectors.html#cb85-36" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb85-37"><a href="hypothesis-tests-on-mean-vectors.html#cb85-37" tabindex="-1"></a>mu0 <span class="op">=</span> np.array([<span class="dv">600</span>, <span class="fl">3.4</span>])  <span class="co"># hypothesized mean</span></span>
<span id="cb85-38"><a href="hypothesis-tests-on-mean-vectors.html#cb85-38" tabindex="-1"></a></span>
<span id="cb85-39"><a href="hypothesis-tests-on-mean-vectors.html#cb85-39" tabindex="-1"></a>ellip_points <span class="op">=</span> confidence_ellipse(cov_mat, mean_vec, n, p, alpha)</span>
<span id="cb85-40"><a href="hypothesis-tests-on-mean-vectors.html#cb85-40" tabindex="-1"></a></span>
<span id="cb85-41"><a href="hypothesis-tests-on-mean-vectors.html#cb85-41" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb85-42"><a href="hypothesis-tests-on-mean-vectors.html#cb85-42" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb85-43"><a href="hypothesis-tests-on-mean-vectors.html#cb85-43" tabindex="-1"></a>plt.plot(ellip_points[:,<span class="dv">0</span>], ellip_points[:,<span class="dv">1</span>], label<span class="op">=</span><span class="st">&#39;Simultaneous Confidence Region (Ellipsoid)&#39;</span>)</span>
<span id="cb85-44"><a href="hypothesis-tests-on-mean-vectors.html#cb85-44" tabindex="-1"></a>plt.scatter(<span class="op">*</span>mu0, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="vs">r&#39;$\mu_0$ (Hypothesized Mean)&#39;</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb85-45"><a href="hypothesis-tests-on-mean-vectors.html#cb85-45" tabindex="-1"></a>plt.scatter(<span class="op">*</span>mean_vec, color<span class="op">=</span><span class="st">&#39;green&#39;</span>, label<span class="op">=</span><span class="st">&#39;Sample Mean&#39;</span>, zorder<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb85-46"><a href="hypothesis-tests-on-mean-vectors.html#cb85-46" tabindex="-1"></a></span>
<span id="cb85-47"><a href="hypothesis-tests-on-mean-vectors.html#cb85-47" tabindex="-1"></a><span class="co"># Plot simultaneous confidence intervals (from previous function)</span></span>
<span id="cb85-48"><a href="hypothesis-tests-on-mean-vectors.html#cb85-48" tabindex="-1"></a>cis <span class="op">=</span> simultaneous_intervals(axx, alpha)</span>
<span id="cb85-49"><a href="hypothesis-tests-on-mean-vectors.html#cb85-49" tabindex="-1"></a>plt.axvline(cis[<span class="dv">0</span>,<span class="dv">0</span>], color<span class="op">=</span><span class="st">&#39;black&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-50"><a href="hypothesis-tests-on-mean-vectors.html#cb85-50" tabindex="-1"></a>plt.axvline(cis[<span class="dv">0</span>,<span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;black&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-51"><a href="hypothesis-tests-on-mean-vectors.html#cb85-51" tabindex="-1"></a>plt.axhline(cis[<span class="dv">1</span>,<span class="dv">0</span>], color<span class="op">=</span><span class="st">&#39;black&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-52"><a href="hypothesis-tests-on-mean-vectors.html#cb85-52" tabindex="-1"></a>plt.axhline(cis[<span class="dv">1</span>,<span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;black&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;-&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-53"><a href="hypothesis-tests-on-mean-vectors.html#cb85-53" tabindex="-1"></a></span>
<span id="cb85-54"><a href="hypothesis-tests-on-mean-vectors.html#cb85-54" tabindex="-1"></a><span class="co"># Plot Bonferroni intervals (from previous function)</span></span>
<span id="cb85-55"><a href="hypothesis-tests-on-mean-vectors.html#cb85-55" tabindex="-1"></a>cib <span class="op">=</span> bonferroni_intervals(axx, alpha)</span>
<span id="cb85-56"><a href="hypothesis-tests-on-mean-vectors.html#cb85-56" tabindex="-1"></a>plt.axvline(cib[<span class="dv">0</span>,<span class="dv">0</span>], color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-57"><a href="hypothesis-tests-on-mean-vectors.html#cb85-57" tabindex="-1"></a>plt.axvline(cib[<span class="dv">0</span>,<span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-58"><a href="hypothesis-tests-on-mean-vectors.html#cb85-58" tabindex="-1"></a>plt.axhline(cib[<span class="dv">1</span>,<span class="dv">0</span>], color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-59"><a href="hypothesis-tests-on-mean-vectors.html#cb85-59" tabindex="-1"></a>plt.axhline(cib[<span class="dv">1</span>,<span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, linestyle<span class="op">=</span><span class="st">&#39;--&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb85-60"><a href="hypothesis-tests-on-mean-vectors.html#cb85-60" tabindex="-1"></a></span>
<span id="cb85-61"><a href="hypothesis-tests-on-mean-vectors.html#cb85-61" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;GRE&#39;</span>)</span>
<span id="cb85-62"><a href="hypothesis-tests-on-mean-vectors.html#cb85-62" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;GPA&#39;</span>)</span>
<span id="cb85-63"><a href="hypothesis-tests-on-mean-vectors.html#cb85-63" tabindex="-1"></a>plt.title(<span class="st">&#39;Simultaneous Confidence Region and Intervals&#39;</span>)</span>
<span id="cb85-64"><a href="hypothesis-tests-on-mean-vectors.html#cb85-64" tabindex="-1"></a>plt.legend()</span>
<span id="cb85-65"><a href="hypothesis-tests-on-mean-vectors.html#cb85-65" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb85-66"><a href="hypothesis-tests-on-mean-vectors.html#cb85-66" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="Plots/confintervalsb-15.png" width="70%" /></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="hypothesis-tests-on-mean-vectors.html#cb86-1" tabindex="-1"></a>plt.close()</span></code></pre></div>
</div>
</div>
<div id="hypothesis-test-for-two-mean-vectors" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Hypothesis Test for Two Mean Vectors<a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-two-mean-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before diving to the multivariate case, let’s review how we compare two population means in the univariate case.</p>
<div id="univariate-case-based-on-two-independent-samples" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Univariate Case Based on Two Independent Samples<a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-two-independent-samples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose there are two populations; we want to compare whether the two population means are the same or not based on two independent samples.
</p>
<pre><code>## (np.float64(-0.5), np.float64(775.5), np.float64(547.5), np.float64(-0.5))</code></pre>
<p><img src="Plots/twosampleT-17.png" width="60%" style="display: block; margin: auto;" /></p>
<p><span class="math inline">\(\textbf{Two-sample t Tests}\)</span></p>
<p>Hypotheses: <span class="math inline">\(H_0: \mu_1-\mu_2=\Delta_0 \mbox{  versus  } H_a: \mu_1-\mu_2\ne \Delta_0\)</span></p>
[
<span class="math display">\[\begin{array}{c|c}

\text{Two-sample Pooled t Test} &amp; \text{Two-sample Non-pooled t Test} \\
----- &amp; ----- \\
\text{Assumptions} &amp; \text{Assumptions} \\
\text{- Simple Random Samples} &amp; \text{- Simple Random Samples} \\
\text{- Two independent samples} &amp; \text{- Two independent samples} \\
\text{- Normal populations or large samples} &amp; \text{- Normal populations or large samples} \\
\frac{\max\{s_1, s_2\}}{\min\{s_1,s_2\}}&lt;2 &amp; \\

\bullet{\text{ Test Statistic:}} &amp; \bullet{\text{ Test Statistic:}} \\


t_o=\frac{(\bar x_1-\bar x_2)-\Delta_0}{\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}, df=n_1+n_2-2

&amp;

t_o=\frac{(\bar x_1-\bar x_2)-\Delta_0}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}


\\ \\

\text{Reject $H_0$ if $|t_o|\ge t_{n1+n2-2}(\alpha/2)$.} &amp; \text{Reject $H_0$ if $|t_o|\ge t_{df^{\ast}}(\alpha/2)$.}
\\

\end{array}\]</span>
<p>]</p>
</div>
<div id="multivariate-case-based-on-two-independent-samples" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Multivariate Case Based on Two Independent Samples<a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-two-independent-samples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We generalize the ideas to the multivariate case, we want to compare whether the two mean vectors are the same, that is,
<span class="math display">\[
H_0: \boldsymbol{\mu_1}-\boldsymbol{\mu_2}=\boldsymbol{\Delta_0} \mbox{  versus } \boldsymbol{\mu_1}-\boldsymbol{\mu_2}=\boldsymbol{\Delta_0}.
\]</span></p>
<p>If the two populations have the same covariance matrix, we could pool the two samples and obtain a better estimate of the common covariance matrix
<span class="math display">\[
\mathbf{S}_{\mbox{pooled}}=\frac{(n_1-1)\mathbf{S_1}+(n_2-1)\mathbf{S_2}}{n_1+n_2-2}
\]</span>
As a result,
<span class="math display">\[
\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\mathbf{S}_{\mbox{pooled}}
\]</span>
is an estimator of <span class="math inline">\(\boldsymbol{\Sigma}_1=\boldsymbol{\Sigma}_2=\boldsymbol{\Sigma}=\text{Var}(\mathbf{\bar x_1}-\mathbf{\bar x_2})\)</span>. And we can use the pooled Hotelling’s <span class="math inline">\(T^2\)</span> test.</p>
<p><span class="math inline">\(\textbf{Assumptions}:\)</span></p>
<ul>
<li>Simple random samples</li>
<li>Two independent samples</li>
<li>Multivariate normal populations or large samples.</li>
</ul>
<p>We could use the chi-square probability plot to assess the multivariate normality assumption. One plot for one sample. If the multivariate normal populations’ assumption is satisfied, the data points should roughly fall on a 45-degree straight line for both samples.</p>
<p>Note that the Central Limit Theorem implies that the sample mean vectors are going to be approximately multivariate normally distributed regardless of the distribution of the original variables when the sample sizes are large enough. Therefore, in general, Hotelling’s <span class="math inline">\(T^2\)</span> is not going to be sensitive to violations of the multivariate normal assumption.
* <span class="math inline">\(\boldsymbol{\Sigma}_1=\boldsymbol{\Sigma}_2\)</span>.</p>
<p>This assumption may be assessed using Barlett’s Test. The hypotheses are <span class="math inline">\(H_0: \boldsymbol{\Sigma}_1=\boldsymbol{\Sigma}_2\)</span> versus <span class="math inline">\(H_a: \boldsymbol{\Sigma}_1\ne \boldsymbol{\Sigma}_2\)</span>. The test statistic for Bartlett’s Test is given by L prime as shown below:
<span class="math display">\[
L^{&#39;}=c\{(n_1+n_2-2)\log|\mathbf{S}_{\mbox{pooled}}|-(n_1-1)\log|\boldsymbol{\Sigma}_1|-(n_2-1)\log|\boldsymbol{\Sigma}_2|\} \mbox{     with}
\]</span>
<span class="math display">\[
c=1-\frac{2p^2+3p-1}{6(p-1)}\left[\frac{1}{n_1-1}+\frac{1}{n_2-1}-\frac{1}{n_1+n_2-2}\right].
\]</span>
Under the null hypothesis, the Bartlett’s test statistic is approximately chi-square distributed with <span class="math inline">\(df=\frac{p(p+1)}{2}\)</span>. Therefore, we reject <span class="math inline">\(H_0: \boldsymbol{\Sigma}_1=\boldsymbol{\Sigma}_2\)</span> at significance level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(L^{&#39;}\ge \chi_{\frac{p(p+1)}{2}}^2 (\alpha)\)</span>. Note that Bartlett’s test is not robust to the violations of multivariate assumption and should not be used if there is any indication that the data are not multivariate normally distributed.</p>
<p>The test statistic of the Hotelling’s <span class="math inline">\(T^2\)</span> test for comparing two mean vectors is
<span class="math display">\[
T^2=[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta}_0]^{T}\left[\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\mathbf{S}_{\mbox{pooled}}\right]^{-1}[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta}_0]
\]</span>
with
<span class="math display">\[
\widehat{\boldsymbol{\Sigma}}=\mathbf{S}_{\mbox{pooled}}=\frac{(n_1-1)\mathbf{S}_1+(n_2-1)\mathbf{S}_2}{n_1+n_2-2}.
\]</span>
It can be shown that <span class="math inline">\(T^2\sim  \frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p, n_1+n_2-p-1}\)</span>. Therefore, we can reject <span class="math inline">\(H_0\)</span> if
<span class="math display">\[
\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2\ge F_{p, n_1+n_2-p-1}(\alpha).
\]</span></p>
</div>
<div id="two-sample-non-pooled-hotellings-t2-test" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Two-sample Non-pooled Hotelling’s <span class="math inline">\(T^2\)</span> Test<a href="hypothesis-tests-on-mean-vectors.html#two-sample-non-pooled-hotellings-t2-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(\boldsymbol{\Sigma}_1\ne \boldsymbol{\Sigma}_2\)</span> and the sample sizes <span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2\)</span> are not large enough, the test statistic becomes
<span class="math display">\[
T^2=[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta}_0]^{T}\left[\frac{1}{n_1}\mathbf{S}_1+\frac{1}{n_2}\mathbf{S}_2\right]^{-1}[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta}_0].
\]</span>
It can be shown that <span class="math inline">\(T^2\sim \frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p, \gamma}\)</span>, where
<span class="math display">\[
\frac{1}{\gamma}=\sum_{i=1}^2 \frac{1}{n_i-1}\left\{\frac{[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta}_0]^{T}\mathbf{S}_T^{-1}(\frac{1}{n_i} \mathbf{S}_i) \mathbf{S}_T^{-1} [(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta}_0]}{T^2}\right\}^2
\]</span>
where <span class="math inline">\(\mathbf{S}_T=\frac{1}{n_1}\mathbf{S}_1+\frac{1}{n_2}\mathbf{S}_2\)</span>. Therefore, we can reject <span class="math inline">\(H_0\)</span> if
<span class="math display">\[
\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2\ge F_{p, \gamma}(\alpha).
\]</span></p>
<p>If sample sizes <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are large enough such that <span class="math inline">\(n_1-p\)</span> and <span class="math inline">\(n_2-p\)</span> are large, then we should reject <span class="math inline">\(H_0: \boldsymbol{\mu_1}-\boldsymbol{\mu_2}=\boldsymbol{\Delta_0}\)</span> if
<span class="math display">\[
[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta_0}]^{T}\left[\frac{1}{n_1}\mathbf{S_1}+\frac{1}{n_2}\mathbf{S_2}\right]^{-1}[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-\boldsymbol{\Delta_0}]\ge \chi^2_{p}(\alpha).
\]</span></p>
</div>
<div id="two-sample-hotellings-t2-confidence-interval" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Two-sample Hotelling’s <span class="math inline">\(T^2\)</span> Confidence Interval<a href="hypothesis-tests-on-mean-vectors.html#two-sample-hotellings-t2-confidence-interval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we reject <span class="math inline">\(H_0: \boldsymbol{\mu}_1-\boldsymbol{\mu}_2=\boldsymbol{\Delta}_0\)</span>, we would like to figure out in which variables the means are different.</p>
<ul>
<li>A <span class="math inline">\((1-\alpha)\times 100 \%\)</span> simultaneous confidence interval for the <span class="math inline">\(i\)</span>th variable <span class="math inline">\(X_i\)</span> is given by
<span class="math display">\[
(\bar x_{1i}-\bar x_{2i}) \pm \sqrt{\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p, n_1+n_2-p-1}(\alpha)}\sqrt{s_{\mbox{ ${pooled}$ },i}^2(\frac{1}{n_1}+\frac{1}{n_2})}, i=1, 2, \cdots, p
\]</span>
where <span class="math inline">\(\bar x_{1i}\)</span> and <span class="math inline">\(\bar x_{2i}\)</span> are the sample mean of <span class="math inline">\(X_i\)</span> based on the first and the second sample respectively. That is
<span class="math display">\[
\bar x_{1i}=\frac{1}{n_1}\sum_{k=1}^{n_1} x_{1ik}, \bar x_{2i}=\frac{1}{n_2}\sum_{k=1}^{n_2} x_{2ik}.
\]</span>
And $s_{,i}^2$ is the pooled sample variance of the <span class="math inline">\(X_i\)</span>. That is
<span class="math display">\[
\small
s_{\mbox{ ${pooled}$ },i}^2=\frac{(n_1-1)s_{1i}^2+(n_2-1)s_{2i}^2}{n_1+n_2-2}, s_{1i}^2=\frac{1}{n_1-1}\sum_{k=1}^{n_1}(x_{1ik}-\bar x_{1i})^2, s_{2i}^2=\frac{1}{n_2-1}\sum_{k=1}^{n_2}(x_{2ik}-\bar x_{2i})^2.
\]</span></li>
<li>A <span class="math inline">\((1-\alpha)\times 100 \%\)</span> Bonferroni interval</li>
</ul>
<p>To address the <em>multiple comparisons</em> problem, a <span class="math inline">\((1-\alpha)\times 100 \%\)</span> Bonferroni interval is given by
<span class="math display">\[
(\bar x_{1i}-\bar x_{2i}) \pm t_{n_1+n_2-2}(\frac{\alpha}{2p})\sqrt{s_{\mbox{ ${pooled}$ },i}^2(\frac{1}{n_1}+\frac{1}{n_2})}, i=1, 2, \cdots, p,
\]</span>
where <span class="math inline">\(t_{n_1+n_2-2}(\frac{\alpha}{2p})\)</span> is the <span class="math inline">\(\frac{\alpha}{2p}\times 100\)</span>th upper-tailed quantile of the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df=n_1+n_2-2\)</span>. The mean vector is different in <span class="math inline">\(X_i\)</span> by <span class="math inline">\(\Delta_{0, i}\)</span> if <span class="math inline">\(\Delta_{0,i}\)</span> is outside the interval.</p>
<p><span class="math inline">\(\textbf{Example: Comparing Two Mean Vectors with Birds Data}\)</span></p>
<p>For Bumpus’s bird data, at the 5% significance level, test whether the survivors and non-survivors are different in the five measurements.</p>
<ul>
<li>Hypotheses. <span class="math inline">\(H_0: \boldsymbol{\mu_1}-\boldsymbol{\mu_2}=\mathbf{0}\)</span>, <span class="math inline">\(H_a: \mbox{at least one } \mu_{1i}\ne \mu_{2i}, i=1, 2, \cdots, p\)</span>.</li>
<li>Find the sample mean vectors and covariance matrices for the survivors and non-survivors.
<span class="math display">\[
\mathbf{\bar x_1}=\left[
\begin{array}{c}
157.381\\
241.000\\
31.433\\
18.500\\
20.810
\end{array}
\right], \mathbf{S_1}=\left[
\begin{array}{rrrrr}
11.048 &amp; 9.10&amp; 1.557&amp; 0.870&amp; 1.286\\
9.100&amp; 17.50&amp; 1.910&amp; 1.310&amp; 0.880\\
1.557 &amp; 1.91&amp; 0.531&amp; 0.189&amp; 0.240\\
0.870&amp;  1.31&amp; 0.189&amp; 0.176&amp; 0.133\\
1.286 &amp; 0.88&amp; 0.240&amp; 0.133&amp; 0.575
\end{array}
\right]
\]</span>
<span class="math display">\[
\mathbf{\bar x_2}=\left[
\begin{array}{c}
158.429\\
241.571\\
31.479\\
18.446\\
20.839
\end{array}
\right], \mathbf{S_2}=\left[
\begin{array}{rrrrr}
15.069&amp; 17.190&amp; 2.243&amp; 1.746&amp; 2.931\\
17.190&amp; 32.550&amp; 3.398&amp; 2.950 &amp;4.066\\
2.243 &amp; 3.398&amp; 0.728 &amp;0.470 &amp;0.559\\
1.746 &amp; 2.950&amp; 0.470&amp; 0.434&amp; 0.506\\
2.931&amp;  4.066&amp; 0.559&amp; 0.506 &amp;1.321
\end{array}
\right]
\]</span></li>
<li>Calculate the observed value of the test statistic <span class="math inline">\(T^2\)</span>. The pooled covariance matrix is
<span class="math display">\[
\mathbf{S}_{\mbox{pooled}}=\left[
\begin{array}{rrrrr}
13.358&amp; 13.747&amp; 1.951&amp; 1.373&amp; 2.231\\
13.747&amp; 26.146&amp; 2.765&amp; 2.252&amp; 2.710\\
1.951&amp;  2.765&amp; 0.644&amp; 0.350&amp; 0.423\\
1.373&amp;  2.252&amp;0.350&amp; 0.324&amp; 0.347\\
2.231&amp;  2.710&amp;0.423&amp; 0.347&amp; 1.004
\end{array}
\right]
\]</span>
and
<span class="math display">\[
\left[\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\mathbf{S}_{\mbox{pooled}}\right]^{-1}=\left[
\begin{array}{rrrrr}
2.473&amp; -0.832&amp;  -2.883 &amp;  0.950&amp;  -2.362\\
-0.832&amp;  1.482&amp;  -0.448&amp;  -6.648&amp;   0.337\\
-2.883&amp; -0.448&amp;  50.793&amp; -39.335&amp;  -0.190\\
0.950&amp; -6.648&amp; -39.335&amp; 138.136&amp; -15.336\\
-2.362&amp;  0.337&amp;  -0.190&amp; -15.336&amp;  21.673
\end{array}
\right],
\]</span>
<span class="math display">\[
\small
\begin{aligned}
T^2&amp;=[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-(\boldsymbol{\mu_1}-\boldsymbol{\mu_2})]^{T}\left[\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\mathbf{S}_{\mbox{pooled}}\right]^{-1}[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-(\boldsymbol{\mu_1}-\boldsymbol{\mu_2})]\\
&amp;=[-1.048, -0.571, -0.046, 0.054, -0.029]\left[
\begin{array}{rrrrr}
2.473&amp; -0.832&amp;  -2.883 &amp;  0.950&amp;  -2.362\\
-0.832&amp;  1.482&amp;  -0.448&amp;  -6.648&amp;   0.337\\
-2.883&amp; -0.448&amp;  50.793&amp; -39.335&amp;  -0.190\\
0.950&amp; -6.648&amp; -39.335&amp; 138.136&amp; -15.336\\
-2.362&amp;  0.337&amp;  -0.190&amp; -15.336&amp;  21.673
\end{array}
\right] \left[
\begin{array}{c}
-1.048\\
-0.571\\
-0.046\\
0.054\\
-0.029
\end{array}
\right]\\
&amp;=2.843
\end{aligned}
\]</span></li>
<li>Since <span class="math inline">\(F_o=\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2=\frac{21+28-5-1}{(21+28-2)5}\times 2.843=0.52&lt;F_{p, n_1+n_2-p-1}(\alpha)=F_{5, 43}(0.05)=2.432\)</span>, there is no sufficient evidence to conclude that the survivors and non-survivors are different in the five body measurements at the 5% significance level.</li>
</ul>
<p>Use <span class="math inline">\(\textsf{R}\)</span> to confirm the result.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="hypothesis-tests-on-mean-vectors.html#cb88-1" tabindex="-1"></a><span class="co"># manual hotelling</span></span>
<span id="cb88-2"><a href="hypothesis-tests-on-mean-vectors.html#cb88-2" tabindex="-1"></a><span class="kw">def</span> hotellings_t2(x1, x2):</span>
<span id="cb88-3"><a href="hypothesis-tests-on-mean-vectors.html#cb88-3" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb88-4"><a href="hypothesis-tests-on-mean-vectors.html#cb88-4" tabindex="-1"></a><span class="co">    Hotelling&#39;s T-squared test for two multivariate samples.</span></span>
<span id="cb88-5"><a href="hypothesis-tests-on-mean-vectors.html#cb88-5" tabindex="-1"></a><span class="co">    Returns the T² statistic and the corresponding F-statistic and p-value.</span></span>
<span id="cb88-6"><a href="hypothesis-tests-on-mean-vectors.html#cb88-6" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb88-7"><a href="hypothesis-tests-on-mean-vectors.html#cb88-7" tabindex="-1"></a>    n1, p <span class="op">=</span> x1.shape</span>
<span id="cb88-8"><a href="hypothesis-tests-on-mean-vectors.html#cb88-8" tabindex="-1"></a>    n2, _ <span class="op">=</span> x2.shape</span>
<span id="cb88-9"><a href="hypothesis-tests-on-mean-vectors.html#cb88-9" tabindex="-1"></a>    <span class="co"># Mean vectors</span></span>
<span id="cb88-10"><a href="hypothesis-tests-on-mean-vectors.html#cb88-10" tabindex="-1"></a>    mean1 <span class="op">=</span> np.mean(x1, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb88-11"><a href="hypothesis-tests-on-mean-vectors.html#cb88-11" tabindex="-1"></a>    mean2 <span class="op">=</span> np.mean(x2, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb88-12"><a href="hypothesis-tests-on-mean-vectors.html#cb88-12" tabindex="-1"></a>    <span class="co"># Pooled covariance matrix</span></span>
<span id="cb88-13"><a href="hypothesis-tests-on-mean-vectors.html#cb88-13" tabindex="-1"></a>    s1 <span class="op">=</span> np.cov(x1, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-14"><a href="hypothesis-tests-on-mean-vectors.html#cb88-14" tabindex="-1"></a>    s2 <span class="op">=</span> np.cov(x2, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-15"><a href="hypothesis-tests-on-mean-vectors.html#cb88-15" tabindex="-1"></a>    spooled <span class="op">=</span> ((n1 <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> s1 <span class="op">+</span> (n2 <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> s2) <span class="op">/</span> (n1 <span class="op">+</span> n2 <span class="op">-</span> <span class="dv">2</span>)</span>
<span id="cb88-16"><a href="hypothesis-tests-on-mean-vectors.html#cb88-16" tabindex="-1"></a>    <span class="co"># T² statistic</span></span>
<span id="cb88-17"><a href="hypothesis-tests-on-mean-vectors.html#cb88-17" tabindex="-1"></a>    diff <span class="op">=</span> mean1 <span class="op">-</span> mean2</span>
<span id="cb88-18"><a href="hypothesis-tests-on-mean-vectors.html#cb88-18" tabindex="-1"></a>    t2 <span class="op">=</span> (n1 <span class="op">*</span> n2) <span class="op">/</span> (n1 <span class="op">+</span> n2) <span class="op">*</span> diff.T <span class="op">@</span> np.linalg.inv(spooled) <span class="op">@</span> diff</span>
<span id="cb88-19"><a href="hypothesis-tests-on-mean-vectors.html#cb88-19" tabindex="-1"></a>    <span class="co"># Convert to F-statistic</span></span>
<span id="cb88-20"><a href="hypothesis-tests-on-mean-vectors.html#cb88-20" tabindex="-1"></a>    f_stat <span class="op">=</span> (n1 <span class="op">+</span> n2 <span class="op">-</span> p <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> t2 <span class="op">/</span> (p <span class="op">*</span> (n1 <span class="op">+</span> n2 <span class="op">-</span> <span class="dv">2</span>))</span>
<span id="cb88-21"><a href="hypothesis-tests-on-mean-vectors.html#cb88-21" tabindex="-1"></a>    df1 <span class="op">=</span> p</span>
<span id="cb88-22"><a href="hypothesis-tests-on-mean-vectors.html#cb88-22" tabindex="-1"></a>    df2 <span class="op">=</span> n1 <span class="op">+</span> n2 <span class="op">-</span> p <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb88-23"><a href="hypothesis-tests-on-mean-vectors.html#cb88-23" tabindex="-1"></a>    p_value <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> f.cdf(f_stat, df1, df2)</span>
<span id="cb88-24"><a href="hypothesis-tests-on-mean-vectors.html#cb88-24" tabindex="-1"></a></span>
<span id="cb88-25"><a href="hypothesis-tests-on-mean-vectors.html#cb88-25" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb88-26"><a href="hypothesis-tests-on-mean-vectors.html#cb88-26" tabindex="-1"></a>        <span class="st">&#39;T2&#39;</span>: t2,</span>
<span id="cb88-27"><a href="hypothesis-tests-on-mean-vectors.html#cb88-27" tabindex="-1"></a>        <span class="st">&#39;F&#39;</span>: f_stat,</span>
<span id="cb88-28"><a href="hypothesis-tests-on-mean-vectors.html#cb88-28" tabindex="-1"></a>        <span class="st">&#39;df1&#39;</span>: df1,</span>
<span id="cb88-29"><a href="hypothesis-tests-on-mean-vectors.html#cb88-29" tabindex="-1"></a>        <span class="st">&#39;df2&#39;</span>: df2,</span>
<span id="cb88-30"><a href="hypothesis-tests-on-mean-vectors.html#cb88-30" tabindex="-1"></a>        <span class="st">&#39;p_value&#39;</span>: p_value</span>
<span id="cb88-31"><a href="hypothesis-tests-on-mean-vectors.html#cb88-31" tabindex="-1"></a>    }</span>
<span id="cb88-32"><a href="hypothesis-tests-on-mean-vectors.html#cb88-32" tabindex="-1"></a></span>
<span id="cb88-33"><a href="hypothesis-tests-on-mean-vectors.html#cb88-33" tabindex="-1"></a>bird <span class="op">=</span> pd.read_csv(<span class="st">&quot;data/bumpus.txt&quot;</span>, sep<span class="op">=</span><span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>)  <span class="co"># or sep=&quot;,&quot; depending on file</span></span>
<span id="cb88-34"><a href="hypothesis-tests-on-mean-vectors.html#cb88-34" tabindex="-1"></a></span>
<span id="cb88-35"><a href="hypothesis-tests-on-mean-vectors.html#cb88-35" tabindex="-1"></a>xmat1 <span class="op">=</span> bird.iloc[<span class="dv">0</span>:<span class="dv">21</span>, <span class="dv">1</span>:].to_numpy()   <span class="co"># survivors: exclude ID column</span></span>
<span id="cb88-36"><a href="hypothesis-tests-on-mean-vectors.html#cb88-36" tabindex="-1"></a>xmat2 <span class="op">=</span> bird.iloc[<span class="dv">21</span>:, <span class="dv">1</span>:].to_numpy()    <span class="co"># non-survivors</span></span>
<span id="cb88-37"><a href="hypothesis-tests-on-mean-vectors.html#cb88-37" tabindex="-1"></a></span>
<span id="cb88-38"><a href="hypothesis-tests-on-mean-vectors.html#cb88-38" tabindex="-1"></a>result <span class="op">=</span> hotellings_t2(xmat1, xmat2)</span>
<span id="cb88-39"><a href="hypothesis-tests-on-mean-vectors.html#cb88-39" tabindex="-1"></a></span>
<span id="cb88-40"><a href="hypothesis-tests-on-mean-vectors.html#cb88-40" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Hotelling&#39;s T²: </span><span class="sc">{</span>result[<span class="st">&#39;T2&#39;</span>]<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## Hotelling&#39;s T²: 0.0000</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="hypothesis-tests-on-mean-vectors.html#cb90-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;F-statistic: </span><span class="sc">{</span>result[<span class="st">&#39;F&#39;</span>]<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## F-statistic: nan</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="hypothesis-tests-on-mean-vectors.html#cb92-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Degrees of freedom: (</span><span class="sc">{</span>result[<span class="st">&#39;df1&#39;</span>]<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>result[<span class="st">&#39;df2&#39;</span>]<span class="sc">}</span><span class="ss">)&quot;</span>)</span></code></pre></div>
<pre><code>## Degrees of freedom: (0, 48)</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="hypothesis-tests-on-mean-vectors.html#cb94-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;P-value: </span><span class="sc">{</span>result[<span class="st">&#39;p_value&#39;</span>]<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## P-value: nan</code></pre>
<p></p>
<p>The admission data set contains 397 graduate school admissions decisions, among which <span class="math inline">\(n_1=271\)</span> were not admitted and <span class="math inline">\(n_2=126\)</span> were admitted. The explanatory variables are the student’s GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution (values from 1 to 4 with 1 having the highest prestige). The response variable, admit/don’t admit, is a binary variable with 1=admitted and 0=not admitted. Test at the 1% significance level whether the non-admitted and admitted students have different GRE and/or GPA scores.</p>
<p>We first check the assumptions of the test.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="hypothesis-tests-on-mean-vectors.html#cb96-1" tabindex="-1"></a><span class="kw">def</span> chi_squared_qq(x, ax<span class="op">=</span><span class="va">None</span>, title<span class="op">=</span><span class="st">&quot;&quot;</span>):</span>
<span id="cb96-2"><a href="hypothesis-tests-on-mean-vectors.html#cb96-2" tabindex="-1"></a>    n, p <span class="op">=</span> x.shape</span>
<span id="cb96-3"><a href="hypothesis-tests-on-mean-vectors.html#cb96-3" tabindex="-1"></a>    pvec <span class="op">=</span> (np.arange(<span class="dv">1</span>, n <span class="op">+</span> <span class="dv">1</span>) <span class="op">-</span> <span class="fl">0.5</span>) <span class="op">/</span> n</span>
<span id="cb96-4"><a href="hypothesis-tests-on-mean-vectors.html#cb96-4" tabindex="-1"></a>    qvec <span class="op">=</span> chi2.ppf(pvec, df<span class="op">=</span>p)</span>
<span id="cb96-5"><a href="hypothesis-tests-on-mean-vectors.html#cb96-5" tabindex="-1"></a></span>
<span id="cb96-6"><a href="hypothesis-tests-on-mean-vectors.html#cb96-6" tabindex="-1"></a>    mvec <span class="op">=</span> x.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb96-7"><a href="hypothesis-tests-on-mean-vectors.html#cb96-7" tabindex="-1"></a>    smat <span class="op">=</span> np.cov(x, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb96-8"><a href="hypothesis-tests-on-mean-vectors.html#cb96-8" tabindex="-1"></a>    inv_smat <span class="op">=</span> np.linalg.inv(smat)</span>
<span id="cb96-9"><a href="hypothesis-tests-on-mean-vectors.html#cb96-9" tabindex="-1"></a></span>
<span id="cb96-10"><a href="hypothesis-tests-on-mean-vectors.html#cb96-10" tabindex="-1"></a>    dvec <span class="op">=</span> np.array([mahalanobis(row, mvec, inv_smat)<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> row <span class="kw">in</span> x])</span>
<span id="cb96-11"><a href="hypothesis-tests-on-mean-vectors.html#cb96-11" tabindex="-1"></a>    dvec_sorted <span class="op">=</span> np.sort(dvec)</span>
<span id="cb96-12"><a href="hypothesis-tests-on-mean-vectors.html#cb96-12" tabindex="-1"></a></span>
<span id="cb96-13"><a href="hypothesis-tests-on-mean-vectors.html#cb96-13" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb96-14"><a href="hypothesis-tests-on-mean-vectors.html#cb96-14" tabindex="-1"></a>        ax <span class="op">=</span> plt.gca()</span>
<span id="cb96-15"><a href="hypothesis-tests-on-mean-vectors.html#cb96-15" tabindex="-1"></a>    ax.scatter(dvec_sorted, qvec, c<span class="op">=</span><span class="st">&#39;black&#39;</span>)</span>
<span id="cb96-16"><a href="hypothesis-tests-on-mean-vectors.html#cb96-16" tabindex="-1"></a>    ax.plot([<span class="dv">0</span>, <span class="bu">max</span>(dvec_sorted)], [<span class="dv">0</span>, <span class="bu">max</span>(qvec)], color<span class="op">=</span><span class="st">&#39;red&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb96-17"><a href="hypothesis-tests-on-mean-vectors.html#cb96-17" tabindex="-1"></a>    ax.set_xlabel(<span class="st">&quot;Observed Quantile&quot;</span>)</span>
<span id="cb96-18"><a href="hypothesis-tests-on-mean-vectors.html#cb96-18" tabindex="-1"></a>    ax.set_ylabel(<span class="st">&quot;Theoretical Quantile&quot;</span>)</span>
<span id="cb96-19"><a href="hypothesis-tests-on-mean-vectors.html#cb96-19" tabindex="-1"></a>    ax.set_title(title)</span>
<span id="cb96-20"><a href="hypothesis-tests-on-mean-vectors.html#cb96-20" tabindex="-1"></a></span>
<span id="cb96-21"><a href="hypothesis-tests-on-mean-vectors.html#cb96-21" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;data/admission.csv&quot;</span>)</span>
<span id="cb96-22"><a href="hypothesis-tests-on-mean-vectors.html#cb96-22" tabindex="-1"></a>df <span class="op">=</span> df.dropna()</span>
<span id="cb96-23"><a href="hypothesis-tests-on-mean-vectors.html#cb96-23" tabindex="-1"></a>adf0 <span class="op">=</span> df[df[<span class="st">&#39;admit&#39;</span>] <span class="op">==</span> <span class="dv">0</span>][[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]]  <span class="co"># Non-admitted</span></span>
<span id="cb96-24"><a href="hypothesis-tests-on-mean-vectors.html#cb96-24" tabindex="-1"></a>adf1 <span class="op">=</span> df[df[<span class="st">&#39;admit&#39;</span>] <span class="op">==</span> <span class="dv">1</span>][[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]]  <span class="co"># Admitted</span></span>
<span id="cb96-25"><a href="hypothesis-tests-on-mean-vectors.html#cb96-25" tabindex="-1"></a></span>
<span id="cb96-26"><a href="hypothesis-tests-on-mean-vectors.html#cb96-26" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb96-27"><a href="hypothesis-tests-on-mean-vectors.html#cb96-27" tabindex="-1"></a>chi_squared_qq(adf0.to_numpy(), ax<span class="op">=</span>axes[<span class="dv">0</span>], title<span class="op">=</span><span class="st">&quot;Chi-square Q-Q Plot for Non-Admitted&quot;</span>)</span>
<span id="cb96-28"><a href="hypothesis-tests-on-mean-vectors.html#cb96-28" tabindex="-1"></a>chi_squared_qq(adf1.to_numpy(), ax<span class="op">=</span>axes[<span class="dv">1</span>], title<span class="op">=</span><span class="st">&quot;Chi-square Q-Q Plot for Admitted&quot;</span>)</span>
<span id="cb96-29"><a href="hypothesis-tests-on-mean-vectors.html#cb96-29" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb96-30"><a href="hypothesis-tests-on-mean-vectors.html#cb96-30" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="Plots/chiassumpt-19.png" width="1152" /></p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="hypothesis-tests-on-mean-vectors.html#cb97-1" tabindex="-1"></a>plt.close()</span></code></pre></div>
<p>Both chi-square Q-Q plots do not show strong evidence against the bivariate normality assumption for both the non-admitted and admitted groups. We use Box’s M-test for Homogeneity of Covariance Matrices with <span class="math inline">\(H_0: \boldsymbol{\Sigma}_1=\boldsymbol{\Sigma}_2\)</span>.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="hypothesis-tests-on-mean-vectors.html#cb98-1" tabindex="-1"></a><span class="co"># manual box test</span></span>
<span id="cb98-2"><a href="hypothesis-tests-on-mean-vectors.html#cb98-2" tabindex="-1"></a><span class="kw">def</span> box_m_test(df, group_col):</span>
<span id="cb98-3"><a href="hypothesis-tests-on-mean-vectors.html#cb98-3" tabindex="-1"></a>    groups <span class="op">=</span> df[group_col].unique()</span>
<span id="cb98-4"><a href="hypothesis-tests-on-mean-vectors.html#cb98-4" tabindex="-1"></a>    k <span class="op">=</span> <span class="bu">len</span>(groups)</span>
<span id="cb98-5"><a href="hypothesis-tests-on-mean-vectors.html#cb98-5" tabindex="-1"></a>    n_total <span class="op">=</span> <span class="bu">len</span>(df)</span>
<span id="cb98-6"><a href="hypothesis-tests-on-mean-vectors.html#cb98-6" tabindex="-1"></a>    </span>
<span id="cb98-7"><a href="hypothesis-tests-on-mean-vectors.html#cb98-7" tabindex="-1"></a>    group_data <span class="op">=</span> [df[df[group_col] <span class="op">==</span> g].drop(columns<span class="op">=</span>group_col).values <span class="cf">for</span> g <span class="kw">in</span> groups]</span>
<span id="cb98-8"><a href="hypothesis-tests-on-mean-vectors.html#cb98-8" tabindex="-1"></a>    ns <span class="op">=</span> [<span class="bu">len</span>(g) <span class="cf">for</span> g <span class="kw">in</span> group_data]</span>
<span id="cb98-9"><a href="hypothesis-tests-on-mean-vectors.html#cb98-9" tabindex="-1"></a>    covs <span class="op">=</span> [np.cov(g, rowvar<span class="op">=</span><span class="va">False</span>) <span class="cf">for</span> g <span class="kw">in</span> group_data]</span>
<span id="cb98-10"><a href="hypothesis-tests-on-mean-vectors.html#cb98-10" tabindex="-1"></a>    pooled_cov <span class="op">=</span> <span class="bu">sum</span>([(ns[i] <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> covs[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)]) <span class="op">/</span> (n_total <span class="op">-</span> k)</span>
<span id="cb98-11"><a href="hypothesis-tests-on-mean-vectors.html#cb98-11" tabindex="-1"></a></span>
<span id="cb98-12"><a href="hypothesis-tests-on-mean-vectors.html#cb98-12" tabindex="-1"></a>    p <span class="op">=</span> pooled_cov.shape[<span class="dv">0</span>]</span>
<span id="cb98-13"><a href="hypothesis-tests-on-mean-vectors.html#cb98-13" tabindex="-1"></a>    M <span class="op">=</span> <span class="bu">sum</span>([(ns[i] <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> np.log(np.linalg.det(covs[i])) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)]) <span class="op">-</span> (n_total <span class="op">-</span> k) <span class="op">*</span> np.log(np.linalg.det(pooled_cov))</span>
<span id="cb98-14"><a href="hypothesis-tests-on-mean-vectors.html#cb98-14" tabindex="-1"></a></span>
<span id="cb98-15"><a href="hypothesis-tests-on-mean-vectors.html#cb98-15" tabindex="-1"></a>    C <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> (<span class="dv">1</span> <span class="op">/</span> (<span class="dv">3</span> <span class="op">*</span> (k <span class="op">-</span> <span class="dv">1</span>))) <span class="op">*</span> (<span class="bu">sum</span>([<span class="dv">1</span> <span class="op">/</span> (ns[i] <span class="op">-</span> <span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)]) <span class="op">-</span> <span class="dv">1</span> <span class="op">/</span> (n_total <span class="op">-</span> k)))  <span class="co"># Correction factor</span></span>
<span id="cb98-16"><a href="hypothesis-tests-on-mean-vectors.html#cb98-16" tabindex="-1"></a>    chi2_stat <span class="op">=</span> M <span class="op">*</span> C</span>
<span id="cb98-17"><a href="hypothesis-tests-on-mean-vectors.html#cb98-17" tabindex="-1"></a>    df_box <span class="op">=</span> (p <span class="op">*</span> (p <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> (k <span class="op">-</span> <span class="dv">1</span>)) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb98-18"><a href="hypothesis-tests-on-mean-vectors.html#cb98-18" tabindex="-1"></a>    p_value <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> chi2.cdf(chi2_stat, df_box)</span>
<span id="cb98-19"><a href="hypothesis-tests-on-mean-vectors.html#cb98-19" tabindex="-1"></a></span>
<span id="cb98-20"><a href="hypothesis-tests-on-mean-vectors.html#cb98-20" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb98-21"><a href="hypothesis-tests-on-mean-vectors.html#cb98-21" tabindex="-1"></a>        <span class="st">&quot;Chi-squared&quot;</span>: chi2_stat,</span>
<span id="cb98-22"><a href="hypothesis-tests-on-mean-vectors.html#cb98-22" tabindex="-1"></a>        <span class="st">&quot;df&quot;</span>: df_box,</span>
<span id="cb98-23"><a href="hypothesis-tests-on-mean-vectors.html#cb98-23" tabindex="-1"></a>        <span class="st">&quot;p-value&quot;</span>: p_value</span>
<span id="cb98-24"><a href="hypothesis-tests-on-mean-vectors.html#cb98-24" tabindex="-1"></a>    }</span>
<span id="cb98-25"><a href="hypothesis-tests-on-mean-vectors.html#cb98-25" tabindex="-1"></a>    </span>
<span id="cb98-26"><a href="hypothesis-tests-on-mean-vectors.html#cb98-26" tabindex="-1"></a>subset_df <span class="op">=</span> df[[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>, <span class="st">&#39;admit&#39;</span>]]</span>
<span id="cb98-27"><a href="hypothesis-tests-on-mean-vectors.html#cb98-27" tabindex="-1"></a>result <span class="op">=</span> box_m_test(subset_df, group_col<span class="op">=</span><span class="st">&#39;admit&#39;</span>)</span>
<span id="cb98-28"><a href="hypothesis-tests-on-mean-vectors.html#cb98-28" tabindex="-1"></a></span>
<span id="cb98-29"><a href="hypothesis-tests-on-mean-vectors.html#cb98-29" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Chi-squared (approx.):&quot;</span>, result[<span class="st">&quot;Chi-squared&quot;</span>])</span></code></pre></div>
<pre><code>## Chi-squared (approx.): -3.93145712583418</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="hypothesis-tests-on-mean-vectors.html#cb100-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;df:&quot;</span>, result[<span class="st">&quot;df&quot;</span>])</span></code></pre></div>
<pre><code>## df: 3</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="hypothesis-tests-on-mean-vectors.html#cb102-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;p-value:&quot;</span>, result[<span class="st">&quot;p-value&quot;</span>])</span></code></pre></div>
<pre><code>## p-value: 1.0</code></pre>
<p>The P-value of the Box’s M-test is <span class="math inline">\(0.2732\)</span>, so there is not sufficient evidence to reject <span class="math inline">\(H_0\)</span>, we could use the pooled procedure.</p>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Hypotheses. <span class="math inline">\(H_0: \boldsymbol{\mu_1}-\boldsymbol{\mu_2}=\mathbf{0}\)</span>, <span class="math inline">\(H_a: \mbox{at least one } \mu_{1i}\ne \mu_{2i}, i=1, 2\)</span>.</li>
<li>Significance level <span class="math inline">\(\alpha=0.01\)</span>.</li>
<li>Test statistic. First find the sample mean vectors and sample covariance matrices.
<span class="math display">\[
\mathbf{\bar x_1}=\left[
\begin{array}{c}
573.579\\
3.347
\end{array}
\right], \mathbf{S_1}=\left[
\begin{array}{rrrrr}
3468.252&amp; 18.265\\
18.265 &amp; 0.1412
\end{array}
\right]
\]</span>
<span class="math display">\[
\mathbf{\bar x_2}=\left[
\begin{array}{c}
618.571\\
3.4892
\end{array}
\right], \mathbf{S_2}=\left[
\begin{array}{rrrrr}
11937.143&amp; 9.452\\
9.452&amp; 0.138
\end{array}
\right]
\]</span></li>
</ol>
<p>The pooled covariance matrix is
<span class="math display">\[
\mathbf{S}_{\mbox{pooled}}=\left[
\begin{array}{rrrrr}
12983.724&amp; 15.476\\
15.476&amp;  0.141
\end{array}
\right]
\]</span>
and
<span class="math display">\[
\left[\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\mathbf{S}_{\mbox{pooled}}\right]^{-1}=\left[
\begin{array}{rrrrr}
0.007625356&amp;  -0.8397307\\
-0.839730679&amp; 704.5116072
\end{array}
\right],
\]</span>
<span class="math display">\[
\begin{aligned}
T^2&amp;=[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-(\boldsymbol{\mu_1}-\boldsymbol{\mu_2})]^{T}\left[\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\mathbf{S}_{\mbox{pooled}}\right]^{-1}[(\mathbf{\bar x_1}-\mathbf{\bar x_2})-(\boldsymbol{\mu_1}-\boldsymbol{\mu_2})]\\
&amp;=[-44.992, -0.142]\left[
\begin{array}{rrrrr}
0.007625356&amp;  -0.8397307\\
-0.839730679&amp; 704.5116072
\end{array}
\right] \left[
\begin{array}{c}
-44.992\\
-0.142
\end{array}
\right]\\
&amp;=18.918
\end{aligned}
\]</span></p>
<p><span class="math inline">\(F_o=\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2=\hspace{10cm}\)</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li>P-value or rejection region.</li>
<li>Decision:</li>
<li>Conclusion:</li>
</ol>
<p>Double check with <span class="math inline">\(\textsf{R}.\)</span></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="hypothesis-tests-on-mean-vectors.html#cb104-1" tabindex="-1"></a>X <span class="op">=</span> adf0[[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]].values  <span class="co"># Non-admitted</span></span>
<span id="cb104-2"><a href="hypothesis-tests-on-mean-vectors.html#cb104-2" tabindex="-1"></a>Y <span class="op">=</span> adf1[[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]].values  <span class="co"># Admitted</span></span>
<span id="cb104-3"><a href="hypothesis-tests-on-mean-vectors.html#cb104-3" tabindex="-1"></a></span>
<span id="cb104-4"><a href="hypothesis-tests-on-mean-vectors.html#cb104-4" tabindex="-1"></a>result <span class="op">=</span> hotellings_t2(X, Y) <span class="co"># previous function</span></span>
<span id="cb104-5"><a href="hypothesis-tests-on-mean-vectors.html#cb104-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;T²:&quot;</span>, result[<span class="st">&#39;T2&#39;</span>], <span class="st">&quot;</span><span class="ch">\n</span><span class="st">F-statistic:&quot;</span>, result[<span class="st">&#39;F&#39;</span>],<span class="st">&quot;</span><span class="ch">\n</span><span class="st">df1:&quot;</span>, result[<span class="st">&#39;df1&#39;</span>],<span class="st">&quot;</span><span class="ch">\n</span><span class="st">df2:&quot;</span>, result[<span class="st">&#39;df2&#39;</span>],<span class="st">&quot;</span><span class="ch">\n</span><span class="st">p-value:&quot;</span>, result[<span class="st">&#39;p_value&#39;</span>])</span></code></pre></div>
<pre><code>## T²: 18.91775855050124 
## F-statistic: 9.434932745439859 
## df1: 2 
## df2: 394 
## p-value: 9.943930954481317e-05</code></pre>
<p>Obtain 99% Bonferroni intervals for the mean difference in GRE and GPA.
<span class="math display">\[
(\bar x_{1i}-\bar x_{2i}) \pm t_{n_1+n_2-2}(\frac{\alpha}{2p})\sqrt{s_{\mbox{ ${pooled}$ },i}^2(\frac{1}{n_1}+\frac{1}{n_2})}, i=1, 2
\]</span></p>
<p>We can use <span class="math inline">\(\textsf{R}\)</span> to obtain a Bonferroni interval.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="hypothesis-tests-on-mean-vectors.html#cb106-1" tabindex="-1"></a><span class="kw">def</span> simultaneous_intervals_two_sample(x, y, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb106-2"><a href="hypothesis-tests-on-mean-vectors.html#cb106-2" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb106-3"><a href="hypothesis-tests-on-mean-vectors.html#cb106-3" tabindex="-1"></a><span class="co">    Simultaneous confidence intervals for the difference between two mean vectors (pooled).</span></span>
<span id="cb106-4"><a href="hypothesis-tests-on-mean-vectors.html#cb106-4" tabindex="-1"></a><span class="co">    x, y: n1 x p and n2 x p numpy arrays</span></span>
<span id="cb106-5"><a href="hypothesis-tests-on-mean-vectors.html#cb106-5" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb106-6"><a href="hypothesis-tests-on-mean-vectors.html#cb106-6" tabindex="-1"></a>    n1, p <span class="op">=</span> x.shape</span>
<span id="cb106-7"><a href="hypothesis-tests-on-mean-vectors.html#cb106-7" tabindex="-1"></a>    n2, _ <span class="op">=</span> y.shape</span>
<span id="cb106-8"><a href="hypothesis-tests-on-mean-vectors.html#cb106-8" tabindex="-1"></a>    xm <span class="op">=</span> np.mean(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb106-9"><a href="hypothesis-tests-on-mean-vectors.html#cb106-9" tabindex="-1"></a>    ym <span class="op">=</span> np.mean(y, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb106-10"><a href="hypothesis-tests-on-mean-vectors.html#cb106-10" tabindex="-1"></a>    xs <span class="op">=</span> np.std(x, axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb106-11"><a href="hypothesis-tests-on-mean-vectors.html#cb106-11" tabindex="-1"></a>    ys <span class="op">=</span> np.std(y, axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb106-12"><a href="hypothesis-tests-on-mean-vectors.html#cb106-12" tabindex="-1"></a>    sp <span class="op">=</span> np.sqrt(((n1 <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> xs<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (n2 <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> ys<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (n1 <span class="op">+</span> n2 <span class="op">-</span> <span class="dv">2</span>))</span>
<span id="cb106-13"><a href="hypothesis-tests-on-mean-vectors.html#cb106-13" tabindex="-1"></a>    fscore <span class="op">=</span> f.ppf(<span class="dv">1</span> <span class="op">-</span> alpha, dfn<span class="op">=</span>p, dfd<span class="op">=</span>n1 <span class="op">+</span> n2 <span class="op">-</span> p <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb106-14"><a href="hypothesis-tests-on-mean-vectors.html#cb106-14" tabindex="-1"></a>    multiplier <span class="op">=</span> np.sqrt(fscore <span class="op">*</span> (n1 <span class="op">+</span> n2 <span class="op">-</span> <span class="dv">2</span>) <span class="op">*</span> p <span class="op">/</span> (n1 <span class="op">+</span> n2 <span class="op">-</span> p <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb106-15"><a href="hypothesis-tests-on-mean-vectors.html#cb106-15" tabindex="-1"></a>    margin <span class="op">=</span> multiplier <span class="op">*</span> sp <span class="op">*</span> np.sqrt(<span class="dv">1</span><span class="op">/</span>n1 <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>n2)</span>
<span id="cb106-16"><a href="hypothesis-tests-on-mean-vectors.html#cb106-16" tabindex="-1"></a>    diff_mean <span class="op">=</span> xm <span class="op">-</span> ym</span>
<span id="cb106-17"><a href="hypothesis-tests-on-mean-vectors.html#cb106-17" tabindex="-1"></a>    lvec <span class="op">=</span> diff_mean <span class="op">-</span> margin</span>
<span id="cb106-18"><a href="hypothesis-tests-on-mean-vectors.html#cb106-18" tabindex="-1"></a>    uvec <span class="op">=</span> diff_mean <span class="op">+</span> margin</span>
<span id="cb106-19"><a href="hypothesis-tests-on-mean-vectors.html#cb106-19" tabindex="-1"></a>    <span class="cf">return</span> np.column_stack((lvec, uvec))</span>
<span id="cb106-20"><a href="hypothesis-tests-on-mean-vectors.html#cb106-20" tabindex="-1"></a></span>
<span id="cb106-21"><a href="hypothesis-tests-on-mean-vectors.html#cb106-21" tabindex="-1"></a></span>
<span id="cb106-22"><a href="hypothesis-tests-on-mean-vectors.html#cb106-22" tabindex="-1"></a><span class="kw">def</span> bonferroni_intervals_two_sample(x, y, alpha<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb106-23"><a href="hypothesis-tests-on-mean-vectors.html#cb106-23" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb106-24"><a href="hypothesis-tests-on-mean-vectors.html#cb106-24" tabindex="-1"></a><span class="co">    Bonferroni corrected confidence intervals for the difference between two mean vectors.</span></span>
<span id="cb106-25"><a href="hypothesis-tests-on-mean-vectors.html#cb106-25" tabindex="-1"></a><span class="co">    Returns pooled and non-pooled versions.</span></span>
<span id="cb106-26"><a href="hypothesis-tests-on-mean-vectors.html#cb106-26" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb106-27"><a href="hypothesis-tests-on-mean-vectors.html#cb106-27" tabindex="-1"></a>    n1, p <span class="op">=</span> x.shape</span>
<span id="cb106-28"><a href="hypothesis-tests-on-mean-vectors.html#cb106-28" tabindex="-1"></a>    n2, _ <span class="op">=</span> y.shape</span>
<span id="cb106-29"><a href="hypothesis-tests-on-mean-vectors.html#cb106-29" tabindex="-1"></a>    xm <span class="op">=</span> np.mean(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb106-30"><a href="hypothesis-tests-on-mean-vectors.html#cb106-30" tabindex="-1"></a>    ym <span class="op">=</span> np.mean(y, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb106-31"><a href="hypothesis-tests-on-mean-vectors.html#cb106-31" tabindex="-1"></a>    xs <span class="op">=</span> np.std(x, axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb106-32"><a href="hypothesis-tests-on-mean-vectors.html#cb106-32" tabindex="-1"></a>    ys <span class="op">=</span> np.std(y, axis<span class="op">=</span><span class="dv">0</span>, ddof<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb106-33"><a href="hypothesis-tests-on-mean-vectors.html#cb106-33" tabindex="-1"></a>    </span>
<span id="cb106-34"><a href="hypothesis-tests-on-mean-vectors.html#cb106-34" tabindex="-1"></a>    <span class="co"># Pooled version</span></span>
<span id="cb106-35"><a href="hypothesis-tests-on-mean-vectors.html#cb106-35" tabindex="-1"></a>    sp <span class="op">=</span> np.sqrt(((n1 <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> xs<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (n2 <span class="op">-</span> <span class="dv">1</span>) <span class="op">*</span> ys<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (n1 <span class="op">+</span> n2 <span class="op">-</span> <span class="dv">2</span>))</span>
<span id="cb106-36"><a href="hypothesis-tests-on-mean-vectors.html#cb106-36" tabindex="-1"></a>    tscore_pool <span class="op">=</span> t.ppf(<span class="dv">1</span> <span class="op">-</span> alpha <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> p), df<span class="op">=</span>n1 <span class="op">+</span> n2 <span class="op">-</span> <span class="dv">2</span>)</span>
<span id="cb106-37"><a href="hypothesis-tests-on-mean-vectors.html#cb106-37" tabindex="-1"></a>    margin_pool <span class="op">=</span> tscore_pool <span class="op">*</span> sp <span class="op">*</span> np.sqrt(<span class="dv">1</span><span class="op">/</span>n1 <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>n2)</span>
<span id="cb106-38"><a href="hypothesis-tests-on-mean-vectors.html#cb106-38" tabindex="-1"></a>    diff_mean <span class="op">=</span> xm <span class="op">-</span> ym</span>
<span id="cb106-39"><a href="hypothesis-tests-on-mean-vectors.html#cb106-39" tabindex="-1"></a>    lvec1 <span class="op">=</span> diff_mean <span class="op">-</span> margin_pool</span>
<span id="cb106-40"><a href="hypothesis-tests-on-mean-vectors.html#cb106-40" tabindex="-1"></a>    uvec1 <span class="op">=</span> diff_mean <span class="op">+</span> margin_pool</span>
<span id="cb106-41"><a href="hypothesis-tests-on-mean-vectors.html#cb106-41" tabindex="-1"></a>    </span>
<span id="cb106-42"><a href="hypothesis-tests-on-mean-vectors.html#cb106-42" tabindex="-1"></a>    <span class="co"># Non-pooled version (Welch-like)</span></span>
<span id="cb106-43"><a href="hypothesis-tests-on-mean-vectors.html#cb106-43" tabindex="-1"></a>    df <span class="op">=</span> (xs<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> n1 <span class="op">+</span> ys<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> n2)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (</span>
<span id="cb106-44"><a href="hypothesis-tests-on-mean-vectors.html#cb106-44" tabindex="-1"></a>        (xs<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> n1)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (n1 <span class="op">-</span> <span class="dv">1</span>) <span class="op">+</span> (ys<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> n2)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (n2 <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb106-45"><a href="hypothesis-tests-on-mean-vectors.html#cb106-45" tabindex="-1"></a>    )</span>
<span id="cb106-46"><a href="hypothesis-tests-on-mean-vectors.html#cb106-46" tabindex="-1"></a>    tscore_nonpool <span class="op">=</span> t.ppf(<span class="dv">1</span> <span class="op">-</span> alpha <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> p), df<span class="op">=</span>df)</span>
<span id="cb106-47"><a href="hypothesis-tests-on-mean-vectors.html#cb106-47" tabindex="-1"></a>    margin_nonpool <span class="op">=</span> tscore_nonpool <span class="op">*</span> np.sqrt(xs<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> n1 <span class="op">+</span> ys<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> n2)</span>
<span id="cb106-48"><a href="hypothesis-tests-on-mean-vectors.html#cb106-48" tabindex="-1"></a>    lvec2 <span class="op">=</span> diff_mean <span class="op">-</span> margin_nonpool</span>
<span id="cb106-49"><a href="hypothesis-tests-on-mean-vectors.html#cb106-49" tabindex="-1"></a>    uvec2 <span class="op">=</span> diff_mean <span class="op">+</span> margin_nonpool</span>
<span id="cb106-50"><a href="hypothesis-tests-on-mean-vectors.html#cb106-50" tabindex="-1"></a>    </span>
<span id="cb106-51"><a href="hypothesis-tests-on-mean-vectors.html#cb106-51" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb106-52"><a href="hypothesis-tests-on-mean-vectors.html#cb106-52" tabindex="-1"></a>        <span class="st">&#39;pooled&#39;</span>: np.column_stack((lvec1, uvec1)),</span>
<span id="cb106-53"><a href="hypothesis-tests-on-mean-vectors.html#cb106-53" tabindex="-1"></a>        <span class="st">&#39;non_pooled&#39;</span>: np.column_stack((lvec2, uvec2))</span>
<span id="cb106-54"><a href="hypothesis-tests-on-mean-vectors.html#cb106-54" tabindex="-1"></a>    }</span>
<span id="cb106-55"><a href="hypothesis-tests-on-mean-vectors.html#cb106-55" tabindex="-1"></a></span>
<span id="cb106-56"><a href="hypothesis-tests-on-mean-vectors.html#cb106-56" tabindex="-1"></a>X <span class="op">=</span> adf0[[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]].to_numpy()</span>
<span id="cb106-57"><a href="hypothesis-tests-on-mean-vectors.html#cb106-57" tabindex="-1"></a>Y <span class="op">=</span> adf1[[<span class="st">&#39;gre&#39;</span>, <span class="st">&#39;gpa&#39;</span>]].to_numpy()</span>
<span id="cb106-58"><a href="hypothesis-tests-on-mean-vectors.html#cb106-58" tabindex="-1"></a></span>
<span id="cb106-59"><a href="hypothesis-tests-on-mean-vectors.html#cb106-59" tabindex="-1"></a>simul_ci <span class="op">=</span> simultaneous_intervals_two_sample(X, Y, alpha<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb106-60"><a href="hypothesis-tests-on-mean-vectors.html#cb106-60" tabindex="-1"></a>bonf_ci <span class="op">=</span> bonferroni_intervals_two_sample(X, Y, alpha<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb106-61"><a href="hypothesis-tests-on-mean-vectors.html#cb106-61" tabindex="-1"></a></span>
<span id="cb106-62"><a href="hypothesis-tests-on-mean-vectors.html#cb106-62" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Simultaneous CI (pooled):</span><span class="ch">\n</span><span class="st">&quot;</span>, simul_ci)</span></code></pre></div>
<pre><code>## Simultaneous CI (pooled):
##  [[-8.25460980e+01 -7.43808761e+00]
##  [-2.65597537e-01 -1.84978178e-02]]</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="hypothesis-tests-on-mean-vectors.html#cb108-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bonferroni CI (pooled):</span><span class="ch">\n</span><span class="st">&quot;</span>, bonf_ci[<span class="st">&#39;pooled&#39;</span>])</span></code></pre></div>
<pre><code>## Bonferroni CI (pooled):
##  [[-7.96752949e+01 -1.03088906e+01]
##  [-2.56152811e-01 -2.79425444e-02]]</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="hypothesis-tests-on-mean-vectors.html#cb110-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Bonferroni CI (non-pooled):</span><span class="ch">\n</span><span class="st">&quot;</span>, bonf_ci[<span class="st">&#39;non_pooled&#39;</span>])</span></code></pre></div>
<pre><code>## Bonferroni CI (non-pooled):
##  [[-7.90205677e+01 -1.09636179e+01]
##  [-2.56015917e-01 -2.80794381e-02]]</code></pre>
</div>
<div id="univariate-case-based-on-a-paired-sample" class="section level3 hasAnchor" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> Univariate Case Based on a Paired Sample<a href="hypothesis-tests-on-mean-vectors.html#univariate-case-based-on-a-paired-sample" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two samples are considered <span class="math inline">\(\textit{paired}\)</span> if each observation in the first sample is related to an observation in the second sample. In univariate case, a paired <span class="math inline">\(t\)</span> test is exactly a one-sample <span class="math inline">\(t\)</span> test on the <span class="math inline">\(\textbf{paired differences}\)</span> <span class="math inline">\(d_i\)</span>. The hypotheses are <span class="math inline">\(H_0: \mu_d=\delta_0\)</span> versus <span class="math inline">\(H_a: \mu_d\ne \delta_0\)</span>.</p>
<p>$: $</p>
<ul>
<li>simple random paired sample;</li>
<li>the paired difference has a normal distribution or large number of pairs.</li>
</ul>
<p>The test statistic is
<span class="math display">\[
t_o=\frac{\bar d-\delta_0}{\frac{s_d}{\sqrt{n}}}, \quad \bar d=\frac{\sum d_i}{n}, \quad s_d=\sqrt{\frac{\sum(d_i-\bar d)^2}{n-1}}=\sqrt{\frac{\sum d_i^2-\frac{(\sum d_i)^2}{n}}{n-1}}, \quad df=n-1.
\]</span>
Reject <span class="math inline">\(H_0: \mu_d=\delta_0\)</span> if <span class="math inline">\(t_o\ge t_{n-1}(\alpha/2)\)</span>.</p>
</div>
<div id="multivariate-case-based-on-a-paired-sample" class="section level3 hasAnchor" number="4.2.6">
<h3><span class="header-section-number">4.2.6</span> Multivariate Case Based on a Paired Sample<a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-based-on-a-paired-sample" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Generalize the univariate case to multivariate case, let the matrix of pairwise differences <span class="math inline">\(\mathbf{D}=\mathbf{X}_1-\mathbf{X}_2\)</span>, the element-wise difference between the two observation matrices <span class="math inline">\(\mathbf{X}_1=[x_{1ik}]_{k=1}^{n}, i=1, 2, \cdots, p\)</span> and <span class="math inline">\(\mathbf{X}_2=[x_{2ik}]_{k=1}^{n}, i=1, 2, \cdots, p\)</span>. And <span class="math inline">\(\bar {\mathbf{d}}\)</span> is the sample mean vector of difference matrix <span class="math inline">\(\mathbf{D}\)</span>, and <span class="math inline">\(\mathbf{S}_d\)</span> is the sample covariance matrix of <span class="math inline">\(\mathbf{D}\)</span>. The hypotheses of the paired Hotelling’s <span class="math inline">\(T^2\)</span> test is <span class="math inline">\(H_0: \boldsymbol{\mu}_d=\boldsymbol{\delta}_0\)</span> versus <span class="math inline">\(H_a: \boldsymbol{\mu}_d\ne \boldsymbol{\delta}_0\)</span>. The test statistic is
<span class="math display">\[
T^2=n(\mathbf{\bar d}-\boldsymbol{\delta}_0)^T\mathbf{S}_d^{-1}(\mathbf{\bar d}-\boldsymbol{\delta}_0) \Longrightarrow \frac{(n-p)}{(n-1)p}T^2\sim F_{p, n-p}.
\]</span>
We reject <span class="math inline">\(H_0: \boldsymbol{\mu}_d=\boldsymbol{\delta}_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if
<span class="math display">\[
F_o=\frac{(n-p)}{p(n-1)}T^2\ge F_{p, n-p}(\alpha)
\]</span>
where <span class="math inline">\(F_{p, n-p}(\alpha)\)</span> is the upper <span class="math inline">\(\alpha\times 100\)</span>th percentile of the <span class="math inline">\(F_{p, n-p}\)</span> distribution.</p>
<p>$$</p>
<p>A sample of husband and wife pairs are asked to respond to each of the following questions:</p>
<ol style="list-style-type: decimal">
<li>What is the level of passionate love you feel for your partner?</li>
<li>What is the level of passionate love your partner feels for you?</li>
<li>What is the level of companionate love you feel for your partner?</li>
<li>What is the level of companionate love your partner feels for you?</li>
</ol>
<p>A total of 30 married couples were questioned. Responses were recorded on the five-point scale. Responses included the following values:</p>
<ol style="list-style-type: decimal">
<li>None at all</li>
<li>Very little</li>
<li>Some</li>
<li>A great deal</li>
<li>Tremendous amount</li>
</ol>
<p>The data are summarized in the following table:</p>
<pre><code>## (np.float64(-0.5), np.float64(630.5), np.float64(299.5), np.float64(-0.5))</code></pre>
<p><img src="Plots/spouseTbl-21.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We can conduct the test in <span class="math inline">\(\textsf{R}.\)</span></p>
<div class="sourceCode" id="cb113"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="hypothesis-tests-on-mean-vectors.html#cb113-1" tabindex="-1"></a>spouse <span class="op">=</span> pd.read_csv(<span class="st">&quot;data/spouse_paired.txt&quot;</span>, header<span class="op">=</span><span class="va">None</span>, delim_whitespace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb113-2"><a href="hypothesis-tests-on-mean-vectors.html#cb113-2" tabindex="-1"></a>hmat <span class="op">=</span> spouse.iloc[:, <span class="dv">0</span>:<span class="dv">4</span>].to_numpy() <span class="co"># husband</span></span>
<span id="cb113-3"><a href="hypothesis-tests-on-mean-vectors.html#cb113-3" tabindex="-1"></a>wmat <span class="op">=</span> spouse.iloc[:, <span class="dv">4</span>:<span class="dv">8</span>].to_numpy() <span class="co"># wife</span></span>
<span id="cb113-4"><a href="hypothesis-tests-on-mean-vectors.html#cb113-4" tabindex="-1"></a></span>
<span id="cb113-5"><a href="hypothesis-tests-on-mean-vectors.html#cb113-5" tabindex="-1"></a>dmat <span class="op">=</span> hmat <span class="op">-</span> wmat</span>
<span id="cb113-6"><a href="hypothesis-tests-on-mean-vectors.html#cb113-6" tabindex="-1"></a>n, p <span class="op">=</span> dmat.shape</span>
<span id="cb113-7"><a href="hypothesis-tests-on-mean-vectors.html#cb113-7" tabindex="-1"></a>mean_diff <span class="op">=</span> np.mean(dmat, axis<span class="op">=</span><span class="dv">0</span>).reshape(p, <span class="dv">1</span>)</span>
<span id="cb113-8"><a href="hypothesis-tests-on-mean-vectors.html#cb113-8" tabindex="-1"></a>cov_diff <span class="op">=</span> np.cov(dmat, rowvar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb113-9"><a href="hypothesis-tests-on-mean-vectors.html#cb113-9" tabindex="-1"></a></span>
<span id="cb113-10"><a href="hypothesis-tests-on-mean-vectors.html#cb113-10" tabindex="-1"></a>t2 <span class="op">=</span> n <span class="op">*</span> mean_diff.T <span class="op">@</span> np.linalg.inv(cov_diff) <span class="op">@</span> mean_diff</span>
<span id="cb113-11"><a href="hypothesis-tests-on-mean-vectors.html#cb113-11" tabindex="-1"></a>f0 <span class="op">=</span> (n <span class="op">-</span> p) <span class="op">/</span> (p <span class="op">*</span> (n <span class="op">-</span> <span class="dv">1</span>)) <span class="op">*</span> t2</span>
<span id="cb113-12"><a href="hypothesis-tests-on-mean-vectors.html#cb113-12" tabindex="-1"></a>f0 <span class="op">=</span> f0.item()</span>
<span id="cb113-13"><a href="hypothesis-tests-on-mean-vectors.html#cb113-13" tabindex="-1"></a>f_crit <span class="op">=</span> f.ppf(<span class="fl">0.95</span>, dfn<span class="op">=</span>p, dfd<span class="op">=</span>n <span class="op">-</span> p)</span>
<span id="cb113-14"><a href="hypothesis-tests-on-mean-vectors.html#cb113-14" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> f.cdf(f0, dfn<span class="op">=</span>p, dfd<span class="op">=</span>n <span class="op">-</span> p)</span>
<span id="cb113-15"><a href="hypothesis-tests-on-mean-vectors.html#cb113-15" tabindex="-1"></a></span>
<span id="cb113-16"><a href="hypothesis-tests-on-mean-vectors.html#cb113-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Hotelling&#39;s T²:&quot;</span>, t2.item())</span></code></pre></div>
<pre><code>## Hotelling&#39;s T²: 13.127840260936427</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="hypothesis-tests-on-mean-vectors.html#cb115-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;F statistic:&quot;</span>, f0)</span></code></pre></div>
<pre><code>## F statistic: 2.942446955037475</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb117-1"><a href="hypothesis-tests-on-mean-vectors.html#cb117-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Critical F-value (alpha = 0.05):&quot;</span>, f_crit)</span></code></pre></div>
<pre><code>## Critical F-value (alpha = 0.05): 2.7425941372218587</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="hypothesis-tests-on-mean-vectors.html#cb119-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;p-value:&quot;</span>, p_value)</span></code></pre></div>
<pre><code>## p-value: 0.03936914422934046</code></pre>
<p>Given that
<span class="math display">\[
\mathbf{\bar d}=\left[
\begin{array}{c}
0.0667\\
-0.1333\\
-0.3000\\
-0.1333\\
\end{array}
\right], \mathbf{S}_d=\left[
\begin{array}{rrrr}
0.8230 &amp; 0.0782&amp; -0.0138&amp; -0.0598\\
0.0782&amp;  0.8092&amp; -0.2138 &amp;-0.1563\\
-0.0138 &amp;-0.2138&amp;  0.5621&amp;  0.5103\\
-0.0598&amp; -0.1563  &amp;0.5103 &amp; 0.6023\\
\end{array}
\right],
\]</span>
test at the 5% significance level whether husbands respond to the questions in the same way as their wives. We can use
<span class="math display">\[
\begin{aligned}
T^2&amp;=n(\mathbf{\bar d}-\boldsymbol{\delta}_0)^T\mathbf{S}_d^{-1}(\mathbf{\bar d}-\boldsymbol{\delta}_0)\\
&amp;=30\times [0.0667, -0.1333, -0.3000, -0.1333]\left[
\begin{array}{rrrrr}
1.2558 &amp; -0.1502&amp; -0.4510 &amp; 0.4678\\
-0.1502&amp;  1.4115&amp;  0.9279&amp; -0.4348\\
-0.4510&amp;  0.9279&amp;  8.4174&amp; -6.9356\\
0.4678&amp; -0.4348&amp; -6.9356&amp;  7.4702\\
\end{array}
\right] \left[
\begin{array}{c}
0.0667\\
-0.1333\\
-0.3000\\
-0.1333\\
\end{array}
\right]\\
&amp;=13.123
\end{aligned}
\]</span>
complete the test.
</p>
</div>
</div>
<div id="hypothesis-test-for-several-mean-vectors" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Hypothesis Test for Several Mean Vectors<a href="hypothesis-tests-on-mean-vectors.html#hypothesis-test-for-several-mean-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We first review how to compare several population means in the univariate case.</p>
<div id="univariate-case-one-way-anova-f-test" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Univariate Case: One-Way ANOVA F Test<a href="hypothesis-tests-on-mean-vectors.html#univariate-case-one-way-anova-f-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use one-way ANOVA comparing several population means in the univariate case. The hypotheses of one-way ANOVA are formulated as</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: all means are equal, <span class="math inline">\(\mu_1=\mu_2=\cdots=\mu_k\)</span>.</li>
<li><span class="math inline">\(H_a\)</span>: not all the means are equal.</li>
</ul>
<p>The test statistic is
<span class="math display">\[
F=\frac{\frac{SSTR}{k-1}}{\frac{SSE}{n-k}}\sim F_{k-1, n-k}
\]</span>
which follows an F distribution with degrees of freedom <span class="math inline">\(k-1\)</span> and <span class="math inline">\(n-k\)</span> where <span class="math inline">\(k\)</span> is the number of means and <span class="math inline">\(n=n_1+n_2+\cdots+n_k\)</span> is the total number of observations from all <span class="math inline">\(k\)</span> populations. The variation <span class="math inline">\(SSTR\)</span> and variation are calculated as
<span class="math display">\[
SSTR=\sum_{i=1}^k n_i(\bar x_i-\bar x)^2; \quad SSE=\sum_{i=1}^k (n_i-1)s_i^2
\]</span>
where <span class="math inline">\(n_i, \bar x_i, s_i^2, i=1, 2, \cdots, k\)</span> are the sample size, the sample mean and sample variance of the <span class="math inline">\(k\)</span> samples from their own populations, <span class="math inline">\(\bar x\)</span> is the mean of all observations. The total variation in the response <span class="math inline">\(SST\)</span> is given by the identity: <span class="math inline">\(SST=SSTR+SSE\)</span>. Reject <span class="math inline">\(H_0: \mu_1=\mu_2=\cdots=\mu_k\)</span> at significance level <span class="math inline">\(\alpha\)</span> if the observed F score is larger than <span class="math inline">\(F_{k-1, n-k}(\alpha)\)</span>.</p>
</div>
<div id="multivariate-case-one-way-manova" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Multivariate Case: One-Way MANOVA<a href="hypothesis-tests-on-mean-vectors.html#multivariate-case-one-way-manova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea for one-way ANOVA can be generalized to the multivariate case by replacing the sample mean <span class="math inline">\(\bar x_i\)</span> with mean vector <span class="math inline">\(\mathbf{\bar x_i}\)</span> and the sample variance <span class="math inline">\(s_i^2\)</span> with the covariance matrix <span class="math inline">\(\mathbf{S_i}\)</span>. The between-treatment variation is given by
<span class="math display">\[
\mathbf{B}=\sum_{i=1}^kn_i (\mathbf{\bar x_i}-\mathbf{\bar x})(\mathbf{\bar x_i}-\mathbf{\bar x})^{T}
\]</span>
and the within-treatment variation is
<span class="math display">\[
\mathbf{W}=\sum_{i=1}^k (n_i-1) \mathbf{S_i}
\]</span>
Reject <span class="math inline">\(H_0: \boldsymbol{\mu_1}=\boldsymbol{\mu_2}=\cdots=\boldsymbol{\mu_k}\)</span> if
<span class="math display">\[
\Lambda^{\ast}=\frac{|\mathbf{W}|}{|\mathbf{B}+\mathbf{W}|}
\]</span>
is too . The quantity <span class="math inline">\(\Lambda^{\ast}\)</span> was originally proposed by Wilks and has distribution as follows:</p>
<p><span class="math display">\[
\begin{array}{ccc}
\hline
\text{No. of variables}&amp;\text{No. of groups}&amp;\text{Sampling distribution of $\Lambda^{\ast}$}\\
\hline
p=1&amp;k\ge2&amp;\left(\frac{n-k}{k-1}\right)\left(\frac{1-\Lambda^{\ast}}{\Lambda^{\ast}}\right)\sim F_{k-1, n-k}\\
p=2&amp;k\ge2&amp;\left(\frac{n-k-1}{k-1}\right)\left(\frac{1-\sqrt{\Lambda^{\ast}}}{\sqrt{\Lambda^{\ast}}}\right)\sim F_{2(k-1), 2(n-k-1)}\\
p\ge1&amp;k=2&amp;\left(\frac{n-p-1}{p}\right)\left(\frac{1-\Lambda^{\ast}}{\Lambda^{\ast}}\right)\sim F_{p, n-p-1}\\
p\ge 1&amp;$k=3&amp;\left(\frac{n-p-2}{p}\right)\left(\frac{1-\sqrt{\Lambda^{\ast}}}{\sqrt{\Lambda^{\ast}}}\right)\sim F_{2p, 2(n-p-2)}\\
\hline
\end{array}
\]</span></p>
<p>If <span class="math inline">\(n\)</span> is large, Barlett has shown that if <span class="math inline">\(H_0\)</span> is true, the quantity
<span class="math display">\[
-\left(n-1-\frac{p+k}{2}\right)\ln{\Lambda^{\ast}}
\]</span>
has approximately a chi-square distribution with <span class="math inline">\(df=p(k-1)\)</span>. Therefore, we can reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span> if
<span class="math display">\[
-\left(n-1-\frac{p+k}{2}\right)\ln\left(\frac{|\mathbf{W}|}{|\mathbf{B}+\mathbf{W}|}\right)\ge \chi^2_{p(k-1)}(\alpha)
\]</span>
This is called the one-way MANOVA (multivariate analysis of variance).</p>
<p></p>
<p>Suppose there are three groups, <span class="math inline">\(n_1=3, n_2=2, n_3=3\)</span> and the data matrices are
<span class="math display">\[
\mathbf{X_1}=\left[
\begin{array}{cc}
9  &amp;  3\\
6 &amp;   2\\
9 &amp;   7
\end{array}
\right]; \quad \mathbf{X_2}=\left[
\begin{array}{cc}
0  &amp;  4\\
2 &amp;   0\\
\end{array}
\right]; \quad \mathbf{X_3}=\left[
\begin{array}{cc}
3  &amp;  8\\
1 &amp;   9\\
2 &amp;   7
\end{array}
\right] \mbox{ with } \mathbf{\bar x}=\left[
\begin{array}{c}
4\\
5
\end{array}
\right]; \quad \mathbf{\bar x_1}=\left[
\begin{array}{c}
8\\
4
\end{array}
\right]; \quad \mathbf{\bar x_2}=\left[
\begin{array}{c}
1\\
2
\end{array}
\right]; \quad \mathbf{\bar x_3}=\left[
\begin{array}{c}
2\\
8
\end{array}
\right]
\]</span>
The between-treatment variation is
<span class="math display">\[
\mathbf{B}=\sum_{i=1}^kn_i (\mathbf{\bar x_i}-\mathbf{\bar x})(\mathbf{\bar x_i}-\mathbf{\bar x})^{T}
=3\left[
\begin{array}{c}
4\\
-1
\end{array}
\right][4, -1]+2\left[
\begin{array}{c}
-3\\
-3
\end{array}
\right][-3, -3]+3\left[
\begin{array}{c}
-2\\
3
\end{array}
\right][-2, 3]
=\left[
\begin{array}{cc}
78  &amp;  -12\\
-12 &amp;   48\\
\end{array}
\right]
\]</span></p>
<p>And then the within-treatment variation is
<span class="math display">\[
\mathbf{W}=\sum_{i=1}^k (n_i-1) \mathbf{S_i}=(3-1)\left[
\begin{array}{cc}
3 &amp;  3\\
3 &amp;   7\\
\end{array}
\right]+(2-1)\left[
\begin{array}{cc}
2 &amp;  -4\\
-4 &amp;   8\\
\end{array}
\right]+(3-1)\left[
\begin{array}{cc}
1 &amp;  -0.5\\
-0.5 &amp;   1\\
\end{array}
\right]=\left[
\begin{array}{cc}
10 &amp;  1\\
1 &amp;   24\\
\end{array}
\right]
\]</span>
Therefore,
<span class="math display">\[
\Lambda^{\ast}=\frac{|\mathbf{W}|}{|\mathbf{B}+\mathbf{W}|}=\frac{\left|
\begin{array}{cc}
10 &amp;  1\\
1 &amp;   24\\
\end{array}
\right|}{\left|
\begin{array}{cc}
88 &amp;  -11\\
-11 &amp;   72\\
\end{array}
\right|}=\frac{239}{6215}=0.0385
\]</span>
Since <span class="math inline">\(p=2, k=3\)</span> and <span class="math inline">\(n=8\)</span> which is small, the distribution of <span class="math inline">\(\Lambda^{\ast}\)</span> is
<span class="math display">\[
\left(\frac{n-k-1}{k-1}\right)\left(\frac{1-\sqrt{\Lambda^{\ast}}}{\sqrt{\Lambda^{\ast}}}\right)=\left(\frac{8-3-1}{3-1}\right)\left(\frac{1-\sqrt{0.0385}}{\sqrt{0.0385}}\right)=8.199
\]</span>
Compared with <span class="math inline">\(F_{2(k-1), 2(n-k-1)}(0.05)=F_{2(3-1), 2(8-3-1)}(0.05)=F_{4, 8}(0.05)=3.838\)</span>. Since <span class="math inline">\(8.199&gt;F_{4, 8}(0.05)=3.838\)</span>, we can reject <span class="math inline">\(H_0: \mu_1=\mu_2=\mu_3\)</span> at the 5% significance level.</p>
<p>Double check with .</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="hypothesis-tests-on-mean-vectors.html#cb121-1" tabindex="-1"></a><span class="im">from</span> statsmodels.multivariate.manova <span class="im">import</span> MANOVA</span>
<span id="cb121-2"><a href="hypothesis-tests-on-mean-vectors.html#cb121-2" tabindex="-1"></a></span>
<span id="cb121-3"><a href="hypothesis-tests-on-mean-vectors.html#cb121-3" tabindex="-1"></a>x1 <span class="op">=</span> np.array([[<span class="dv">9</span>, <span class="dv">3</span>],</span>
<span id="cb121-4"><a href="hypothesis-tests-on-mean-vectors.html#cb121-4" tabindex="-1"></a>               [<span class="dv">6</span>, <span class="dv">2</span>],</span>
<span id="cb121-5"><a href="hypothesis-tests-on-mean-vectors.html#cb121-5" tabindex="-1"></a>               [<span class="dv">9</span>, <span class="dv">7</span>]])</span>
<span id="cb121-6"><a href="hypothesis-tests-on-mean-vectors.html#cb121-6" tabindex="-1"></a>x2 <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">4</span>],</span>
<span id="cb121-7"><a href="hypothesis-tests-on-mean-vectors.html#cb121-7" tabindex="-1"></a>               [<span class="dv">2</span>, <span class="dv">0</span>]])</span>
<span id="cb121-8"><a href="hypothesis-tests-on-mean-vectors.html#cb121-8" tabindex="-1"></a>x3 <span class="op">=</span> np.array([[<span class="dv">3</span>, <span class="dv">8</span>],</span>
<span id="cb121-9"><a href="hypothesis-tests-on-mean-vectors.html#cb121-9" tabindex="-1"></a>               [<span class="dv">1</span>, <span class="dv">9</span>],</span>
<span id="cb121-10"><a href="hypothesis-tests-on-mean-vectors.html#cb121-10" tabindex="-1"></a>               [<span class="dv">2</span>, <span class="dv">7</span>]])</span>
<span id="cb121-11"><a href="hypothesis-tests-on-mean-vectors.html#cb121-11" tabindex="-1"></a></span>
<span id="cb121-12"><a href="hypothesis-tests-on-mean-vectors.html#cb121-12" tabindex="-1"></a>x <span class="op">=</span> np.vstack([x1, x2, x3])</span>
<span id="cb121-13"><a href="hypothesis-tests-on-mean-vectors.html#cb121-13" tabindex="-1"></a>groups <span class="op">=</span> np.array([<span class="dv">1</span>]<span class="op">*</span><span class="dv">3</span> <span class="op">+</span> [<span class="dv">2</span>]<span class="op">*</span><span class="dv">2</span> <span class="op">+</span> [<span class="dv">3</span>]<span class="op">*</span><span class="dv">3</span>)</span>
<span id="cb121-14"><a href="hypothesis-tests-on-mean-vectors.html#cb121-14" tabindex="-1"></a></span>
<span id="cb121-15"><a href="hypothesis-tests-on-mean-vectors.html#cb121-15" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(x, columns<span class="op">=</span>[<span class="st">&#39;V1&#39;</span>, <span class="st">&#39;V2&#39;</span>])</span>
<span id="cb121-16"><a href="hypothesis-tests-on-mean-vectors.html#cb121-16" tabindex="-1"></a>df[<span class="st">&#39;group&#39;</span>] <span class="op">=</span> groups.astype(<span class="bu">str</span>)  <span class="co"># categorical variable as string</span></span>
<span id="cb121-17"><a href="hypothesis-tests-on-mean-vectors.html#cb121-17" tabindex="-1"></a></span>
<span id="cb121-18"><a href="hypothesis-tests-on-mean-vectors.html#cb121-18" tabindex="-1"></a>manova <span class="op">=</span> MANOVA.from_formula(<span class="st">&#39;V1 + V2 ~ group&#39;</span>, data<span class="op">=</span>df)</span>
<span id="cb121-19"><a href="hypothesis-tests-on-mean-vectors.html#cb121-19" tabindex="-1"></a><span class="bu">print</span>(manova.mv_test()) <span class="co"># gives wilks test</span></span></code></pre></div>
<pre><code>## /Users/ruskin/r-python-env/lib/python3.13/site-packages/statsmodels/multivariate/multivariate_ols.py:216: RuntimeWarning: divide by zero encountered in scalar divide
##   b = (p + 2*n) * (q + 2*n) / 2 / (2*n + 1) / (n - 1)
## /Users/ruskin/r-python-env/lib/python3.13/site-packages/statsmodels/multivariate/multivariate_ols.py:216: RuntimeWarning: divide by zero encountered in scalar divide
##   b = (p + 2*n) * (q + 2*n) / 2 / (2*n + 1) / (n - 1)
##                  Multivariate linear model
## ============================================================
##                                                             
## ------------------------------------------------------------
##        Intercept         Value  Num DF Den DF F Value Pr &gt; F
## ------------------------------------------------------------
##           Wilks&#39; lambda  0.0465 2.0000 4.0000 40.9707 0.0022
##          Pillai&#39;s trace  0.9535 2.0000 4.0000 40.9707 0.0022
##  Hotelling-Lawley trace 20.4854 2.0000 4.0000 40.9707 0.0022
##     Roy&#39;s greatest root 20.4854 2.0000 4.0000 40.9707 0.0022
## ------------------------------------------------------------
##                                                             
## ------------------------------------------------------------
##          group          Value  Num DF  Den DF F Value Pr &gt; F
## ------------------------------------------------------------
##           Wilks&#39; lambda 0.0385 4.0000  8.0000  8.1989 0.0062
##          Pillai&#39;s trace 1.5408 4.0000 10.0000  8.3882 0.0031
##  Hotelling-Lawley trace 9.9414 4.0000  4.0000  9.9414 0.0235
##     Roy&#39;s greatest root 8.0764 2.0000  5.0000 20.1910 0.0040
## ============================================================</code></pre>
<p></p>
<p>We can also apply Wilks’ lambda test on the Iris flowers data to see whether the three species have different mean values on the measurements. I will use the two variables and to make <span class="math inline">\(p=2\)</span> in the notes.</p>
<p>The sample mean vectors for the three species and the overall mean vector are:
<span class="math display">\[
\mathbf{\bar x_1}=\left[
\begin{array}{c}
1.462\\
0.246
\end{array}
\right]; \quad \mathbf{\bar x_2}=\left[
\begin{array}{c}
4.260\\
1.326
\end{array}
\right]; \quad \mathbf{\bar x_3}=\left[
\begin{array}{c}
5.552\\
2.026
\end{array}
\right]; \quad  \mathbf{\bar x}=\left[
\begin{array}{c}
3.758\\
1.199
\end{array}
\right]
\]</span></p>
<p>The between and within treatment variations are
<span class="math display">\[
\mathbf{B}=\left[
\begin{array}{cc}
437.103 &amp;  186.774\\
186.774 &amp;  80.413\\
\end{array}
\right]; \quad \mathbf{W}=\left[
\begin{array}{cc}
27.223 &amp;  6.272\\
6.272 &amp;  6.157\\
\end{array}
\right]
\]</span>
which gives <span class="math inline">\(\Lambda^{\ast}=0.0438\)</span>. The distribution of <span class="math inline">\(\Lambda^{\ast}\)</span> is
<span class="math display">\[
\left(\frac{n-k-1}{k-1}\right)\left(\frac{1-\sqrt{\Lambda^{\ast}}}{\sqrt{\Lambda^{\ast}}}\right)=\left(\frac{150-3-1}{3-1}\right)\left(\frac{1-\sqrt{0.0438}}{\sqrt{0.0438}}\right)=275.9
\]</span>
Compared with <span class="math inline">\(F_{2(k-1), 2(n-k-1)}(0.05)=F_{2(3-1), 2(150-3-1)}(0.05)=F_{4, 292}(0.05)=2.403\)</span>. We can reject <span class="math inline">\(H_0\)</span>. Since <span class="math inline">\(n=n_1+n_2+n_3=3\times 50=150\)</span> is relatively large, we can use the chi-square approximation,
<span class="math display">\[
-\left(n-1-\frac{p+k}{2}\right)\ln\left(\frac{|\mathbf{W}|}{|\mathbf{B}+\mathbf{W}|}\right)=458.348
\]</span>
compared with <span class="math inline">\(\chi^2_{p(k-1)}(0.05)=\chi^2_4(0.05)=9.488\)</span>, reject <span class="math inline">\(H_0\)</span>.</p>
<p>Confirm with .</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb123-1"><a href="hypothesis-tests-on-mean-vectors.html#cb123-1" tabindex="-1"></a>iris <span class="op">=</span> load_iris(as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb123-2"><a href="hypothesis-tests-on-mean-vectors.html#cb123-2" tabindex="-1"></a>df <span class="op">=</span> iris.frame</span>
<span id="cb123-3"><a href="hypothesis-tests-on-mean-vectors.html#cb123-3" tabindex="-1"></a>df[<span class="st">&#39;species&#39;</span>] <span class="op">=</span> iris.target_names[iris.target]</span>
<span id="cb123-4"><a href="hypothesis-tests-on-mean-vectors.html#cb123-4" tabindex="-1"></a></span>
<span id="cb123-5"><a href="hypothesis-tests-on-mean-vectors.html#cb123-5" tabindex="-1"></a>df_subset <span class="op">=</span> df[[<span class="st">&#39;petal length (cm)&#39;</span>, <span class="st">&#39;petal width (cm)&#39;</span>, <span class="st">&#39;species&#39;</span>]]</span>
<span id="cb123-6"><a href="hypothesis-tests-on-mean-vectors.html#cb123-6" tabindex="-1"></a>df_subset.columns <span class="op">=</span> [<span class="st">&#39;PetalLength&#39;</span>, <span class="st">&#39;PetalWidth&#39;</span>, <span class="st">&#39;Species&#39;</span>]</span>
<span id="cb123-7"><a href="hypothesis-tests-on-mean-vectors.html#cb123-7" tabindex="-1"></a></span>
<span id="cb123-8"><a href="hypothesis-tests-on-mean-vectors.html#cb123-8" tabindex="-1"></a>manova <span class="op">=</span> MANOVA.from_formula(<span class="st">&#39;PetalLength + PetalWidth ~ Species&#39;</span>, data<span class="op">=</span>df_subset)</span>
<span id="cb123-9"><a href="hypothesis-tests-on-mean-vectors.html#cb123-9" tabindex="-1"></a><span class="bu">print</span>(manova.mv_test())</span></code></pre></div>
<pre><code>##                    Multivariate linear model
## ================================================================
##                                                                 
## ----------------------------------------------------------------
##          Intercept        Value  Num DF  Den DF  F Value  Pr &gt; F
## ----------------------------------------------------------------
##             Wilks&#39; lambda 0.1995 2.0000 146.0000 292.9791 0.0000
##            Pillai&#39;s trace 0.8005 2.0000 146.0000 292.9791 0.0000
##    Hotelling-Lawley trace 4.0134 2.0000 146.0000 292.9791 0.0000
##       Roy&#39;s greatest root 4.0134 2.0000 146.0000 292.9791 0.0000
## ----------------------------------------------------------------
##                                                                 
## ----------------------------------------------------------------
##         Species          Value  Num DF  Den DF   F Value  Pr &gt; F
## ----------------------------------------------------------------
##           Wilks&#39; lambda  0.0438 4.0000 292.0000  275.9001 0.0000
##          Pillai&#39;s trace  1.0465 4.0000 294.0000   80.6612 0.0000
##  Hotelling-Lawley trace 19.7821 4.0000 174.1653  720.4267 0.0000
##     Roy&#39;s greatest root 19.6773 2.0000 147.0000 1446.2819 0.0000
## ================================================================</code></pre>
<p>If we use all four predicted variables, the result becomes:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="hypothesis-tests-on-mean-vectors.html#cb125-1" tabindex="-1"></a>iris <span class="op">=</span> load_iris(as_frame<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb125-2"><a href="hypothesis-tests-on-mean-vectors.html#cb125-2" tabindex="-1"></a>df <span class="op">=</span> iris.frame.copy()</span>
<span id="cb125-3"><a href="hypothesis-tests-on-mean-vectors.html#cb125-3" tabindex="-1"></a>df[<span class="st">&#39;Species&#39;</span>] <span class="op">=</span> iris.target_names[iris.target]</span>
<span id="cb125-4"><a href="hypothesis-tests-on-mean-vectors.html#cb125-4" tabindex="-1"></a></span>
<span id="cb125-5"><a href="hypothesis-tests-on-mean-vectors.html#cb125-5" tabindex="-1"></a>df.rename(columns<span class="op">=</span>{</span>
<span id="cb125-6"><a href="hypothesis-tests-on-mean-vectors.html#cb125-6" tabindex="-1"></a>    <span class="st">&#39;sepal length (cm)&#39;</span>: <span class="st">&#39;SepalLength&#39;</span>,</span>
<span id="cb125-7"><a href="hypothesis-tests-on-mean-vectors.html#cb125-7" tabindex="-1"></a>    <span class="st">&#39;sepal width (cm)&#39;</span>: <span class="st">&#39;SepalWidth&#39;</span>,</span>
<span id="cb125-8"><a href="hypothesis-tests-on-mean-vectors.html#cb125-8" tabindex="-1"></a>    <span class="st">&#39;petal length (cm)&#39;</span>: <span class="st">&#39;PetalLength&#39;</span>,</span>
<span id="cb125-9"><a href="hypothesis-tests-on-mean-vectors.html#cb125-9" tabindex="-1"></a>    <span class="st">&#39;petal width (cm)&#39;</span>: <span class="st">&#39;PetalWidth&#39;</span></span>
<span id="cb125-10"><a href="hypothesis-tests-on-mean-vectors.html#cb125-10" tabindex="-1"></a>}, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb125-11"><a href="hypothesis-tests-on-mean-vectors.html#cb125-11" tabindex="-1"></a></span>
<span id="cb125-12"><a href="hypothesis-tests-on-mean-vectors.html#cb125-12" tabindex="-1"></a>manova <span class="op">=</span> MANOVA.from_formula(<span class="st">&#39;SepalLength + SepalWidth + PetalLength + PetalWidth ~ Species&#39;</span>, data<span class="op">=</span>df)</span>
<span id="cb125-13"><a href="hypothesis-tests-on-mean-vectors.html#cb125-13" tabindex="-1"></a><span class="bu">print</span>(manova.mv_test())</span></code></pre></div>
<pre><code>##                    Multivariate linear model
## ================================================================
##                                                                 
## ----------------------------------------------------------------
##        Intercept         Value  Num DF  Den DF   F Value  Pr &gt; F
## ----------------------------------------------------------------
##           Wilks&#39; lambda  0.0170 4.0000 144.0000 2086.7720 0.0000
##          Pillai&#39;s trace  0.9830 4.0000 144.0000 2086.7720 0.0000
##  Hotelling-Lawley trace 57.9659 4.0000 144.0000 2086.7720 0.0000
##     Roy&#39;s greatest root 57.9659 4.0000 144.0000 2086.7720 0.0000
## ----------------------------------------------------------------
##                                                                 
## ----------------------------------------------------------------
##         Species          Value  Num DF  Den DF   F Value  Pr &gt; F
## ----------------------------------------------------------------
##           Wilks&#39; lambda  0.0234 8.0000 288.0000  199.1453 0.0000
##          Pillai&#39;s trace  1.1919 8.0000 290.0000   53.4665 0.0000
##  Hotelling-Lawley trace 32.4773 8.0000 203.4024  582.1970 0.0000
##     Roy&#39;s greatest root 32.1919 4.0000 145.0000 1166.9574 0.0000
## ================================================================</code></pre>
</div>
</div>
<div id="revisit-the-learning-outcomes-2" class="section level2 unnumbered hasAnchor">
<h2>Revisit the Learning Outcomes<a href="hypothesis-tests-on-mean-vectors.html#revisit-the-learning-outcomes-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After finishing this note, students should be able to</p>
<ul>
<li>Explain the main idea of using a chi-square Q-Q plot to assess the multivariate normality assumption.</li>
<li>Assess the multivariate normality through a chi-square Q-Q plot and casual procedures using R.</li>
<li>Conduct a Hotelling’s <span class="math inline">\(T^2\)</span> test on one mean vector based on one sample or paired sample.</li>
<li>Conduct a Hotelling’s <span class="math inline">\(T^2\)</span> test on two mean vectors based on two independent samples.</li>
<li>Conduct a one-way MANOVA test on at least two mean vectors based on at least two independent samples.</li>
<li>Obtain <span class="math inline">\((1-\alpha)\times 100\%\)</span> Bonferroni confidence intervals associated with a certain test if applicable.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="displaying-multivariate-data-and-measures-of-distance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="principal-component-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
