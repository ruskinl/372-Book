[["index.html", "STAT 372 Open Textbook (Python) Preamble", " STAT 372 Open Textbook (Python) Dr. Wanhua Su 2025-08-11 Preamble This book contains the lecture notes for the course STAT 372 - Introduction to Multivariate Statistics and Machine Learning at Macewan University. These notes were written and compiled by Dr. Wanhua Su and stored in this online resource for students and instructors to access. The code snippets in this book are written in the Python programming language. There is also a version of the book with code snippets in the R language available here. (Link to be added when complete) "],["intro.html", "1 Introduction Learning Outcomes 1.1 Introduction 1.2 Some Examples 1.3 Multivariate Methods Covered in STAT 372 1.4 Review: Univariate Analysis Revisit Learning Learning Outcomes", " 1 Introduction Learning Outcomes After finishing this chapter, students should be able to: Explain the differences between univariate analysis and multivariate analysis. Describe briefly several applications that multivariate analysis can be used. Calculate the expected value and variance of a random variable. Conduct hypothesis tests about population means covered in Stat 151 such as one-sample, two-sample, paired \\(t\\) test, one-way ANOVA F test. 1.1 Introduction In Stat 151, we focused on descriptive and inferential statistics on a single random variable, for example, your height or your grade in the final exam; this is called univariate analysis. In most applications, however, we encounter data set in which several measurements are taken from the individuals. For example, in order to have a basic idea about the shape of a person, we need to know his height and weight, knowing either the height or weight alone is not enough. The analysis of this kind of data is called multivariate analysis. Students could be able to find a lot of data sets related to multivariate analysis from the Machine Learning and Data Science Community at https://www.kaggle.com/, the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml/, and case studies in data analysis competition of annual SSC (Statistical Society of Canada) meeting. 1.2 Some Examples Multivariate analysis methods are widely used in practice. The simplest example is fitting a multiple linear regression which covered in Stat 252. Recall that a simple linear regression model is given by \\[ Y=\\beta_0+\\beta_1 x+\\epsilon, \\epsilon \\sim N(0, \\sigma), \\] where \\(Y\\) is the response (dependent) variable and \\(x\\) is an value of the predictor variable \\(X\\), \\(\\epsilon\\) is the error term, \\(\\beta_0\\) is the population intercept, and \\(\\beta_1\\) is the population slope. Simple linear regression has only one predictor variable. It can be generalized to multiple linear regression to include more predictor variables. For example, we can model the relationship between price of single houses and their features such as age (\\(x_1\\)), size (\\(x_2\\)), number of baths (\\(x_3\\)), roof type (\\(x_4\\), tiled or non-tiled) by fitting a multiple regression model \\[ Y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4 x_4+\\epsilon, \\epsilon \\sim N(0, \\sigma) \\] with dummy variable \\[x_4= \\left\\{ \\begin{array}{ll} 1&amp;\\mbox{tiled roof},\\\\ 0&amp;\\mbox{non-tiled roof}. \\end{array} \\right. \\] Here are several examples will be used in STAT 372. Example 1: Storm Survival of Sparrows The data set gives the body measurements of 49 female sparrows. The first measurement is \\(X_1\\)=total length, the second one is \\(X_2\\)=alar length, the third is \\(X_3\\)=length of beak and head, the fourth is \\(X_4\\)=length of humerus, and the fifth is \\(X_5\\)=length of keel and sternum, all in mm. ## +------+------+------+------+------+------+ ## | ID | X1 | X2 | X3 | X4 | X5 | ## +======+======+======+======+======+======+ ## | 1 | 156 | 245 | 31.6 | 18.5 | 20.5 | ## +------+------+------+------+------+------+ ## | 2 | 154 | 240 | 30.4 | 17.9 | 19.6 | ## +------+------+------+------+------+------+ ## | 3 | 153 | 240 | 31 | 18.4 | 20.6 | ## +------+------+------+------+------+------+ ## | 4 | 153 | 236 | 30.9 | 17.7 | 20.2 | ## +------+------+------+------+------+------+ ## | 5 | 155 | 243 | 31.5 | 18.6 | 20.3 | ## +------+------+------+------+------+------+ ## | 6 | 163 | 247 | 32 | 19 | 20.9 | ## +------+------+------+------+------+------+ ## | 7 | 157 | 238 | 30.9 | 18.4 | 20.2 | ## +------+------+------+------+------+------+ ## | 8 | 155 | 239 | 32.8 | 18.6 | 21.2 | ## +------+------+------+------+------+------+ ## | 9 | 164 | 248 | 32.7 | 19.1 | 21.1 | ## +------+------+------+------+------+------+ ## | 10 | 158 | 238 | 31 | 18.8 | 22 | ## +------+------+------+------+------+------+ ## | 11 | 158 | 240 | 31.3 | 18.6 | 22 | ## +------+------+------+------+------+------+ ## | 12 | 160 | 244 | 31.1 | 18.6 | 20.5 | ## +------+------+------+------+------+------+ ## | 13 | 161 | 246 | 32.3 | 19.3 | 21.8 | ## +------+------+------+------+------+------+ ## | 14 | 157 | 245 | 32 | 19.1 | 20 | ## +------+------+------+------+------+------+ ## | 15 | 157 | 235 | 31.5 | 18.1 | 19.8 | ## +------+------+------+------+------+------+ ## | 16 | 156 | 237 | 30.9 | 18 | 20.3 | ## +------+------+------+------+------+------+ ## | 17 | 158 | 244 | 31.4 | 18.5 | 21.6 | ## +------+------+------+------+------+------+ ## | 18 | 153 | 238 | 30.5 | 18.2 | 20.9 | ## +------+------+------+------+------+------+ ## | 19 | 155 | 236 | 30.3 | 18.5 | 20.1 | ## +------+------+------+------+------+------+ ## | 20 | 163 | 246 | 32.5 | 18.6 | 21.9 | ## +------+------+------+------+------+------+ ## | 21 | 159 | 236 | 31.5 | 18 | 21.5 | ## +------+------+------+------+------+------+ ## | 22 | 155 | 240 | 31.4 | 18 | 20.7 | ## +------+------+------+------+------+------+ ## | 23 | 156 | 240 | 31.5 | 18.2 | 20.6 | ## +------+------+------+------+------+------+ ## | 24 | 160 | 242 | 32.6 | 18.8 | 21.7 | ## +------+------+------+------+------+------+ ## | 25 | 152 | 232 | 30.3 | 17.2 | 19.8 | ## +------+------+------+------+------+------+ ## | 26 | 160 | 250 | 31.7 | 18.8 | 22.5 | ## +------+------+------+------+------+------+ ## | 27 | 155 | 237 | 31 | 18.5 | 20 | ## +------+------+------+------+------+------+ ## | 28 | 157 | 245 | 32.2 | 19.5 | 21.4 | ## +------+------+------+------+------+------+ ## | 29 | 165 | 245 | 33.1 | 19.8 | 22.7 | ## +------+------+------+------+------+------+ ## | 30 | 153 | 231 | 30.1 | 17.3 | 19.8 | ## +------+------+------+------+------+------+ ## | 31 | 162 | 239 | 30.3 | 18 | 23.1 | ## +------+------+------+------+------+------+ ## | 32 | 162 | 243 | 31.6 | 18.8 | 21.3 | ## +------+------+------+------+------+------+ ## | 33 | 159 | 245 | 31.8 | 18.5 | 21.7 | ## +------+------+------+------+------+------+ ## | 34 | 159 | 247 | 30.9 | 18.1 | 19 | ## +------+------+------+------+------+------+ ## | 35 | 155 | 243 | 30.9 | 18.5 | 21.3 | ## +------+------+------+------+------+------+ ## | 36 | 162 | 252 | 31.9 | 19.1 | 22.2 | ## +------+------+------+------+------+------+ ## | 37 | 152 | 230 | 30.4 | 17.3 | 18.6 | ## +------+------+------+------+------+------+ ## | 38 | 159 | 242 | 30.8 | 18.2 | 20.5 | ## +------+------+------+------+------+------+ ## | 39 | 155 | 238 | 31.2 | 17.9 | 19.3 | ## +------+------+------+------+------+------+ ## | 40 | 163 | 249 | 33.4 | 19.5 | 22.8 | ## +------+------+------+------+------+------+ ## | 41 | 163 | 242 | 31 | 18.1 | 20.7 | ## +------+------+------+------+------+------+ ## | 42 | 156 | 237 | 31.7 | 18.2 | 20.3 | ## +------+------+------+------+------+------+ ## | 43 | 159 | 238 | 31.5 | 18.4 | 20.3 | ## +------+------+------+------+------+------+ ## | 44 | 161 | 245 | 32.1 | 19.1 | 20.8 | ## +------+------+------+------+------+------+ ## | 45 | 155 | 235 | 30.7 | 17.7 | 19.6 | ## +------+------+------+------+------+------+ ## | 46 | 162 | 247 | 31.9 | 19.1 | 20.4 | ## +------+------+------+------+------+------+ ## | 47 | 153 | 237 | 30.6 | 18.6 | 20.4 | ## +------+------+------+------+------+------+ ## | 48 | 162 | 245 | 32.5 | 18.5 | 21.1 | ## +------+------+------+------+------+------+ ## | 49 | 164 | 248 | 32.3 | 18.8 | 20.9 | ## +------+------+------+------+------+------+ Birds 1 to 21 survived a severe storm near Brown University in Rhode Island while the remainder died. (Original source Bumpus 1898.) Note that five measurements were taken from each bird, this is an example of multivariate analysis. Questions of interest might be: How are the measurements related to with one another? Do the survivors and non-survivors have significantly different mean values in the variables? Do the survivors and non-survivors show similar amounts of variation for the variables? Do the variables provide similar information to distinguish the survivors and non-survivors? Is it possible to construct a function of the variables that separates the survivors and non-survivors? Example 2: Spam or E-mail? The Spam Email database contains 4601 instances and 57 explanatory variables: 48 continuous explanatory variables in the forms of word_freq_WORD = percentage of words in the e-mail that match WORD. A “word” in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string. For example, \\[ \\mbox{word_freq_edu}=100 \\times \\frac{\\mbox{number of times the &quot;edu&quot; appears in the e-mail}}{\\mbox{total number of words in e-mail}} \\] 6 continuous variables in terms of char_freq_CHAR= percentage of characters in the e-mail that match CHAR. The characters can be $, !, etc. 1 continuous variable in the forms of capital_run_length_average= average length of uninterrupted sequences of capital letters. 1 continuous variable of type capital_run_length_longest= length of longest uninterrupted sequence of capital letters 1 continuous integer variable of type capital_run_length_total= sum of length of uninterrupted sequences of capital letters= total number of capital letters in the e-mail 1 categorical variable indicating whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail. Questions of interest: Could we find a subset of those 57 explanatory variables that separate spams and e-mails? Could we build a model that predicts a message’s probability of being a spam? Example 3: Classification of Iris The Iris flower data set is a multivariate data set introduced by Sir Ronald Fisher (1936) as an example of discriminant analysis. The data set consists of 50 samples from each of three species of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). Pictures of the three species can be found on http://en.wikipedia.org/wiki/Iris_flower_data_set. ## (np.float64(-0.5), np.float64(520.5), np.float64(340.5), np.float64(-0.5)) Four features were measured from each flower: the length and the width of the sepal and petal, in centimeters. The questions of interest for this data set are: Is there any two-dimensional sub-spaces that well separate the three species? For this discriminant/classification problem, what models are good in terms of misclassification rate? 1.3 Multivariate Methods Covered in STAT 372 We will cover the following topics in STAT 372 this semester: Matrix algebra. In multivariate analysis, at least two measurements are taken from each individual and hence the data are organized in vectors and matrices. Basic principles in matrices and vectors will be covered. Descriptive statistics for multivariate data. Before analyzing the data, we should explore the data numerically or/and graphically. For example, we should calculate the covariance matrix of the explanatory variables to see whether the variables are related to one another or not; we can also plot scatter plots to see their relationships. Multivariate normal distribution and measures of distance between two observations. In univariate cases, most of the inferential statistics are based on the assumption that the variable follows a normal distribution. This assumption is generalized to the multivariate cases. For some hypothesis tests, we need the joint distribution of the variables to be multivariate normal. It is important to quantify the distance between two observations (vectors), because if their distance is larger than a certain threshold, these two observations are significantly different. Hypothesis test about a mean vector. In univariate cases, a one-sample \\(t\\) test is used when making inferences on a population mean. In multivariate cases, the mean is a vector. The corresponding test statistic becomes Hotelling’s \\(T^2\\) which follows a distribution proportional to an F distribution. Hypothesis test about two or more mean vectors. We use a paired \\(t\\) test or two-sample \\(t\\) test when we compare two population means, and one-way ANOVA when comparing more than two means for univariate data. What are the counterparts for multivariate cases? Principal component analysis and factor analysis. Some multivariate applications involve a large number of variables with high correlation. The high dimensional data make it difficult to explore the relationship between variables and interpret the models. Principal component analysis and factor analysis are two different methods that explain the variation of the data using another set of variables, the number of variables is less than the original set. Discriminant analysis and classification. This is one of the most important applications of multivariate analysis. The objective is to classify the objects into two or more groups based on their measurements. The data sets usually consist of a set of explanatory variables and a response variable indicating which group the individual belongs to. Classification methods introduced in STAT 372 include Fisher’s linear discriminant analysis, Logistic regression, K-nearest neighbors and decision tree. Clustering analysis. Unlike a classification problem, we do not know the group label for the individuals in a clustering problem. Therefore, the objectives of a clustering analysis are to determine the number of groups and assign the individuals to the groups. The hierarchical methods and K-means method will be introduced. Canonical correlation analysis. The objective of canonical correlation analysis is to identify and quantify the associations between two sets of variables. Multidimensional scaling. An iterative process for finding coordinates in a lower dimension to present the similarity of the data. 1.4 Review: Univariate Analysis This section reviews the concepts of random variables, distribution of a random variable, expected value and variance of a random variable, one-sample \\(t\\) test, two-sample \\(t\\) test, paired \\(t\\) test, and one-way ANOVA F test. 1.4.1 Random Variable and Its Distribution Definition: A random variable is a function (or a mapping) from the sample space \\(S\\) into the real numbers. Random variables are usually denoted as uppercase letters, such as \\(X, Y, Z\\). A random variable can be either discrete or continuous. For a discrete random variable, we are able to list all the possible values and we use a probability mass function to describe its distribution. A continuous random variable maps the sample space into an interval; we can not list all of its possible values, and we use a probability density function to describe its distribution. 1.4.1.1 Discrete Random Variable Example 1: Discrete Random Variable and Its Distribution Flip an unbalanced coin twice, let \\(Y=\\)number of heads observed. Suppose \\(P(H)=\\frac{1}{3}\\) and \\(P(T)=\\frac{2}{3}\\) (this is just an assumption since the coin is not balanced). Denote \\(H\\)=event of observing a head; \\(T\\)=event of observing a tail. All possible outcomes of flipping a coin twice are \\(S=\\{HH, HT, TH, TT\\}\\). If the outcome is \\(\\{HH\\}\\), we have \\(Y=2\\); if the outcome is \\(\\{HT\\}\\), \\(Y=1\\). Therefore \\(Y\\) is a function (mapping) from the sample space \\(S\\) into real numbers as follows \\[ \\begin{array}{cc} \\hline S &amp; Y(S) \\\\ \\hline HH &amp; 2 \\\\ HT &amp; 1 \\\\ TH &amp; 1 \\\\ TT &amp; 0 \\\\ \\hline \\end{array} \\] The possible values for \\(Y\\) is 0, 1, and 2. Consider \\(\\{Y=0\\}\\) as an event, we can assign probability to that event, which is \\(P(Y=0)=P(\\{TT\\})=P(\\text{observing a tail in the first flip})\\times P(\\text{observing a tail in the second flip})=\\frac{2}{3}\\times \\frac{2}{3}=\\frac{4}{9}\\). We use the notation \\(p(y)\\) to represent \\(P(Y=y)\\), e.g., \\(p(0)\\) is the probability that the random variable \\(Y\\) takes value \\(Y=0\\), i.e., \\(p(0)=P(Y=0)\\). Similarly, \\[ \\begin{aligned} p(1)&amp;=P(Y=1)=P(\\{HT\\} \\mbox{ or } \\{TH\\})=P(\\{HT\\})+P(\\{TH\\})=\\frac{1}{3}\\times\\frac{2}{3}+\\frac{2}{3}\\times\\frac{1}{3}=\\frac{4}{9}\\\\ p(2)&amp;=P(Y=2)=P(\\{HH\\})=\\frac{1}{3}\\times\\frac{1}{3}=\\frac{1}{9}. \\end{aligned} \\] Definition Probability distribution of a discrete random variable lists all its possible values and their associated probabilities of a random variable. The probability distribution of \\(Y=\\)number of heads observed is \\[ \\begin{array}{c|c|c|c} \\hline y &amp; 0 &amp; 1 &amp; 2 \\\\ \\hline p(y) &amp; \\frac{4}{9} &amp; \\frac{4}{9} &amp; \\frac{1}{9} \\\\ \\hline \\end{array} \\] The first row of the table lists all the possible values of the random variable and the second row gives the corresponding probabilities of taking those values. For any discrete probability distribution, the following holds: \\(0\\le p(y)\\le1\\) for all \\(y\\). \\[ \\sum_{\\text{all possible y}} p(y) = 1 \\] , the sum of the probabilities of all possible values of \\(y\\) must be 1. It is obvious that the probability distribution of \\(Y=\\mbox{number of heads observed if flip a coin twice}\\) satisfies these two properties. Expected Value and Variance of a Random Variable or a Function of a Random Variable Suppose a class has 60 students, the following table summarizes the frequencies of the number of siblings the students have. \\[ \\begin{array}{c|c} \\hline \\text{# of Siblings}&amp;\\text{Frequencies}\\\\ \\hline 0&amp;6\\\\ 1&amp;27\\\\ 2&amp;24\\\\ 3&amp;3\\\\ \\hline &amp;60\\\\ \\hline \\end{array} \\] Randomly pick a student from the class, let \\(Y=\\mbox{# of siblings the student has}\\). Six students out of 60 have no sibling, by the equally-likely outcome model, \\[ p(0)=P(Y=0)=\\frac{6}{60}=0.1 \\] Therefore, the probability distribution of \\(Y\\) is \\[ \\begin{array}{c|c|c|c|c} \\hline y&amp;0&amp;1&amp;2&amp;3\\\\ \\hline p(y)&amp; \\frac{6}{60}&amp; \\frac{27}{60}&amp; \\frac{24}{60}&amp; \\frac{3}{60}\\\\ \\hline \\end{array} \\] What is average number of siblings those 60 students have? Since we observe 6 zeros, 27 ones, 24 twos, 3 threes, the average \\(\\mu\\) is given by \\[ \\mu=\\frac{0\\times 6+1\\times 27+2\\times 24+3\\times 3}{60}=0\\times \\frac{6}{60}+1\\times \\frac{27}{60}+2\\times \\frac{24}{60}+3\\times \\frac{3}{60}=\\sum_{y=0}^3 yp(y)=1.4 \\] Definition Let \\(Y\\) be a discrete random variable with the probability function \\(p(y)\\). The expected value of \\(Y\\), \\(E(Y)\\), is defined to be \\[ E(Y)=\\sum_{\\mbox{all } y}yp(y) \\] The expected value of \\(Y\\) is also called the expectation or the mean of \\(Y\\), denoted by the Geek letter \\(\\mu\\). The expected value is a weighted average over all possible values of the random variable \\(Y\\), weighted by the probability function \\(p(y)\\). Sometimes we may not be interested in the expected value of \\(Y\\) itself, but in some function of \\(Y\\), \\(g(Y)\\). For example, \\(g(Y)=(Y-\\mu)^2\\), the squared deviation of the observation to the mean. The expected value of \\(g(Y)=(Y-\\mu)^2\\), \\(E[g(Y)]\\) measures the variability of the observations and it is called the variance of the random variable. Definition Let \\(Y\\) be a discrete random variable with the probability function \\(p(y)\\) and \\(g(Y)\\) be a real-valued function of \\(Y\\). Then the expected value of \\(g(Y)\\) is \\[ E[g(Y)]=\\sum_{\\mbox{all } y} g(y) p(y). \\] If \\(Y\\) is a discrete random variable with mean \\(E(Y)=\\mu\\), the variance of \\(Y\\) is defined as \\[ \\mbox{Var}{(Y)}=E[(Y-\\mu)^2] \\] and denoted as \\(\\sigma^2\\). The positive square root of \\(Var{Y}\\) is the {\\(\\textit{standard deviation}\\)} of \\(Y\\), denoted as \\(\\sigma\\). That is \\[ \\sigma=\\sqrt{E[(Y-\\mu)^2]}. \\] The standard deviation \\(\\sigma\\) measures the average distance from the observations to the mean \\(\\mu\\). It might be easier to calculate the standard deviation using the following formula: \\[ \\sigma=\\sqrt{E(Y^2)-\\mu^2}. \\] 1.4.2 Properties of Expectation and Variance Think of the expectation operator \\(E\\) as being a linear operator in linear algebra. For any constant \\(c\\), \\[ E(c)=c; \\quad \\mbox{Var}(c)= 0 \\] Let \\(Y\\) be a random variable. For any constants \\(a\\) and \\(b\\) \\[ E(aY+b)=aE(Y) + b; \\quad \\mbox{Var}(aY+b)=a^2\\mbox{Var}(Y) + 0 \\] For any constants \\(a\\), \\(b\\) and function of \\(Y\\), \\(g(Y)\\), \\[ E[ag(Y)+b]=aE[g(Y)] + b; \\quad \\mbox{Var}[ag(Y)+b]=a^2\\mbox{Var[g(Y)] + 0} \\] For any constants \\(a\\), \\(b\\) and functions \\(g_1(Y)\\) and \\(g_2(Y)\\), \\[ E[ag_1(Y)+bg_2(Y)]=aE[g_1(Y)] + bE[g_2(Y)]; \\quad \\mbox{Var}[ag_1(Y)+bg_2(Y)]=a^2\\mbox{Var}[g_1(Y)] + b^2\\mbox{Var}[g_2(Y)] \\] Note that for a nonlinear function \\(g\\), we can not switch the order of the two operators. That is \\[ E[g(Y)]\\ne g(E(Y)). \\] For example, \\[ E\\left[\\frac{1}{Y}\\right]\\ne \\frac{1}{E(Y)}. \\] This is a common mistake. Example: Expectation and Variance of a Function of Variable The probability distribution of random variable \\(Y\\) is given in the following table: \\[ \\begin{array}{c|ccc} \\hline y&amp;\\quad 0&amp;\\quad 1 \\quad &amp; \\quad 3 \\quad\\\\ \\hline p(y)&amp;\\quad 0.4&amp;\\quad 0.4 \\quad&amp;\\quad0.2\\quad\\\\ \\hline \\end{array} \\] Find the expected value and variance of \\(g(Y)=2Y+1\\). The expected value is given by: \\[ E[g(Y)] = E[ 2Y + 1 ] = 2E(Y) + 1 \\\\ E(Y) = \\Sigma y.p(y) = (0*0.4) + (1*0.4) + (3*0.2) = 0.4 + 0.6 = 1\\\\ \\therefore E[g(Y)] = 2E(Y) + 1 = 2(1) + 1 = 3 \\\\ E[g(Y)] = 3 \\] The variance is given by: [ 4(Y) + 0 \\ (Y) = E[Y^2] - E[Y]^2 \\ = y^2p(y) - (1^2) = (0^20.4) + (1^20.4) + (3^2 * 0.2) - 1 \\ = 0.4 + (1.8) - 1 = 1.2 = (Y) \\ = 4 (Y) = 4*1.2 = 4.8 \\ [g(Y)] = 4.8 ] 1.4.3 Continuous Random Variables We use the so-called density curve and the area under the curve to describe continuous random variables. Let \\(f(y)\\) be the probability density function (pdf) of random variable \\(Y\\), \\(f(y)\\) has the following properties: The total area under the curve is 1. That is \\(\\int f(y)dy=1\\). The entire curve is above the horizontal axis. That is \\(f(y)\\ge 0\\). Area under the curve=probability. For example, \\(P(Y\\le t)=\\int_{-\\infty}^tf(y)dy\\); \\(P(a&lt;Y\\le b)=P(a\\le Y\\le b)=P(a&lt;Y&lt; b)=P(a&lt;Y&lt;b)=\\int_{a}^b f(y)dy\\). \\(P(Y=a)=0\\). Recall that the density function of a normal distribution \\(Y\\sim N(\\mu, \\sigma)\\) is given by \\[ f(y)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}, -\\infty&lt;y&lt;\\infty. \\] 1.4.3.1 Expected Values and Variance for Continuous Random Variables Let \\(Y\\) be a continuous random variable with probability density function \\(f(y)\\), The expected value of \\(Y\\) is defined as \\[ \\mbox{E}(Y)=\\int_{-\\infty}^{\\infty} yf(y)dy \\] provided that the integral exists. Let \\(g(Y)\\) be a function of \\(Y\\), then the expected value of \\(g(Y)\\) is given by \\[ \\mbox{E}[g(Y)]=\\int_{-\\infty}^{\\infty} g(y)f(y)dy \\] provided that the integral exists. The variance of \\(Y\\) is defined as \\[ \\mbox{Var}(Y)=\\mbox{E}(Y^2)-[\\mbox{E}(Y)]^2, \\text{ with } \\mbox{E}(Y^2)=\\int_{-\\infty}^{\\infty} y^2f(y)dy \\] Example: Binomial Distribution A quiz consists of 10 multiple choices questions with four choices A, B, C and D. I did not study and randomly picked one answer for each question. Find the probability that I got at least one correct answer. We can also use Python to get the answer from scipy.stats import binom prob_a1 = 1 - binom.pmf(0, n=10, p=0.25) # P(Y &gt; 0) = 1 - P(Y = 0) prob_a2 = binom.sf(0, n=10, p=0.25) # sf = 1 - cdf print(&quot;P(Y &gt; 0):&quot;, prob_a1, &quot;(using 1 - pmf)&quot;) ## P(Y &gt; 0): 0.9436864852905273 (using 1 - pmf) print(&quot;P(Y &gt; 0):&quot;, prob_a2, &quot;(using sf)&quot;) ## P(Y &gt; 0): 0.9436864852905273 (using sf) Find the probability that I got at least 80% correct. We can use Python to get the answer. probs_b = binom.pmf([8, 9, 10], n=10, p=0.25) # P(Y = 8 or 9 or 10) sum_b = probs_b.sum() prob_b_tail = binom.sf(7, n=10, p=0.25) # P(Y &gt; 7) print(&quot;P(Y = 8,9,10):&quot;, probs_b) ## P(Y = 8,9,10): [3.86238098e-04 2.86102295e-05 9.53674316e-07] print(&quot;Sum P(Y ≥ 8):&quot;, sum_b) ## Sum P(Y ≥ 8): 0.0004158020019531253 print(&quot;P(Y &gt; 7):&quot;, prob_b_tail) ## P(Y &gt; 7): 0.000415802001953125 Find the probability that I got two to three inclusive correct answers. We can use Python to get the answer. probs_c = binom.pmf([2, 3], n=10, p=0.25) # P(Y = 2 or 3) sum_c = probs_c.sum() # sum print(&quot;P(Y = 2,3):&quot;, probs_c) ## P(Y = 2,3): [0.28156757 0.25028229] print(&quot;Sum P(Y = 2 or 3):&quot;, sum_c) ## Sum P(Y = 2 or 3): 0.5318498611450198 On average, how many correct answers do I expect to obtain? Find the standard deviation of \\(Y\\) = # of correct answers. Revisit Learning Learning Outcomes After finishing this chapter, students should be able to Classify variables as qualitative/categorical nominal, qualitative/categorical ordinal, quantitative continuous, or quantitative discrete. Calculate the mean (expected value) and variance of a discrete random variable based on its probability distribution. Select the proper discrete probability distribution among Bernoulli, binomial, multinomial and Poisson distributions to model a random variable associated with the given application. Explain the main idea of the maximum likelihood estimation method. Derive the maximum likelihood estimate for the probability of success (proportion) graphically and theoretically based on the Bernoulli distribution. Derive the maximum likelihood estimate for the rate of an event graphically and theoretically based on the Poisson distribution. Explain the trade-off between type-I and type-II error. Relate the power and sample size calculation of a one-sample z test. Use R or an online app to obtain the sample size of a t test. "],["matrix-algebra.html", "2 Matrix Algebra Learning Outcomes 2.1 Vectors 2.2 Matrices 2.3 Mean Vectors and Covariance Matrices 2.4 Sample Mean Vector and Covariance Matrix 2.5 Review Exercises Revisit the Learning Outcomes", " 2 Matrix Algebra Learning Outcomes After finishing this note, students should be able to: Represent the data using vectors and matrices. Conduct basic matrix calculations such as addition, multiplication, and transposing. Verify whether a matrix is symmetric, orthogonal, positive definite, or positive semidefinite. Find the eigenvalues and corresponding eigenvectors of a \\(2\\times 2\\), \\(3 \\times 3\\), and simple matrix. Conduct a spectral decomposition and/or a singular-value decomposition of a given matrix. Find the mean vector and variance-covariance matrix for a given multivariate distribution. Find the sample mean vector and sample variance-covariance matrix of a data set using R. 2.1 Vectors An array \\(\\mathbf{x}\\) of \\(n\\) real numbers \\(x_1, x_2, \\cdots, x_n\\) is called a vector and is written as a column vector \\[ \\mathbf{x}= \\left[ \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{array} \\right] \\mbox{ or } \\quad \\mathbf{x}^{T}=[x_1, x_2, \\cdots, x_n] \\] where the “\\(T\\)” denotes the operation of transposing a column to a row vector. Take the bird data for example, each bird has five measurements: \\(X_1\\)=total length, \\(X_2\\)=alar length, \\(X_3\\)=length of beak and head, \\(X_4\\)=length of humerus, \\(X_5\\)=length of keel and sternum. As a result, the randomness is presented in a vector of five random variables. The measurements for the first bird is \\(\\mathbf{x_1}=[156, 245, 31.6, 18.5, 20.5]^{T}\\), for the second bird \\(\\mathbf{x_2}=[154, 240, 30.4, 17.9, 19.6]^{T}\\). In general, the measurements for the \\(i\\)th bird is \\(\\mathbf{x_i}=[x_{i1}, x_{i2}, \\cdots, x_{ip}]^{T}\\), where \\(p\\) is the number of explanatory variables, \\(p=5\\) in the bird example. 2.1.1 Some Basic Operations on Vectors For any constant \\(c\\), \\[ c\\mathbf{x}= \\left[ \\begin{array}{c} cx_1\\\\ cx_2\\\\ \\vdots\\\\ cx_n \\end{array} \\right] \\] The length of the vector \\(\\mathbf{x}=[x_1, x_2, \\cdots, x_n]^{T}\\) is \\[ L_{\\mathbf{x}}=||\\mathbf{x}||=\\sqrt {\\mathbf{x}^T\\mathbf{x}}=\\sqrt{[x_1, x_2, \\cdots, x_n]\\left[ \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{array} \\right]}=\\sqrt{x_1^2+x_2^2+\\cdots+x_n^2}. \\] For any two vectors of the same number of elements, the sum of the two vectors is \\[ \\mathbf{x}+\\mathbf{y}= \\left[ \\begin{array}{c} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{array} \\right]+\\left[ \\begin{array}{c} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{array} \\right]=\\left[ \\begin{array}{c} x_1+y_1\\\\ x_2+y_2\\\\ \\vdots\\\\ x_n+y_n \\end{array} \\right] \\] For any two vectors with the same number of elements, the \\({\\it inner \\: product}\\) of the two vectors is \\[ \\mathbf{x}^{T}\\mathbf{y}=[x_1, x_2, \\cdots, x_n]\\left[ \\begin{array}{c} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{array} \\right]=x_1y_1+x_2y_2+\\cdots+x_ny_n=\\sum x_iy_i. \\] A special case of the inner product is \\(L_{\\mathbf{x}}=||\\mathbf{x}||=\\sqrt{\\mathbf{x}^{T}\\mathbf{x}}=\\sqrt{\\sum x_i^2}\\). Let \\(\\theta\\) be the angle of two vectors with the same of elements, we have \\[ \\cos{\\theta}=\\frac{\\mathbf{x}^{T}\\mathbf{y}}{L_{\\mathbf{x}}L_{\\mathbf{y}}}=\\frac{\\mathbf{x}^{T}\\mathbf{y}}{||\\mathbf{x}||.||\\mathbf{y}||}=\\frac{\\sum x_iy_i}{\\sqrt{(\\sum x_i^2)(\\sum y_i^2)}}. \\] This can be proved by the law of cosines: \\(c^2=a^2+b^2-2ab\\cos \\theta\\), where \\(a, b,c\\) are the sides of a triangle and \\(\\theta\\) is the angle between sides \\(a\\) and \\(b\\). Two vectors are perpendicular if and only if \\(\\mathbf{x}^{T}\\mathbf{y}=0\\). Could you find any connection between \\(\\cos{\\theta}\\) and the correlation coefficient between the two variables \\(r\\)? A set of vectors \\(\\mathbf{x_1}, \\mathbf{x_2}, \\cdots, \\mathbf{x_k}\\) of the same dimension is said to be linearly dependent if there exist constants \\(c_1, c_2, \\cdots, c_k\\), not all zero, such that \\[ c_1\\mathbf{x_1}+c_2\\mathbf{x_2}+\\cdots+c_k\\mathbf{x_k}=\\mathbf{0}. \\] The projection of a vector \\(\\mathbf{x}\\) on another vector \\(\\mathbf{y}\\) is \\[ \\mbox{Projection of $\\mathbf{x}$ on $\\mathbf{y}$}=\\frac{\\mathbf{x}^{T}\\mathbf{y}}{\\mathbf{y}^{T}\\mathbf{y}}\\mathbf{y}. \\] 2.2 Matrices Putting all the birds together, one bird in one row, we will obtain the data matrix \\[ \\mathbf{X}= \\left[ \\begin{array}{ccccc} x_{11} &amp;x_{12}&amp; x_{13}&amp;\\cdots&amp; x_{1p}\\\\ x_{21} &amp;x_{22}&amp; x_{23}&amp;\\cdots&amp; x_{2p}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ x_{n1} &amp;x_{n2}&amp; x_{n3}&amp;\\cdots&amp; x_{np} \\end{array} \\right] \\] which is a \\(n\\times p\\) matrix with \\(n\\)= the number of individuals. In the bird example \\(n=49\\). In general, an \\(m\\times n\\) matrix is an array of numbers with \\(m\\) rows and \\(n\\) columns of the form \\[ \\mathbf{A}= \\left[ \\begin{array}{ccccc} a_{11} &amp;a_{12}&amp; a_{13}&amp;\\cdots&amp; a_{1n}\\\\ a_{21} &amp;a_{22}&amp; a_{23}&amp;\\cdots&amp; a_{2n}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ a_{m1} &amp;a_{m2}&amp; a_{m3}&amp;\\cdots&amp; a_{mn} \\end{array} \\right] \\] We use bold upper-case letters to represent matrices and bold lower-case letters to represent vectors. If \\(m=n\\), then the matrix is a square matrix. The transpose of the matrix is obtained by interchanging the rows and the columns, that is \\[ \\mathbf{A}^{T}= \\left[ \\begin{array}{ccccc} a_{11} &amp;a_{21}&amp; a_{31}&amp;\\cdots&amp; a_{m1}\\\\ a_{12} &amp;a_{22}&amp; a_{32}&amp;\\cdots&amp; a_{m2}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ a_{1n} &amp;a_{2n}&amp; a_{3n}&amp;\\cdots&amp; a_{mn} \\end{array} \\right] \\] which is a \\(n\\times m\\) matrix. Some special matrices include: Zero matrix: all elements are 0. Diagonal matrix: All the off-diagonal entries are 0 and not all of the diagonal entries are 0. Identity matrix \\(\\mathbf{I}\\): all diagonal entries are 1 and all off-diagonal entries are 0. A matrix \\(\\mathbf{A}\\) is \\({\\it symmetric}\\), if \\(\\mathbf{A}^{T}=\\mathbf{A}\\). A matrix \\(\\mathbf{A}\\) is \\({\\it orthogonal}\\) if \\(\\mathbf{A}^{-1}=\\mathbf{A}^{T}\\). 2.2.1 Basic Operations on Matrix The trace of square matrix \\(\\mathbf{A}_{n\\times n}\\) is defined as the sum of the diagonal elements. That is \\(\\mbox{Trace}(\\mathbf{A})=\\displaystyle\\sum_{i=1}^n a_{ii}\\). For an square matrix \\(\\mathbf{A}_{n\\times n}\\), if there exists another square matrix \\(\\mathbf{B}_{n\\times n}\\), such that \\(\\mathbf{A}\\mathbf{B}=\\mathbf{B}\\mathbf{A}=\\mathbf{I}_{n\\times n}\\), then \\(\\mathbf{B}\\) is called the {} of \\(\\mathbf{A}\\). The inverse of a matrix does not necessarily exist, the condition that the inverse of \\(\\mathbf{A}_{n\\times n}\\) exists is that the \\(n\\) columns of \\(\\mathbf{A}\\) are linearly independent. For any constant \\(c\\), we have \\[ c\\mathbf{A}= \\left[ \\begin{array}{ccccc} ca_{11} &amp;ca_{12}&amp; ca_{13}&amp;\\cdots&amp; ca_{1n}\\\\ ca_{21} &amp;ca_{22}&amp; ca_{23}&amp;\\cdots&amp; ca_{2n}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ ca_{m1} &amp;ca_{m2}&amp; ca_{m3}&amp;\\cdots&amp; ca_{mn} \\end{array} \\right] \\] For two matrices \\(\\mathbf{A}_{m\\times n}\\) and \\(\\mathbf{B}_{m\\times n}\\), their sum is defined as \\[ \\mathbf{A}+\\mathbf{B}= \\left[ \\begin{array}{ccccc} a_{11} +b_{11}&amp;a_{12}+b_{12}&amp; a_{13}+b_{13}&amp;\\cdots&amp; a_{1n}+b_{1n}\\\\ a_{21} +b_{21}&amp;a_{22}+b_{22}&amp; a_{23}+b_{23}&amp;\\cdots&amp; a_{2n}+b_{2n}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ a_{m1}+b_{m1} &amp;a_{m2}+b_{m2}&amp; a_{m3}+b_{m3}&amp;\\cdots&amp; a_{mn}+b_{mn} \\end{array} \\right] \\] For two matrices \\(\\mathbf{A}_{m\\times k}\\) and \\(\\mathbf{B}_{k\\times n}\\), their product is a \\(m \\times n\\) matrix whose entry on the \\(i\\)th row and \\(j\\)th column is defined as \\([\\mathbf{A}\\mathbf{B}]_{i,j}=\\sum_{t=1}^k a_{it}b_{tj}\\). In general, \\(\\mathbf{A}\\mathbf{B}\\ne \\mathbf{B}\\mathbf{A}\\) \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) \\((\\mathbf{A}+\\mathbf{B})+\\mathbf{C}=\\mathbf{A}+(\\mathbf{B}+\\mathbf{C})\\) \\(\\mathbf{C}(\\mathbf{A}+\\mathbf{B})=\\mathbf{C}\\mathbf{A}+\\mathbf{C}\\mathbf{B}\\) \\((\\mathbf{A}\\mathbf{B})\\mathbf{C}=\\mathbf{A}(\\mathbf{B}\\mathbf{C})\\) \\(\\mathbf{C}(\\mathbf{A}\\mathbf{B})=(\\mathbf{C}\\mathbf{A})\\mathbf{B}\\) \\((\\mathbf{A}^{-1})^{T}=(\\mathbf{A}^{T})^{-1}\\) \\((\\mathbf{A}\\mathbf{B})^{T}=\\mathbf{B}^{T}\\mathbf{A}^{T}\\) \\((\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{B}^{-1}\\mathbf{A}^{-1}\\) 2.2.2 Eigenvalues and Eigenvectors A square matrix \\(\\mathbf{A}\\) is said to have an eigenvalue \\(\\lambda\\) with a corresponding eigenvector \\(\\mathbf{x}\\ne \\mathbf{0}\\) if \\(\\mathbf{Ax}=\\lambda\\mathbf{x}\\). The eigenvalues can be found by solving \\(|\\mathbf{A}-\\lambda \\mathbf{I}|=0\\). For a certain eigenvalue \\(\\lambda\\), its corresponding eigenvector \\(\\mathbf{x}\\) is not unique, we can obtain the normalized eigenvector \\(\\mathbf{e}=\\frac{\\mathbf{x}}{||\\mathbf{x}||}\\) which is unique up to the sign. Steps to find the eigenvalue and eigenvector pairs: Solve the eigen-polynomial \\(|\\mathbf{A}-\\lambda \\mathbf{I}|=0\\) for the eigenvalues. For each eigenvalue \\(\\lambda_i\\), solve for the corresponding eigenvector \\(\\mathbf{x}_i\\) by \\(\\mathbf{A}\\mathbf{x}_i=\\lambda_i\\mathbf{x}_i\\). Convert the eigenvector \\(\\mathbf{x}_i\\) to a unit vector \\(\\mathbf{e}_i=\\frac{\\mathbf{x}_i}{||\\mathbf{x}_i||}\\). Example: Eigenvalues and Eigenvectors Find the eigenvalue(s) and eigenvector(s) of matrix \\[ \\mathbf{A}= \\left[ \\begin{array}{cc} 1&amp; -5\\\\ -5&amp; 1 \\end{array} \\right] \\] import numpy as np A = np.array([[1,-5], [-5,1]]) eigenvalues, eigenvectors = np.linalg.eig(A) print(eigenvalues) ## [ 6. -4.] print(eigenvectors) ## [[ 0.70710678 0.70710678] ## [-0.70710678 0.70710678]] Note: the unit eigenvectors can be \\[\\mathbf{e_1}=\\left[ \\begin{array}{c} -\\frac{1}{\\sqrt{2}}\\\\\\frac{1}{\\sqrt{2}} \\end{array} \\right] \\] for \\(\\lambda_1=6\\) and \\[\\mathbf{e_2}=\\left[ \\begin{array}{c} -\\frac{1}{\\sqrt{2}}\\\\-\\frac{1}{\\sqrt{2}} \\end{array} \\right] \\] for \\(\\lambda_2=-4\\) as well, since the unit eigenvectors are unique up to the sign. 2.2.3 Spectral (Eigen) Decomposition The spectral decomposition of a \\(n\\times n\\) symmetric matrix \\(\\mathbf{A}\\) is given by \\[ \\mathbf{A}=\\lambda_1\\mathbf{e_1}\\mathbf{e_1}^{T}+\\lambda_2\\mathbf{e_2}\\mathbf{e_2}^{T}+\\cdots+\\lambda_n\\mathbf{e_n}\\mathbf{e_n}^{T}=\\sum_{i=1}^n \\lambda_i\\mathbf{e_i}\\mathbf{e_i}^{T} \\] where \\(\\lambda_i\\) are the eigenvalues of \\(\\mathbf{A}\\) and \\(\\mathbf{e_i}\\) are the corresponding normalized eigenvectors. The spectral decomposition can be also expressed in a quadratic form as follows: \\[ \\mathbf{A}=\\sum_{i=1}^n \\lambda_i\\mathbf{e_i}\\mathbf{e_i}^{T}=\\mathbf{P}\\mathbf{\\Lambda}\\mathbf{P}^{T} \\] where \\(\\mathbf{P}=[\\mathbf{e_1}, \\mathbf{e_2}, \\cdots, \\mathbf{e_n}]\\) (the \\(i\\)th column is the eigenvector associated with the \\(i\\)th eigenvalue \\(\\lambda_i\\)) and \\(\\mathbf{\\Lambda}\\) is the diagonal matrix with the eigenvalues as elements \\[ \\mathbf{\\Lambda}= \\left[ \\begin{array}{ccccc} \\lambda_1 &amp;0&amp;0&amp;\\cdots&amp; 0\\\\ 0&amp;\\lambda_2&amp;0&amp;\\cdots&amp; 0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;\\cdots&amp; \\lambda_n \\end{array} \\right]. \\] Not all square matrices have a spectral decomposition, only diagonalizable matrices can be factorized in this way. If an \\(n\\times n\\) square matrix \\(\\mathbf{A}\\) has \\(n\\) distinct eigenvalues, it is diagonalizable and has a spectral decomposition. Theorem: Diagonalizable The following statements are equivalent: The symmetric matrix \\(\\mathbf{A}\\) is diagonalizable. The sum of the geometric multiplicities of all eigenvalues equals to \\(n\\): \\(\\sum m_g(\\lambda)=n\\). The geometric multiplicity of the eigenvalue \\(\\lambda\\), denoted as \\(m_g(\\lambda)\\), is the rank of the matrix \\(\\mathbf{A}-\\lambda \\mathbf{I}\\). For each eigenvalue \\(\\lambda\\), the algebraic multiplicity equals the geometric multiplicity. That is \\(m_g(\\lambda)=m_a(\\lambda)\\), where the algebraic multiplicity of an eigenvalue \\(\\lambda\\) is the number of times \\(\\lambda\\) appears as a root of the characteristic polynomial \\(|\\mathbf{A}-\\lambda \\mathbf{I}|\\). Note that \\(1\\le m_g(\\lambda)\\le m_a(\\lambda)\\). Example: Spectral Decomposition [(a)] Verify that \\[\\mathbf{A}= \\left[ \\begin{array}{cc} 1&amp; -5\\\\ -5&amp; 1 \\end{array} \\right] \\] is diagonalizable and has a spectral decomposition. [(b)] Write down the spectral decomposition of \\(\\mathbf{A}\\). Definition A symmetric matrix \\(\\mathbf{A}_{n\\times n}\\) is called positive semidefinite if the quadratic form \\(\\mathbf{x}^T\\mathbf{A} \\mathbf{x}\\ge 0\\) for all \\(\\mathbf{x} \\in \\Re^n\\). It is called positive definite if \\(\\mathbf{x}^T\\mathbf{A} \\mathbf{x}&gt; 0\\) for all non-zero \\(\\mathbf{x} \\in \\Re^n\\). Theorem: Positive Semidefinite The following statements are equivalent: The symmetric matrix \\(\\mathbf{A}\\) is positive semidefinite. All eigenvalues of \\(\\mathbf{A}\\) are nonnegative. There exists \\(\\mathbf{B}\\) such that \\(\\mathbf{A}=\\mathbf{B}^T\\mathbf{B}\\). Theorem: Positive Definite The following statements are equivalent: The symmetric matrix \\(\\mathbf{A}\\) is positive definite. All eigenvalues of \\(\\mathbf{A}\\) are positive. There exists nonsingular square \\(\\mathbf{B}\\) such that \\(\\mathbf{A}=\\mathbf{B}^T\\mathbf{B}\\). It can be shown that a symmetric matrix \\(\\mathbf{A}\\) is positive definite if and only if every eigenvalue of \\(\\mathbf{A}\\) is positive. 2.2.4 Singular-Value Decomposition Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. There exists an \\(m \\times r\\) orthogonal matrix \\(\\mathbf{U}\\) and a \\(n \\times r\\) orthogonal matrix \\(\\mathbf{V}\\), such that \\[ \\mathbf{A}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T}=\\sum_{i=1}^r \\sigma_i \\mathbf{u}_i\\mathbf{v}_i \\] \\(\\mathbf{U}\\): the left singular vectors, it is an \\(m\\times r\\) column orthonormal matrix. That is \\(\\mathbf{U}^T \\mathbf{U}=\\mathbf{I}\\), with \\(1&lt;r\\le \\min(m, n)\\). \\(\\mathbf{V}\\): the right singular vectors, it is an \\(n \\times r\\) column orthonormal matrix. That is \\(\\mathbf{V}^T \\mathbf{V}=\\mathbf{I}\\). \\(\\mathbf{\\Sigma}\\): singular values, it is an \\(r \\times r\\) diagonal matrix with positive entries sorted in decreasing order. The is \\(\\sigma_1\\ge \\sigma_2\\ge \\cdots &gt;0\\). Unlike spectral decomposition, singular-value decomposition is defined for all matrices (rectangle or square). Note that \\[ \\mathbf{A}\\mathbf{A}^T=(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T})(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T})^T=(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T}) (\\mathbf{V}\\mathbf{\\Sigma}^T\\mathbf{U}^T)=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{\\Sigma}^T\\mathbf{U}^{T} \\] Therefore, \\(\\mathbf{U}\\) contains the eigenvectors of the \\(m \\times m\\) matrix \\(\\mathbf{A}\\mathbf{A}^T\\). Similarly \\[ \\mathbf{A}^T\\mathbf{A}=(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T})^T(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T})=(\\mathbf{V}\\mathbf{\\Sigma}^T\\mathbf{U}^{T}) (\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T)=\\mathbf{V}\\mathbf{\\Sigma}\\mathbf{\\Sigma}^T\\mathbf{V}^{T} \\] Therefore, \\(\\mathbf{V}\\) contains the eigenvectors of the \\(n \\times n\\) matrix \\(\\mathbf{A}^T\\mathbf{A}\\). Since \\[ \\mathbf{\\Sigma}= \\left[ \\begin{array}{ccccc} \\sigma_1 &amp;0&amp;0&amp;\\cdots&amp; 0\\\\ 0&amp;\\sigma_2&amp;0&amp;\\cdots&amp; 0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;\\cdots&amp; \\sigma_r \\end{array} \\right] \\] is a diagonal matrix, and \\[ \\mathbf{\\Sigma}\\mathbf{\\Sigma}^T=\\mathbf{\\Sigma}^T\\mathbf{\\Sigma}=\\left[ \\begin{array}{ccccc} \\sigma_1^2 &amp;0&amp;0&amp;\\cdots&amp; 0\\\\ 0&amp;\\sigma_2^2&amp;0&amp;\\cdots&amp; 0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;\\cdots&amp; \\sigma_r^2 \\end{array} \\right] \\]. Therefore, the entries of \\(\\mathbf{\\Sigma}\\) are the square root of the largest \\(r\\) eigenvalues of both \\(\\mathbf{A}\\mathbf{A}^T\\) and \\(\\mathbf{A}^T\\mathbf{A}\\). Steps to find the singular-value decomposition of an \\(m\\times n\\) matrix \\(\\mathbf{A}\\): Find the eigenvalues and unit eigenvectors of \\(\\mathbf{A}\\mathbf{A}^T\\). Arrange the eigenvalues from the largest to the smallest, and their associated eigenvectors column-wise accordingly. Find the eigenvalues and unit eigenvectors of \\(\\mathbf{A}^T\\mathbf{A}\\). Arrange the eigenvalues from the largest to the smallest, and their associated eigenvectors column-wise accordingly. Let \\(r\\) be the number of positive eigenvalues for both \\(\\mathbf{A}\\mathbf{A}^T\\) and \\(\\mathbf{A}^T\\mathbf{A}\\), \\(\\mathbf{u}_i\\) be the unit eigenvector corresponding to the \\(i\\)th largest positive common value of \\(\\mathbf{A}\\mathbf{A}^T\\), \\(\\mathbf{v}_i\\) be the unit eigenvector corresponding to the \\(i\\)th largest positive common value of \\(\\mathbf{A}^T\\mathbf{A}\\). The singular-value decomposition of \\(\\mathbf{A}\\) is \\[ \\mathbf{A}=\\sum_{i=1}^r \\sqrt{\\lambda_i} \\mathbf{u}_i \\mathbf{v}_i^T. \\] 2.3 Mean Vectors and Covariance Matrices A random vector is a vector whose elements are random variables. Similarly, a random matrix is a matrix whose elements are random variables. The expected value of a random vector (matrix) is a vector (matrix) of expected values of each of its elements. Suppose \\(\\mathbf{X}=[X_1, X_2, \\cdots, X_p]^{T}\\) is a column vector of \\(p\\) random variables, each element of \\(\\mathbf{X}\\) has its own marginal probability distribution with mean \\(\\mu_i=E(X_i)\\) and variance \\(Var(X_i)=E(X_i-\\mu_i)^2\\). Given the joint distribution of any pair of two distinct variables \\(X_i\\) and \\(X_j\\), we can calculate their covariance by \\[ Cov(X_i, X_j)=E[(X_i-\\mu_i)(X_j-\\mu_j)]=E(X_iX_j)-\\mu_i\\mu_j \\] Consider all the possible distinct \\(_pC_2=\\frac{p(p-1)}{2}\\) pairs, their covariance yields the upper-triangular elements of the covariance matrix and the variance of each variable gives the elements on diagonal. Therefore, we have mean vector \\[ E(\\mathbf{X})=E\\left[ \\begin{array}{c} X_1\\\\X_2\\\\\\vdots\\\\X_p \\end{array} \\right]=\\left[ \\begin{array}{c} E(X_1)\\\\E(X_2)\\\\\\vdots\\\\E(X_p) \\end{array} \\right]=\\left[ \\begin{array}{c} \\mu_1\\\\ \\mu_2\\\\\\vdots\\\\ \\mu_p \\end{array} \\right]=\\mathbf{\\mu} \\] and the covariance matrix \\[ \\small \\begin{aligned} \\mathbf{\\Sigma}&amp;=E(\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})^{T}\\\\ &amp;=E\\left(\\left[ \\begin{array}{c} X_1-\\mu_1\\\\X_2-\\mu_2\\\\ \\vdots \\\\X_p-\\mu_p \\end{array} \\right] [X_1-\\mu_1, X_2-\\mu_2, \\cdots, X_p-\\mu_p]\\right)\\\\ &amp;=E\\left[ \\begin{array}{ccccc} (X_1-\\mu_1)^2 &amp;(X_1-\\mu_1)(X_2-\\mu_2)&amp;(X_1-\\mu_1)(X_3-\\mu_3)&amp;\\cdots&amp;(X_1-\\mu_1)(X_p-\\mu_p)\\\\ (X_2-\\mu_2)(X_1-\\mu_1) &amp;(X_2-\\mu_2)^2&amp; (X_2-\\mu_2)(X_3-\\mu_3)&amp;\\cdots&amp;(X_2-\\mu_2)(X_p-\\mu_p)\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ (X_p-\\mu_p)(X_1-\\mu_1)&amp;(X_p-\\mu_p)(X_2-\\mu_2)&amp; (X_p-\\mu_p)(X_3-\\mu_3)&amp;\\cdots&amp; (X_p-\\mu_p)^2 \\end{array} \\right]\\\\ &amp;=\\left[ \\begin{array}{ccccc} E[(X_1-\\mu_1)^2] &amp;E[(X_1-\\mu_1)(X_2-\\mu_2)]&amp;E[(X_1-\\mu_1)(X_3-\\mu_3)]&amp;\\cdots&amp;E[(X_1-\\mu_1)(X_p-\\mu_p)]\\\\ E[(X_2-\\mu_2)(X_1-\\mu_1)]&amp;E[(X_2-\\mu_2)^2]&amp; E[(X_2-\\mu_2)(X_3-\\mu_3)]&amp;\\cdots&amp;E[(X_2-\\mu_2)(X_p-\\mu_p)]\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ E[(X_p-\\mu_p)(X_1-\\mu_1)]&amp;E[(X_p-\\mu_p)(X_2-\\mu_2)]&amp; E[(X_p-\\mu_p)(X_3-\\mu_3)]&amp;\\cdots&amp; E[(X_p-\\mu_p)^2] \\end{array} \\right]\\\\ &amp;=\\left[ \\begin{array}{ccccc} \\sigma_{11} &amp;\\sigma_{12}&amp; \\sigma_{13}&amp;\\cdots&amp; \\sigma_{1p}\\\\ \\sigma_{21} &amp;\\sigma_{22}&amp; \\sigma_{23}&amp;\\cdots&amp; \\sigma_{2p}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ \\sigma_{p1} &amp;\\sigma_{p2}&amp; \\sigma_{p3}&amp;\\cdots&amp; \\sigma_{pp} \\end{array} \\right] \\end{aligned} \\] where \\(\\sigma_{ii}=Var(X_i)\\) and \\(\\sigma_{ij}=E[(X_i-\\mu_i)(X_j-\\mu_j)]=Cov(X_i, X_j)\\). The covariance matrix is a \\(p\\times p\\) symmetric matrix with variances as the diagonal elements and covariance of-diagonal elements. Example: Mean Vector and Covariance Matrix Find the mean vector and covariance matrix for the two random variables \\(X_1\\) and \\(X_2\\) with the marginal and joint probability distribution as follows: ## (np.float64(-0.5), np.float64(197.5), np.float64(117.5), np.float64(-0.5)) 2.4 Sample Mean Vector and Covariance Matrix In univariate case, we use the sample mean \\(\\bar x=\\frac{1}{n}\\sum x_i\\) and sample variance \\(s^2=\\frac{1}{n-1}\\sum (x_i-\\bar x)^2\\) to estimate the population mean \\(\\mu\\) and population variance \\(\\sigma^2\\) respectively. In multivariate cases, given the data matrix we can calculate the sample mean vector \\(\\mathbf{\\bar x}\\) and sample covariance matrix \\(\\mathbf{S}\\) as follows: \\[ \\mathbf{\\bar x}^{T}=[\\bar x_1, \\bar x_2, \\cdots, \\bar x_n] \\] where \\(\\bar x_j=\\frac{1}{n}\\displaystyle\\sum_{i=1}^n x_{ij}, j=1, 2, \\cdots, p\\); and \\[ \\mathbf{S}=\\left[\\begin{array}{ccc} s_{11} &amp;\\cdots&amp; s_{1p}\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ s_{p1}&amp; \\cdots&amp; s_{pp} \\end{array} \\right]=\\left[\\begin{array}{ccc} \\frac{1}{n-1}\\displaystyle\\sum_{j=1}^n (x_{j1}-\\bar x_1)^2 &amp;\\cdots&amp; \\frac{1}{n-1}\\displaystyle\\sum_{j=1}^n (x_{j1}-\\bar x_1)(x_{jp}-\\bar x_p)\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ \\frac{1}{n-1}\\displaystyle\\sum_{j=1}^n (x_{jp}-\\bar x_p)(x_{j1}-\\bar x_1)&amp; \\cdots&amp; \\frac{1}{n-1}\\displaystyle\\sum_{j=1}^n (x_{jp}-\\bar x_p)^2 \\end{array} \\right] \\] Like the population covariance matrix, the sample covariance matrix \\(\\mathbf{S}\\) is a \\(p\\times p\\) symmetric matrix. For the birds data, the sample mean vector is \\(\\mathbf{\\bar x}^{T}=[157.97959, 241.32653, 31.45918, 18.46939, 20.82653]\\) and the sample covariance matrix is \\[ \\mathbf{S}=\\left[\\begin{array}{ccccc} 13.354&amp; 13.611&amp; 1.922&amp; 1.331&amp; 2.192\\\\ 13.611&amp; 25.683&amp; 2.714&amp; 2.198&amp; 2.658\\\\ 1.922 &amp; 2.714&amp; 0.632&amp; 0.342&amp; 0.415\\\\ 1.331 &amp; 2.198&amp; 0.342&amp; 0.318&amp; 0.339\\\\ 2.192&amp; 2.658&amp; 0.415&amp; 0.339&amp; 0.983 \\end{array} \\right] \\] The eigenvalues of the above sample covariance matrix are \\(\\lambda_1=35.326, \\lambda_2=4.622, \\lambda_3=0.631, \\lambda_4=0.313, \\lambda_5=0.078\\). Since all the eigenvalues are positive, the sample covariance matrix is positive definite. 2.5 Review Exercises Given the matrix \\[\\mathbf{A}= \\left[ \\begin{array}{cc} 7&amp; 3\\\\ 3&amp; -1 \\end{array} \\right] \\] Explain whether \\(\\mathbf{A}\\) is symmetric. Find the eigenvalue(s) and corresponding eigenvector(s) of \\(\\mathbf{A}\\). Check that whether \\(\\mathbf{A}\\) is positive definite. Write the spectral decomposition of \\(\\mathbf{A}\\). Use R to confirm your answers in the first question. Given the matrix \\[\\mathbf{A}= \\left[ \\begin{array}{ccc} 3&amp; 1&amp;1\\\\ -1&amp;3&amp; 1 \\end{array} \\right] \\] Find the eigenvalues and eigenvectors of \\(\\mathbf{A}\\mathbf{A}^T\\). Find the eigenvalues and eigenvectors of \\(\\mathbf{A}^T\\mathbf{A}\\). Write the singular-value decomposition of \\(\\mathbf{A}\\). Use R to confirm your answers in the third question. Solution to Review Question 2: import numpy as np A = np.array([[7, 3], [3, -1]]) print(&quot;Matrix A:\\n&quot;, A) ## Matrix A: ## [[ 7 3] ## [ 3 -1]] print(&quot;Transpose of A:\\n&quot;, A.T) ## Transpose of A: ## [[ 7 3] ## [ 3 -1]] eigenvalues, eigenvectors = np.linalg.eig(A) print(&quot;Eigenvalues:\\n&quot;, eigenvalues) ## Eigenvalues: ## [ 8. -2.] print(&quot;Eigenvectors:\\n&quot;, eigenvectors) ## Eigenvectors: ## [[ 0.9486833 -0.31622777] ## [ 0.31622777 0.9486833 ]] Solution to Review Question 4: import numpy as np A = np.array([[3, 1, 1], [-1, 3, 1]]) print(&quot;Matrix A:\\n&quot;, A) ## Matrix A: ## [[ 3 1 1] ## [-1 3 1]] eigenvals1, eigenvecs1 = np.linalg.eig(A @ A.T) # eigen decomp of A @ Atranspose print(&quot;\\nEigenvalues of A @ ATranspose:\\n&quot;, eigenvals1) ## ## Eigenvalues of A @ ATranspose: ## [12. 10.] print(&quot;\\nEigenvectors of A @ ATranspose:\\n&quot;, eigenvecs1) ## ## Eigenvectors of A @ ATranspose: ## [[ 0.70710678 -0.70710678] ## [ 0.70710678 0.70710678]] eigenvals2, eigenvecs2 = np.linalg.eig(A.T @ A) # eigen decomp of Atranspose @ A print(&quot;\\nEigenvalues of ATranspose @ A:\\n&quot;, eigenvals2) ## ## Eigenvalues of ATranspose @ A: ## [ 0. 10. 12.] print(&quot;\\nEigenvectors of ATranspose @ A:\\n&quot;, eigenvecs2) ## ## Eigenvectors of ATranspose @ A: ## [[ 1.82574186e-01 -8.94427191e-01 -4.08248290e-01] ## [ 3.65148372e-01 4.47213595e-01 -8.16496581e-01] ## [-9.12870929e-01 1.60116688e-16 -4.08248290e-01]] # svd U, s, Vt = np.linalg.svd(A) S = np.zeros_like(A, dtype=float) S[:2, :2] = np.diag(s) A_reconstructed = U @ S @ Vt print(&quot;\\nReconstructed A:\\n&quot;, A_reconstructed) ## ## Reconstructed A: ## [[ 3. 1. 1.] ## [-1. 3. 1.]] Revisit the Learning Outcomes After finishing this note, students should be able to Represent the data using vectors and matrices. Conduct basic matrix calculations such as addition, multiplication, transposing. Verify whether a matrix is symmetric, orthogonal, positive definite, positive semidefinite. Find the eigenvalues and corresponding eigenvectors of a \\(2\\times 2\\), \\(3 \\times 3\\), and simple matrix. Conduct a spectral decomposition and/or a singular-value decomposition of a given matrix. Find the mean vector and variance-covariance matrix for a given multivariate distribution. Find the sample mean vector and sample variance-covariance matrix of a data set using R. "],["displaying-multivariate-data-and-measures-of-distance.html", "3 Displaying Multivariate Data and Measures of Distance Learning Outcomes 3.1 Display Multivariate Data 3.2 Distance in Multivariate Analysis 3.3 Multivariate Normal Distribution 3.4 The Sampling Distribution of \\(\\mathbf{\\bar X}\\) and \\(\\boldsymbol{S}\\) 3.5 Review Exercises Revisit the Learning Outcomes", " 3 Displaying Multivariate Data and Measures of Distance This note covers four main topics: How to display multivariate data? We will introduce scatterplot, scatterplot matrix, growth curve, stars plot, and Chernoff faces; How to measure the distance between multivariate observations? Manhattan distance, Euclidean distance, Mahalanobis distance, Hamming distance will be covered; Multivariate normal distribution. We focus on bivariate normal in this course; Sampling distribution of the sample mean vector \\(\\mathbf{\\bar X}\\) and the sample covariance matrix \\(\\mathbf{S}\\). Learning Outcomes After finishing this chapter, students should be able to Plot the index scatter plots matrix, draw a stars plot and Chernoff faces plot of a given multivariate data set using R, and interpret the plots. Verify whether a distance function is valid. Choose the proper distance metric to calculate the distance between observations based on the data types for multivariate data. Write down the density function of a multivariate normal distribution. Describe the properties of a multivariate normal distribution. Find a \\((1-\\alpha)\\times 100\\%\\) contour for a given bivariate normal distribution. Explain the distributions related to the sample mean vector and sample covariance matrix. 3.1 Display Multivariate Data In multivariate analysis, at least two measurements are taken from the same individuals. Before conducting any data analysis, we should examine the preliminary relationship among the data using graphs. We will cover several popular ways to display multivariate data. 3.1.1 Scatterplot We have seen the 2-dimensional scatter plot in Stat 151 or 141 when two measurements are taken on the same individuals, for example, the age and price of a used car. In a scatter plot, we put one variable in the \\(x\\)-axis and another one in the \\(y\\)-axis. From a scatter plot, we can tell the direction, form and strength of the relationship between the two variables. If three measurements are taken from each individual, a 3-dimensional scatter plot can be used with one variable on the \\(x\\), \\(y\\), \\(z\\) axis respectively. Take the Iris flowers data for example, a 2-dimensional scatter plot based on \\(\\verb`sepal width`\\) and \\(\\verb`petal width`\\) and a 3-dimensional scatter plot bases on \\(\\verb`sepal width`\\), \\(\\verb`petal width`\\) and \\(\\verb`sepal length`\\) are given below. Different symbols are used for three different species: red circle for Setosa, green triangle for Versicolor, and blue square for Virginica. A plot with different symbols for distinct groups is called an index plot. If there are more than 3 measurements, a matrix of scatter plots is used to explore the relationship between any two variables. Figure 1 shows the scatter plots matrix for the Iris flowers data. The scatter plots suggest that the variable \\(\\verb`petal width`\\) might be a useful variable which separates the three species. Figure 3.1: Scatter Plot Matrix of Iris Data Figure 3.1: Scatter Plot Matrix of Iris Data 3.1.2 Graphs of Growth Curves When the height of a child is measured at each birthday, the points can be plotted and connected by lines to produce a graph. This is an example of growth curve which is widely used for repeated measurements of the same characteristic on the same individuals at different visits. Figure 3.2 shows the reading ability of six kids at two different ages. Each individual measured twice; therefore, the measurements are repeated measurements. If this information is ignored, a scatter plot of \\(\\verb`reading ability`\\) versus \\(\\verb`age`\\) (left panel) suggests a negative association; that means reading ability drops when the kids get older, this is intuitively not true. The growth curve connects the two measurements on the same kids and it shows a positive association, that is reading ability grows when the kids get older. ## (np.float64(-0.5), np.float64(1023.5), np.float64(720.5), np.float64(-0.5)) Figure 3.2: Growth curves of kid’s reading ability at two different ages 3.1.3 Star Plots A star plot (or called radar chart) is a plot that consists of a sequence of equi-angular rays, with each ray representing one of the variables. The data length of a ray is proportional to the magnitude of the variable for the data point relative to the maximum magnitude of the variable across all data points. A line is drawn connecting the data values for each ray. This gives the plot a star-like appearance and the origin of one of the popular names for this plot. The star plot can be used to answer the following questions: Which observations are most similar? Are there clusters of observations? Are there outliers? What is the trend of change along time for repeated measurements? Figure @(fig:star1) is the stars plot of 36 randomly picked iris flowers, 12 from each species. The numbers under the stars plot are the ID number of the flower, we know that observations 1 to 50 belong to Setosa, 51 to 100 belong to Versicolor and 101 to 150 belong to Virginica. Could you tell how many clusters of flowers? ## (np.float64(-0.5), np.float64(1023.5), np.float64(1023.5), np.float64(-0.5)) Figure 3.3: Stars plot of 36 observations of the Iris flowers data data = load_iris(as_frame=True) df = data.frame df[&#39;Species&#39;] = data.target np.random.seed(4061) ind = np.sort(np.concatenate([ np.random.choice(range(0, 50), 12, replace=False), np.random.choice(range(50, 100), 12, replace=False), np.random.choice(range(100, 150), 12, replace=False) ])) df_sample = df.iloc[ind, :-1] # dropping species # normalizing df_norm = (df_sample - df_sample.min()) / (df_sample.max() - df_sample.min()) labels = df_sample.columns num_vars = len(labels) angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist() angles += angles[:1] fig, axs = plt.subplots(6, 6, figsize=(12, 8), subplot_kw=dict(polar=True)) axs = axs.flatten() for i, (ax, row) in enumerate(zip(axs, df_norm.values)): values = row.tolist() + [row[0]] # close the polygon ax.plot(angles, values, color=&#39;black&#39;, linewidth=0.5) ax.fill(angles, values, color=&#39;black&#39;, alpha=0.1) ax.set_xticks([]) ax.set_yticks([]) ax.set_ylim(0, 1) ax.set_title(str(ind[i]), fontsize=8, pad=2) for j in range(len(df_norm), len(axs)): fig.delaxes(axs[j]) plt.show() plt.tight_layout() plt.close() 3.1.4 Chernoff Faces Plot Chernoff faces, proposed by Herman Chernoff, is a novel method of representing multivariate data by a cartoon of a face whose features, such as length of nose and curvature of mouth, correspond to the variables. Thus each multivariate observation is visualized as a computer-drawn face. The implementation of Chernoff faces plot in R is able to take up to 15 variables, those variables correspond to the features of the face such as: height of face, width of face, structure of face, height of mouth, width of mouth, smiling, height of eyes, width of eyes, height of hair, width of hair, style of hair, height of nose, width of nose, width of ear, height of ear. Similar to stars plot, Chernoff faces are useful for identifying different groups (clusters) and showing changes over time for repeated measurements. Figure 3.4 shows the faces of 36 randomly picked iris flowers, 12 from each species. The 15 features of the faces correspond to sepal length, sepal width, petal length, petal width in cycles. ## (np.float64(-0.5), np.float64(1023.5), np.float64(1023.5), np.float64(-0.5)) Figure 3.4: Chernoff face plots of 36 observations of the Iris flowers data data = load_iris(as_frame=True) df = data.frame np.random.seed(4061) ind = np.sort(np.concatenate([ np.random.choice(range(0, 50), 12, replace=False), np.random.choice(range(50, 100), 12, replace=False), np.random.choice(range(100, 150), 12, replace=False) ])) df_sample = df.iloc[ind, :4] # 4 features - excluding species # normalizing df_norm = (df_sample - df_sample.min()) / (df_sample.max() - df_sample.min()) # Simple face plot function (MANUAL) def draw_face(ax, features): eye_size = 0.02 + 0.08 * features[0] mouth_curve = -0.5 + features[1] face_width = 0.3 + 0.3 * features[2] eyebrow_angle = features[3] # Head head = plt.Circle((0, 0), 1, color=&#39;black&#39;, fill=False) ax.add_artist(head) # Eyes ax.plot([-0.5, 0.5], [0.5, 0.5], &#39;ko&#39;, markersize=eye_size * 50) # Mouth x = np.linspace(-0.5, 0.5, 100) y = mouth_curve * (x**2 - 0.25) ax.plot(x, y - 0.5, &#39;k&#39;) # Eyebrows ax.plot([-0.6, -0.4], [0.7, 0.7 + 0.2 * (eyebrow_angle - 0.5)], &#39;k&#39;) ax.plot([0.4, 0.6], [0.7 + 0.2 * (eyebrow_angle - 0.5), 0.7], &#39;k&#39;) ax.set_xlim(-1.2, 1.2) ax.set_ylim(-1.2, 1.2) ax.axis(&#39;off&#39;) fig, axs = plt.subplots(6, 6, figsize=(12, 10)) axs = axs.flatten() for i, ax in enumerate(axs): draw_face(ax, df_norm.iloc[i].values) ax.set_title(str(ind[i]), fontsize=8) plt.tight_layout() plt.subplots_adjust(top=0.93) plt.show() plt.close() 3.2 Distance in Multivariate Analysis In multivariate analysis, most methods are based on the simple concept of distance. Take clustering analysis for example, we need to group observations that are similar or close to one another. Therefore, we need to calculate the distance between the observations. 3.2.1 Distances for Quantitative Variables In univariate cases, the distance between two observations \\(x_1\\) and \\(x_2\\) is defined as \\(d(x_1,x_2)=|x_1-x_2|=\\sqrt{(x_1-x_2)^2}\\). This definition can be extended to the multivariate cases. Suppose \\(\\mathbf{x}=[x_1, x_2, \\cdots, x_n]^{T}\\) and \\(\\mathbf{y}=[y_1, y_2, \\cdots, y_n]^{T}\\) are two vectors, their Euclidean distance is defined as \\[ d(\\mathbf{x}, \\mathbf{y})=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\\cdots+(x_n-y_n)^2}=\\sqrt{(\\mathbf{x}-\\mathbf{y})^{T}(\\mathbf{x}-\\mathbf{y})}. \\] And the Manhattan distance is defined as \\[ d(\\mathbf{x}, \\mathbf{y})=|x_1-y_1|+|x_2-y_2|+\\cdots+|x_n-y_n|=\\sum_{i=1}^n |x_i-y_i|. \\] The Minkowski distance is defined as \\[ d(\\mathbf{x}, \\mathbf{y})=\\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}, \\] which includes the Manhattan distance (when \\(p=1\\)) and the Euclidean distance (when \\(p=2\\)) as special cases. Euclidean distance treats each coordinate equally without accounting for the amount of variability in each dimension. A measure that does take into account the variance and covariance of the variables is the Mahalanobis distance \\[ d(\\mathbf{x}, \\mathbf{y})=\\sqrt{(\\mathbf{x}-\\mathbf{y})^{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{y})} \\] where \\(\\mathbf{\\Sigma}\\) is the variance-covariance matrix which can be replaced by the sample variance-covariance matrix if it is unknown. One can define his own way to calculate distance as long as the function \\(d(.)\\) satisfies the following properties: Non-negative. For any \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), \\(d(\\mathbf{x}, \\mathbf{y})\\ge 0\\). Identified. \\(d(\\mathbf{x}, \\mathbf{x})= 0\\). For any \\(\\mathbf{x}\\ne \\mathbf{y}\\), \\(d(\\mathbf{x}, \\mathbf{y})&gt; 0\\); if \\(\\mathbf{x}=\\mathbf{y}\\), \\(d(\\mathbf{x}, \\mathbf{y})=0\\) Symmetric. \\(d(\\mathbf{x}, \\mathbf{y})=d(\\mathbf{y}, \\mathbf{x})\\). Definite. If \\(d(\\mathbf{x}, \\mathbf{y})=0\\), then \\(\\mathbf{x}=\\mathbf{y}\\). Triangle inequality. For any \\(\\mathbf{x}\\ne \\mathbf{y}, \\mathbf{z}\\), \\(d(\\mathbf{x}, \\mathbf{y})\\le d(\\mathbf{x}, \\mathbf{z})+d(\\mathbf{z}, \\mathbf{y})\\). Note: If a ‘’unit change’’ means dramatically different things for different variables, we shall standardize the measurements by subtracting its mean and dividing its standard deviation before we calculate the distance. Example: Verify that the Euclidean distance is a valid distance merit. 3.2.2 Distance for Categorical Variables For categorical variables whose values are categories, it is meaningless to calculate the distance using the functions given in the previous section. For example, there are four possible blood types: A, B, O, AB. Even though we can recode the values as 1=A, 2=B, 3=O and 4=AB, we would not say that the distance between types A and B is closer than the distance between types A and O. For categorical measurements, we can use the Hamming distance \\[ d(\\mathbf{x}, \\mathbf{y})=\\sum_{i=1}^p \\mbox{I}(x_i\\ne y_i), \\] where I(.) is an indicator function which takes the value 1 if the statement is true otherwise 0. Hamming distance between two observations counts the number of not-matched measurements. For example, \\[ \\begin{array}{c|cc} \\hline &amp;\\text{Gender}&amp;\\text{Employment Status}\\\\ \\hline Kate&amp;F&amp;\\text{employed}\\\\ John&amp;M&amp;\\text{unemployed}\\\\ Adam&amp;M&amp;\\text{employed}\\\\ \\hline \\end{array} \\] The Hamming distance between Kate and John is 2 and between Kate and Adam is 1. One can also standardize the Hamming distance by dividing the number of categorical variables. That is \\[ d(\\mathbf{x}, \\mathbf{y})=\\frac{\\sum_{i=1}^p \\mbox{I}(x_i\\ne y_i)}{p}. \\] One special categorical variable is the binary variable which takes only two possible values: 0 (a certain attribute absent) or 1(a certain attribute present). A binary variable is called asymmetric if one of the two states (e.g. state “0”) is interpreted as more informative than the other state. For example, Married (1) or Not Married (0); not married can be single, divorced or widowed. If both observations have value ‘’0’’, we can not say if they are the same or different. Therefore, for asymmetric binary variable, the distance can be calculated as \\[ d(\\mathbf{x}, \\mathbf{y})=\\frac{\\sum_{i=1}^p \\mbox{I}(x_i\\ne y_i)}{p-\\mbox{# of 0-0 pairs} } \\] this is also called the Jaccard coefficient. 3.2.3 Distance for Mixed Variable Types When the multivariate measurements are mixed of quantitative and categorical data, Gower’s coefficient can be calculated to measure the distance between two observations: \\[ d(\\mathbf{x_i}, \\mathbf{x_j})=\\frac{\\sum_{k=1}^p \\delta_{ijk}d_{ijk}}{\\sum_{k=1}^p \\delta_{ijk}} \\] where \\[ \\delta_{ijk}=\\left\\{ \\begin{array}{ll} 1&amp;\\mbox{if we could use the variable $k$ to compare observations $i$ and $j$},\\\\ 0&amp;\\mbox{if we could not tell whether observations $i$ and $j$ are the same or not using variable $k$}. \\end{array} \\right. \\] and \\[ d_{ijk}=\\left\\{ \\begin{array}{ll} \\frac{|x_{ik}-x_{jk}|}{\\mbox{range of variable $k$}} &amp;\\mbox{for quantitative variables},\\\\ \\mbox{I}(x_{ik}\\ne x_{jk})&amp;\\mbox{for categorical variables}. \\end{array} \\right. \\] To summarize, \\(\\delta_{ijk}=0\\) for only 0-0 pairs of asymmetric binary variables. Example: Gower’s Coefficient Find the Gower’s coefficients for the following three individuals: \\[\\begin{array}{c|ccccc} \\hline &amp;$\\text{Gender}$&amp;$\\text{Hair}$ $\\text{Color}$&amp; $\\text{Asian}$&amp;$\\text{Height}$&amp;$\\text{Weight}$\\\\ \\hline $\\text{Kate}$&amp;$\\text{F}$&amp;$\\text{Brown}$&amp;$\\text{Yes}$&amp;$\\text{60}$&amp;80\\\\ John&amp;M&amp;Grey&amp;No&amp;50&amp;60\\\\ Adam&amp;M&amp;Brown&amp;No&amp;70&amp;90\\\\ \\hline \\end{array}\\] Among the categorical variables ‘’Gender’‘,’‘Hair color’‘,’‘Asian’‘,’‘Asian’’ is asymmetric binary. We can construct a working table to find the distance between the observations. # import gower # # df = pd.DataFrame({ # &#39;Gender&#39;: [&#39;F&#39;, &#39;M&#39;, &#39;M&#39;], # &#39;HairColor&#39;: [&#39;Brown&#39;, &#39;Grey&#39;, &#39;Brown&#39;], # &#39;Asian&#39;: [1, 0, 0], # asymmetric binary # &#39;Height&#39;: [60, 50, 70], # &#39;Weight&#39;: [80, 60, 90] # }, index=[&#39;Kate&#39;, &#39;John&#39;, &#39;Adam&#39;]) # # df[&#39;Gender&#39;] = df[&#39;Gender&#39;].astype(&#39;category&#39;) # df[&#39;HairColor&#39;] = df[&#39;HairColor&#39;].astype(&#39;category&#39;) # # gower_dist = gower.gower_matrix(df) # # print(&quot;Dissimilarities:&quot;) # for i in range(len(df)): # for j in range(i + 1, len(df)): # print(f&quot;{df.index[i]} vs {df.index[j]}: {gower_dist[i][j]:.6f}&quot;) # # types = [] # for col in df.columns: # if isinstance(df[col].dtype, pd.CategoricalDtype): # types.append(&#39;N&#39;) # Nominal # elif set(df[col].unique()).issubset({0, 1}) and col.lower() == &#39;asian&#39;: # types.append(&#39;A&#39;) # Asymmetric binary # else: # types.append(&#39;I&#39;) # Interval # # print(&quot;\\nMetric : mixed&quot;) # print(f&quot;Types : {&#39; , &#39;.join(types)}&quot;) # print(f&quot;Number of objects : {len(df)}&quot;) If we were told the range of height is 30 and the range of weight is 50 instead, we can create a fake observation to tell \\(\\textsf{R}\\) the information. For the previous example, we need to create a new observation with height=80 and weight=110, so the range of height is 80-50=30 and range for weight is 110-60=50. For the computer output, just ignore the distance between everyone to the fake person. # # hair_color = df[&#39;HairColor&#39;].cat.add_categories(&#39;Black&#39;) # # fake = pd.DataFrame({ # &#39;Gender&#39;: pd.Categorical([&#39;F&#39;], categories=df[&#39;Gender&#39;].cat.categories), # &#39;HairColor&#39;: pd.Categorical([&#39;Black&#39;], categories=hair_color.categories), # &#39;Asian&#39;: [1], # &#39;Height&#39;: [80], # &#39;Weight&#39;: [110] # }, index=[&#39;Fake&#39;]) # # # # Combine with real data # df1 = pd.concat([df, fake]) # # gower_dist = gower.gower_matrix(df1) # # print(&quot;Dissimilarities:&quot;) # n = len(df) # for i in range(n): # for j in range(i + 1, n): # print(f&quot;{df1.index[i]} vs {df1.index[j]}: {gower_dist[i][j]:.6f}&quot;) # # types = [] # for col in df1.columns: # if isinstance(df1[col].dtype, pd.CategoricalDtype): # types.append(&#39;N&#39;) # Nominal # elif set(df1[col].unique()).issubset({0, 1}) and col.lower() == &#39;asian&#39;: # types.append(&#39;A&#39;) # Asymmetric binary # else: # types.append(&#39;I&#39;) # Interval # # print(&quot;\\nMetric : mixed&quot;) # print(f&quot;Types : {&#39; , &#39;.join(types)}&quot;) # print(f&quot;Number of objects : {n}&quot;) 3.3 Multivariate Normal Distribution Most of the inferential statistical methods, such as \\(t\\) tests, one-way ANOVA, are based on the normality assumption in univariate analysis. Similarly, some multivariate analysis techniques are based on the assumption that the data are from a multivariate normal distribution. The univariate density function of normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is \\[\\begin{equation} f(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, -\\infty&lt;x&lt;\\infty \\tag{3.1} \\end{equation}\\] The term \\[ \\left(\\frac{x-\\mu}{\\sigma}\\right)^2=(x-\\mu)(\\sigma^2)^{-1}(x-\\mu) \\] in the exponent of the univariate normal density function measures the squared distance between \\(x\\) and \\(\\mu\\) in standard deviation units. This can be generalized in vectors for multivariate cases as \\[ (\\mathbf{x}-\\mathbf{\\mu})^{T}\\mathbf{\\Sigma}^{-1} (\\mathbf{x}-\\mathbf{\\mu}) \\] where \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\Sigma}\\) are the mean vector and variance-covariance matrix. It can be shown that the \\(p\\)-dimensional multivariate normal density function is \\[\\begin{equation} f(\\mathbf{x})=\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}}\\exp \\left\\{-\\frac{(\\mathbf{x}-\\mathbf{\\mu})^{T}\\mathbf{\\Sigma}^{-1} (\\mathbf{x}-\\mathbf{\\mu})}{2}\\right\\}. \\tag{3.2} \\end{equation}\\] As the constant \\(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\) is the normalizing constant such that the area under the density curve given in Equation (3.1) is 1, the constant \\(\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}}\\) is the normalizing constant ensuring that the volume under the surface defined in Equation (3.2) is 1. 3.3.1 Properties of Multivariate Normal Distribution Suppose that the joint distribution of random variables \\(X_1, X_2, \\cdots, X_p\\) is a multivariate normal with mean vector \\(\\boldsymbol{\\mu}\\) and variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\), then we have the following results. The marginal distribution of \\(X_i\\) is \\(N(\\mu_i, \\sqrt{\\sigma_{ii}})\\). For any pair of \\(X_i, X_j\\), they are independent if and only if \\(Cov(X_i, X_j)=0\\). Any linear combination of \\(X_i\\), \\(\\mathbf{c}^{T}\\mathbf{X}=c_1X_1+c_2X_2+\\cdots+c_pX_p\\) is distributed as \\(N(\\mathbf{c}^{T}\\boldsymbol{\\mu}, \\boldsymbol{c}^{T}\\boldsymbol{\\Sigma}\\mathbf{c})\\). All points with the same distance to the mean \\(\\boldsymbol{\\mu}\\) form a contour of the multivariate normal distribution. Contours for the \\(p\\)-dimensional normal distribution are ellipsoids defined by \\(\\mathbf{x}\\) such that \\[ (\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\mathbf{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})=c^2. \\] These ellipsoids are centered at \\(\\boldsymbol{\\mu}\\) and have axes \\(\\pm c\\sqrt{\\lambda_i}\\mathbf{e}_i\\), where \\(\\lambda_i\\) and \\(\\mathbf{e}_i\\) are the eigenvalues and corresponding unit eigenvectors of the variance-covariance matrix \\(\\mathbf{\\Sigma}\\). If \\(|\\boldsymbol{\\Sigma}|&gt;0\\), that is covariance matrix \\(\\boldsymbol{\\Sigma}\\) is positive definite, then \\((\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\mathbf{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\) is distributed as \\(\\chi^2_p\\), a chi-square with degrees of freedom \\(p\\). The multivariate normal distribution \\(N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) assigns probability \\((1-\\alpha)\\) to the solid ellipsoid \\(\\{(\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\mathbf{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\le \\chi^2_p(\\alpha)\\}\\), where \\(\\chi^2_p(\\alpha)\\) is the upper (\\(100\\alpha\\))th percentile of the \\(\\chi^2_p\\) distribution. 3.3.2 Bivariate Normal Distribution It can be shown that when \\(p=2\\), we have the bivariate normal density \\[\\begin{equation} \\tiny f(x_1, x_2)=\\frac{1}{2\\pi\\sqrt{\\sigma_{11}\\sigma_{22}(1-\\rho_{12}^2)}}\\exp\\left\\{-\\frac{1}{2(1-\\rho_{12}^2)}\\left[\\left(\\frac{x_1-\\mu_1}{\\sqrt{\\sigma_{11}}}\\right)^2+\\left(\\frac{x_2-\\mu_2}{\\sqrt{\\sigma_{22}}}\\right)^2-2\\rho_{12}\\left(\\frac{x_1-\\mu_1}{\\sqrt{\\sigma_{11}}}\\right)\\left(\\frac{x_2-\\mu_2}{\\sqrt{\\sigma_{22}}}\\right)\\right]\\right\\} \\tag{3.3} \\end{equation}\\] where \\(\\mu_1=E(X_1), \\mu_2=E(X_2), \\sigma_{11}=Var(X_1), \\sigma_{22}=Var(X_2), \\rho_{12}=\\frac{\\sigma_{12}}{\\sqrt{\\sigma_{11}\\sigma_{22}}}.\\) Example: Bivariate Normal Distribution If \\(X_1\\sim N(\\mu_1, \\sigma_1), X_2\\sim N(\\mu_2, \\sigma_2)\\), and they are independent. Find the joint density of \\(X_1\\) and \\(X_2\\). Find the joint density of \\(X_1\\) and \\(X_2\\) using the matrix form. How about if \\(X_1\\) and \\(X_2\\) and not independent and with correlation \\(\\rho\\)? Example: Surface and Contour Plot of Bivariate Normal Consider the bivariate normal distributions with the following mean vectors and covariance matrices: \\[ \\boldsymbol{\\mu_1}=\\boldsymbol{\\mu_2}=\\boldsymbol{\\mu_3}=\\left[ \\begin{array}{c} 0\\\\ 0 \\end{array} \\right] ,\\quad \\boldsymbol{\\Sigma_1}=\\left[ \\begin{array}{cc} 1&amp;0\\\\ 0&amp;1 \\end{array} \\right], \\quad \\boldsymbol{\\Sigma_2}=\\left[ \\begin{array}{cc} 1&amp;0.75\\\\ 0.75&amp;1 \\end{array} \\right], \\quad \\boldsymbol{\\Sigma_3}=\\left[ \\begin{array}{cc} 1&amp;-0.75\\\\ -0.75&amp;1 \\end{array} \\right]. \\] The distributions (surface plots) of the three bivariate normal are shown in the first row of Figure ?? and their corresponding contour plots are shown in the second row. The values on the contour paths are the value of density \\(f(x_1, x_2)\\) given in Equation (3.3). ## &lt;matplotlib.image.AxesImage object at 0x2896c56d0&gt; ## (np.float64(-0.5), np.float64(790.5), np.float64(1023.5), np.float64(-0.5)) ## &lt;matplotlib.image.AxesImage object at 0x2896c5810&gt; ## (np.float64(-0.5), np.float64(790.5), np.float64(1023.5), np.float64(-0.5)) ## &lt;matplotlib.image.AxesImage object at 0x2896c5950&gt; ## (np.float64(-0.5), np.float64(790.5), np.float64(1023.5), np.float64(-0.5)) ## &lt;matplotlib.image.AxesImage object at 0x292b43110&gt; ## (np.float64(-0.5), np.float64(790.5), np.float64(1023.5), np.float64(-0.5)) ## &lt;matplotlib.image.AxesImage object at 0x292b43250&gt; ## (np.float64(-0.5), np.float64(790.5), np.float64(1023.5), np.float64(-0.5)) ## &lt;matplotlib.image.AxesImage object at 0x292b43390&gt; ## (np.float64(-0.5), np.float64(790.5), np.float64(1023.5), np.float64(-0.5)) 3.3.3 Contour of Multivariate Normal Distribution Definition Contour at level \\(c_0^2\\) is the collection of all points of \\(\\mathbf{x}\\) such that \\(f(\\mathbf{x})=c_0^2\\). Since \\[ f(\\mathbf{x})=\\frac{1}{(2\\pi)^{p/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\exp \\left\\{-\\frac{(\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})}{2}\\right\\}=c_0^2 \\Longrightarrow (\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})=c^2 \\] a \\((1-\\alpha)\\times 100\\%\\) contour is the collection of points \\(\\mathbf{x}\\) such that \\[ P((\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\le c^2)=1-\\alpha. \\] Since \\((\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\sim \\chi^2_{p}\\), we get \\(c^2=\\chi^2_{p, \\alpha}\\). For example, a 95% contour for a bivariate normal distribution is the collection of all points \\(\\mathbf{x}\\) such that \\[ (\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\le \\chi^2_{2, 0.05}=5.991 \\] Example: Contour of Multivariate Normal Distribution Suppose \\[\\boldsymbol{\\mu}=\\left[ \\begin{array}{c} 0\\\\ 0 \\end{array} \\right] ,\\quad \\boldsymbol{\\Sigma}=\\left[ \\begin{array}{cc} 1&amp;0\\\\ 0&amp;1 \\end{array} \\right]\\], find a 95% contour for the bivariate normal distribution. For bivariate normal distribution, a 95% contour is \\[ (\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})=5.991 \\] \\[ \\Longrightarrow \\left[ \\begin{array}{cc} x_1, x_2 \\end{array} \\right] \\left[ \\begin{array}{cc} 1&amp;0\\\\ 0&amp;1 \\end{array} \\right] \\left[ \\begin{array}{c} x_1\\\\ x_2 \\end{array} \\right]=x_1^2+x_2^2=5.991 \\] which is a circle with radius \\(r=\\sqrt{5.991}\\). Suppose \\(\\boldsymbol{\\mu}=\\left[ \\begin{array}{c} 0\\\\ 0 \\end{array} \\right] ,\\quad \\boldsymbol{\\Sigma}=\\left[ \\begin{array}{cc} 1&amp;0\\\\ 0&amp;4 \\end{array} \\right]\\), find a 95% contour for the bivariate normal distribution. For bivariate normal distribution, a 95% contour is \\[ (\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})=5.991 \\Longrightarrow \\hspace{10cm} \\] Suppose \\[\\boldsymbol{\\mu}=\\left[ \\begin{array}{c} 0\\\\ 0 \\end{array} \\right] ,\\quad \\boldsymbol{\\Sigma}=\\left[ \\begin{array}{cc} 1&amp;\\frac{3}{4}\\\\ \\frac{3}{4}&amp;1 \\end{array} \\right]\\], find a 95% contour for the bivariate normal distribution. For bivariate normal distribution, a 95% contour is \\[ (\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})=5.991 \\Longrightarrow \\] \\[ \\left[ \\begin{array}{cc} x_1, x_2 \\end{array} \\right] \\frac{\\left[ \\begin{array}{cc} 1&amp;-\\frac{3}{4}\\\\ -\\frac{3}{4}&amp;1 \\end{array} \\right]}{1-\\frac{9}{16}} \\left[ \\begin{array}{c} x_1\\\\ x_2 \\end{array} \\right] \\] \\[=\\frac{16}{7\\times 5.991}x_1^2-\\frac{24}{7\\times 5.991}x_1x_2+\\frac{16}{7\\times 5.991}x_2^2=1 \\] which is an ellipse center \\((0, 0)\\) with axes \\(\\pm \\frac{\\sqrt{5.991\\times 7}}{2}\\) and \\(\\pm \\frac{\\sqrt{5.991}}{2}\\) and rotating counterclockwise 45 degrees. Side-note: The generate form of a rotated counterclockwise about the origin through an angle \\(\\theta\\) with axes \\(\\pm a\\) and \\(\\pm b\\) is \\[ x^2\\left(\\frac{\\cos^2\\theta}{a^2}+\\frac{\\sin^2\\theta}{b^2}\\right)+2xy\\left(\\frac{\\cos \\theta \\sin \\theta}{a^2}-\\frac{\\cos \\theta \\sin \\theta}{b^2}\\right)+y^2\\left(\\frac{\\sin^2 \\theta}{a^2}+\\frac{\\cos^2 \\theta}{b^2}\\right)=1 \\] To find the axes \\(a^2\\), \\(b^2\\) and the angle \\(\\theta\\), solve for equations \\[ \\left\\{ \\begin{array}{l} \\frac{\\cos^2\\theta}{a^2}+\\frac{\\sin^2\\theta}{b^2}=\\frac{16}{7\\times 5.991} \\\\ \\frac{\\sin^2 \\theta}{a^2}+\\frac{\\cos^2 \\theta}{b^2}=\\frac{16}{7\\times 5.991} \\\\ \\frac{\\cos \\theta \\sin \\theta}{a^2}-\\frac{\\cos \\theta \\sin \\theta}{b^2}=-\\frac{12}{7\\times 5.991} \\end{array} \\right. \\] The solutions are \\(\\theta=45, a^2=\\frac{5.991\\times 7}{4}, b^2=\\frac{5.991}{4}\\) or \\(\\theta=135, a^2=\\frac{5.991}{4}, b^2=\\frac{5.991\\times 7}{4}\\). They are the same ellipse. Theorem The contour \\((\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\le c^2\\) is an ellipsoids centered at \\(\\boldsymbol{\\mu}\\) and have axes \\(\\pm c\\sqrt{\\lambda_i}\\mathbf{e}_i\\), where \\(\\lambda_i\\) and \\(\\mathbf{e}_i\\) are the eigenvalues and corresponding unit eigenvectors of the variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\). We revisit the previous examples to apply the theorem. Example: Find contour for the bivariate normal distribution by eigen-pairs of \\(\\boldsymbol{\\Sigma}\\) Suppose \\[\\boldsymbol{\\mu}=\\left[ \\begin{array}{c} 0\\\\ 0 \\end{array} \\right] ,\\quad \\boldsymbol{\\Sigma}=\\left[ \\begin{array}{cc} 1&amp;0\\\\ 0&amp;4 \\end{array} \\right]\\], find a 95% contour for the bivariate normal distribution. The eigenvalues of \\(\\boldsymbol{\\Sigma}\\) are \\[ |\\mathbf{A}-\\lambda \\mathbf{I}|=\\left| \\begin{array}{cc} 1-\\lambda&amp; 0\\\\ 0&amp; 4-\\lambda \\end{array} \\right|=(1-\\lambda)(4-\\lambda)=0 \\] which gives the eigenvalues are \\(\\lambda_1=4\\) and \\(\\lambda_2=1\\). For \\(\\lambda_1=4\\), we have \\[ \\left[ \\begin{array}{cc} 1&amp; 0\\\\ 0&amp; 4 \\end{array} \\right] \\left[ \\begin{array}{c} x_1\\\\x_2 \\end{array} \\right]=4\\left[ \\begin{array}{c} x_1\\\\x_2 \\end{array} \\right] \\] which gives \\(x_1=4x_1, 4x_2=4x_2\\); therefore the eigenvector could be \\[\\mathbf{e_1}=\\left[ \\begin{array}{c} 0\\\\1 \\end{array} \\right] \\] For \\(\\lambda_2=1\\), the eigenvector could be \\[ \\mathbf{e_2}=\\left[ \\begin{array}{c} 1\\\\0 \\end{array} \\right] \\] The axes are \\[\\pm c\\sqrt{\\lambda_1}\\mathbf{e}_1=\\pm \\sqrt{5.991} \\sqrt{4} \\left[ \\begin{array}{c} 0\\\\1 \\end{array} \\right]=\\pm\\left[ \\begin{array}{c} 0\\\\4.895 \\end{array} \\right] \\] and \\[\\pm c\\sqrt{\\lambda_2}\\mathbf{e}_2=\\pm \\sqrt{5.991} \\sqrt{1} \\left[ \\begin{array}{c} 1\\\\0 \\end{array} \\right]=\\pm\\left[ \\begin{array}{c} 2.448\\\\0 \\end{array} \\right] \\] Suppose \\[\\boldsymbol{\\mu}=\\left[ \\begin{array}{c} 0\\\\ 0 \\end{array} \\right] ,\\quad \\boldsymbol{\\Sigma}=\\left[ \\begin{array}{cc} 1&amp;\\frac{3}{4}\\\\ \\frac{3}{4}&amp;1 \\end{array} \\right]\\], find a 95% contour for the bivariate normal distribution. 3.4 The Sampling Distribution of \\(\\mathbf{\\bar X}\\) and \\(\\boldsymbol{S}\\) In the univariate case, the sample mean \\(\\bar X\\) is an unbiased estimator of the population mean \\(\\mu\\). Inferences on \\(\\mu\\) is based on the sampling distribution of \\(\\bar X\\). It is known that \\(E(\\bar X)=\\mu, Var(\\bar X)=\\frac{\\sigma^2}{n}\\), where \\(\\sigma^2\\) is the population variance. If population distribution is normal, \\(\\bar X\\) is also normally distributed; if the population distribution is non-normal, by central limit theorem, when the sample size is large enough, \\(\\bar X\\) is approximately normally distributed. As for the sample variance \\(s^2\\), we have \\(\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\). And \\(\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{\\sigma^2}\\sim \\chi^2_n\\). Before generalizing to multivariate cases, let’s review distributions related to normal for univariate variables. 3.4.1 Distributions Related to Normal Distribution Most of the following conclusions can be shown by using the moment-generating function. If \\(X\\sim N(\\mu, \\sigma)\\), then \\(Z=\\frac{X-\\mu}{\\sigma}\\sim N(0, 1)\\). If \\(Z\\sim N(0, 1)\\), then \\(Z^2\\sim \\chi^2_1\\), a chi-square distribution with degrees of freedom \\(df=1\\). If \\(W\\sim \\chi^2_p, V\\sim \\chi^2_q\\), and they are independent, then \\[ W+V\\sim \\chi^2_{p+q} \\mbox{ and } \\quad \\frac{W/p}{V/q}\\sim F_{p, q} \\mbox{ (an F distribution with $df_n=p, df_d=q$)}. \\] If \\(Z\\sim N(0, 1), W\\sim \\chi^2_{\\gamma}\\), and they are independent, then \\[ \\frac{Z}{\\sqrt{\\frac{W}{\\gamma}}}\\sim t_{\\gamma-1} \\quad \\mbox{(a $t$ distribution with degrees of freedom $df=\\gamma$)} \\] 3.4.2 Applications to Distributions Related to Sample Means Suppose that \\(X_1, X_2, \\cdots, X_n\\) are iid from a normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the sample mean is defined as \\(\\bar X=\\frac{\\sum X_i}{n}\\). Then we have \\(\\bar X\\sim N(\\mu, \\frac{\\sigma^2}{n})\\). \\(\\sum_{i=1}^n \\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\) follows a chi-square distribution with \\(df=n\\), denoted as \\(\\chi^2_{n}\\). \\(\\sum_{i=1}^n \\left(\\frac{X_i-\\bar X}{\\sigma}\\right)^2=\\frac{(n-1)S^2}{\\sigma^2}\\) follows a chi-square distribution with \\(df=n-1\\), denoted as \\(\\chi^2_{n-1}\\). It can be shown that \\(S^2\\) and \\(\\bar Y\\) are independent. \\(T=\\frac{\\bar X-\\mu}{S/\\sqrt{n}}=\\frac{\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{[\\frac{(n-1)S^2}{\\sigma^2}]/(n-1)}}\\sim t_{n-1}\\). Note that \\(\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0, 1)\\) and \\(\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\), therefore \\[ T^2=\\left(\\frac{\\bar X-\\mu}{S/\\sqrt{n}}\\right)^2=\\frac{\\frac{n(\\bar X-\\mu)^2}{\\sigma^2}/1}{\\frac{(n-1)S^2}{\\sigma^2}/(n-1)} \\sim F_{1, n-1} \\] 3.4.3 Generalize to Multivariate Cases Review the limit theorems covered in STAT 266. They are useful to prove convergency. If \\(X_n\\stackrel{P}{\\longrightarrow} a\\) and \\(Y_n\\stackrel{P}{\\longrightarrow} b\\), then \\(X_n\\pm Y_n\\stackrel{P}{\\longrightarrow} a\\pm b\\). If \\(X_n\\stackrel{P}{\\longrightarrow} a\\) and \\(Y_n\\stackrel{P}{\\longrightarrow} b\\), then \\(X_n Y_n\\stackrel{P}{\\longrightarrow} a b\\). If \\(X_n\\stackrel{P}{\\longrightarrow} X\\) and \\(a\\) is a constant, then \\(aX_n\\stackrel{P}{\\longrightarrow} aX\\). If \\(X_n\\stackrel{P}{\\longrightarrow} a\\) where \\(a\\) is a fixed constant and \\(g\\) is a real-valued function which is continuous at \\(a\\), then \\(g(X_n)\\stackrel{P}{\\longrightarrow} g(a)\\). If \\(X_n\\stackrel{D}{\\longrightarrow}a\\) where \\(a\\) is a fixed constant and \\(g\\) is a real-valued function which is continuous at \\(a\\), then \\(g(X_n)\\stackrel{D}{\\longrightarrow} g(a)\\). Slutsky theorem. If \\(X_n\\stackrel{P}{\\longrightarrow} a\\) and \\(Y_n\\stackrel{D}{\\longrightarrow} Y\\) then \\(X_n+Y_n\\stackrel{D}{\\longrightarrow} a+Y\\) \\(X_nY_n\\stackrel{D}{\\longrightarrow}aY\\) \\(X_n^{-1}Y_n\\stackrel{D}{\\longrightarrow} a^{-1}Y\\) Here we generalize some of limiting distributions from univariate case to multivariate case: \\[ \\small \\begin{array}{c|c} \\hline \\text{Univariate Case} &amp; \\text{Multivariate Case} \\\\ \\hline \\sqrt{n}(\\bar X-\\mu) \\stackrel{D}{\\longrightarrow} N(0, \\sigma^2) &amp; \\sqrt{n}(\\mathbf{\\bar X}-\\boldsymbol{\\mu}) \\stackrel{D}{\\longrightarrow} N(\\mathbf{0}, \\boldsymbol{\\Sigma}) \\\\ n(\\bar X-\\mu)(\\sigma^2)^{-1}(\\bar X-\\mu) \\stackrel{D}{\\longrightarrow} \\chi^2_1 &amp; n(\\mathbf{\\bar X}-\\boldsymbol{\\mu})^{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{\\bar X}-\\boldsymbol{\\mu}) \\stackrel{D}{\\longrightarrow} \\chi^2_p \\\\ n(\\bar X-\\mu)(S^2)^{-1}(\\bar X-\\mu) \\stackrel{D}{\\longrightarrow} \\chi^2_1 \\: \\text{when $n$ is large enough} &amp; n(\\mathbf{\\bar X}-\\boldsymbol{\\mu})^{T}\\mathbf{S}^{-1}(\\mathbf{\\bar X}-\\boldsymbol{\\mu}) \\stackrel{D}{\\longrightarrow} \\chi^2_p \\: \\text{when $(n-p)$ is large.} \\end{array} \\] 3.5 Review Exercises We consider the modified admission data set which contain 400 graduate school admissions decisions. There are five variables: admit/don’t admit to graduate school (1=admitted and 0=not admitted), GRE (Graduate Record Exam scores), GPA (grade point average), prestige of the undergraduate institution (values from 1 to 4 with 1 having the highest prestige), whether the undergraduate institution is in Ontario or not. Use the most proper distance metric to find the distance between the following three individuals: \\[ \\begin{array}{ccccc} \\hline Admit&amp; GRE&amp; GPA&amp; Prestige&amp; Ontario\\\\ \\hline 0 &amp;380&amp; 3.61 &amp; 3&amp; Yes\\\\ 1&amp; 660&amp; 3.67 &amp; 3&amp; NO\\\\ 1&amp; 800&amp; 4.00 &amp; 1&amp; NO\\\\ \\hline \\end{array} \\] Suppose the range of GRE is 580 and the range of GPA is 1.74. Consider a bivariate normal population with \\(\\mu_1=1, \\mu_2=3, \\sigma_{11}=2, \\sigma_{22}=1, \\rho_{12}=-0.8\\). Write out the bivariate normal density. Write out the squared Mahalanobis distance \\((\\mathbf{x}-\\boldsymbol{\\mu})^{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\) as a function of \\(x_1\\) and \\(x_2\\). Determine and sketch the constant -density contour that contains 90% of the probability. Revisit the Learning Outcomes After finishing this note, students should be able to Plot the index scatter plots matrix, draw a stars plot and Chernoff faces plot of a given multivariate data set using R, and interpret the plots. Verify whether a distance function is valid. Choose the proper distance metric to calculate the distance between observations based on the data types for multivariate data. Write down the density function of a multivariate normal distribution. Describe the properties of a multivariate normal distribution. Find a \\((1-\\alpha)\\times 100\\%\\) contour for a given bivariate normal distribution. Explain the distributions related to the sample mean vector and sample covariance matrix. "],["hypothesis-tests-on-mean-vectors.html", "4 Hypothesis Tests on Mean Vectors Learning Outcomes 4.1 Hypothesis Test for one Mean Vector 4.2 Hypothesis Test for Two Mean Vectors 4.3 Hypothesis Test for Several Mean Vectors Revisit the Learning Outcomes", " 4 Hypothesis Tests on Mean Vectors In the univariate case, a one-sample \\(t\\) test is used for the hypothesis test for one population mean, the test statistic is \\(t=\\frac{\\bar x-\\mu_0}{s/\\sqrt{n}}\\). We apply the one-sample \\(t\\) test on the paired differences when comparing two population means with paired samples. When comparing two population means with two independent samples, we use a non-pooled two-sample \\(t\\) test. The test statistic is \\[ t=\\frac{(\\bar x_1-\\bar x_2)-\\Delta_0}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}} \\] If the two population variances \\(\\sigma_1^2, \\sigma_2^2\\) are the same, we can combine the two samples and the common variance can be estimated as \\[ s_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2} \\] and the test statistic becomes \\[ t=\\frac{(\\bar x_1-\\bar x_2)-\\Delta_0}{s_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}} \\] When comparing more than two population means, we use one-way ANOVA. This note covers how to conduct hypothesis tests on the mean vector for multivariate data. Learning Outcomes After finishing this note, students should be able to Explain the main idea of using a chi-square Q-Q plot to assess the multivariate normality assumption. Assess the multivariate normality through a chi-square Q-Q plot and casual procedures using R. Conduct a Hotelling’s \\(T^2\\) test on one mean vector based on one sample or paired sample. Conduct a Hotelling’s \\(T^2\\) test on two mean vectors based on two independent samples. Conduct a one-way MANOVA test on at least two mean vectors based on at least two independent samples. Obtain \\((1-\\alpha)\\times 100\\%\\) Bonferroni confidence intervals associated with a certain test if applicable. 4.1 Hypothesis Test for one Mean Vector Let’s first review hypothesis tests for one population mean in the univarite case. 4.1.1 Univariate Case For one population mean in the univariate case, the hypotheses for a two-tailed test are: \\[ H_0: \\mu=\\mu_0 \\mbox{ versus } H_a: \\mu\\ne \\mu_0. \\] By the critical value approach, we reject \\(H_0: \\mu=\\mu_0\\) at the significance level \\(\\alpha\\) if the observed test statistic \\[ t_o=\\frac{\\bar x-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\ge t_{n-1} (\\frac{\\alpha}{2}) \\mbox{ or } t_o\\le -t_{n-1} (\\frac{\\alpha}{2}) \\] where \\(t_{n-1} (\\frac{\\alpha}{2})\\) is the \\(t\\)-score with an area \\(\\frac{\\alpha}{2}\\) to its right under a \\(t\\) density curve with \\(df=n-1\\). It is equivalent to reject \\(H_0\\) if \\[ |t_o|=\\frac{|\\bar x-\\mu_0|}{s/\\sqrt{n}}\\ge t_{n-1} (\\frac{\\alpha}{2}) \\mbox{ or if } t_o^2=\\frac{(\\bar x-\\mu_0)^2}{s^2/n} \\ge F_{1, n-1}(\\alpha). \\] Note that \\[ T^2=\\frac{(\\bar X-\\mu)^2}{s^2/n}=\\frac{\\left(\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}}\\right)^2}{s^2/\\sigma^2}\\sim F_{1, n-1} \\] because \\(\\left(\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}}\\right)^2\\sim \\chi^2_1\\) and \\(\\frac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\). The term \\(T^2=\\frac{(\\bar X-\\mu)^2}{s^2/n}\\) can be rewritten as \\[ T^2=n(\\bar X-\\mu)(s^2)^{-1}(\\bar X-\\mu). \\] 4.1.2 Multivariate Case The one-sample t test can be generalized to the multivariate case for one population mean vector. One-sample Hotelling’s \\(T^2\\) Test For multivariate cases, each observation has multiple measurements in a vector form. The hypotheses for a two-tailed test are: \\[ H_0: \\boldsymbol{\\mu}=\\boldsymbol{\\mu}_0 \\mbox{ versus } H_a: \\boldsymbol{\\mu}\\ne \\boldsymbol{\\mu}_0. \\] Reject \\(H_0: \\boldsymbol{\\mu}=\\boldsymbol{\\mu}_0\\) if the test statistic \\(T^2=n(\\mathbf{\\bar X}-\\boldsymbol{\\mu}_0)^T\\mathbf{S}^{-1}(\\mathbf{\\bar X}-\\boldsymbol{\\mu}_0)\\) is too large, where \\(\\mathbf{\\bar X}_{p \\times 1}\\) is the sample mean vector and \\(\\mathbf{S}_{p\\times p}\\) is the sample covariance matrix respectively based on the observation matrix \\(\\mathbf{X}_{n \\times p}\\). The test statistic \\(T^2\\) follows a Hotelling’s \\(T^2\\) distribution with degrees of freedom \\(p\\) and \\(n-1\\). It can be shown that \\[ \\frac{n-p}{p(n-1)}T^2_{p, n-1}\\sim F_{p, n-p}. \\] Therefore, we reject \\(H_0: \\boldsymbol{\\mu}=\\boldsymbol{\\mu}_0\\) at significance level \\(\\alpha\\) if \\[ \\frac{n(n-p)}{p(n-1)}(\\mathbf{\\bar x}-\\boldsymbol{\\mu}_0)^T\\mathbf{S}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu}_0)\\ge F_{p, n-p}(\\alpha) \\] where \\(F_{p, n-p}(\\alpha)\\) is the upper \\(100\\alpha\\)th percentile of the \\(F_{p, n-p}\\) distribution. A \\((1-\\alpha)\\times 100 \\%\\) confidence region for the mean vector \\(\\boldsymbol{\\mu}\\) is the ellipsoid such that \\[ (\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T\\mathbf{S}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})\\le c^2=\\frac{p(n-1)}{n(n-p)} F_{p, n-p}(\\alpha). \\] The ellipsoid is centered at \\(\\mathbf{\\bar x}\\) and has axes \\(\\pm \\sqrt{\\frac{p(n-1)}{n(n-p)} F_{p, n-p}(\\alpha)}\\sqrt{\\lambda_i}\\mathbf{e}_i\\), where \\(\\lambda_i\\) and \\(\\mathbf{e}_i\\) are the eigenvalues and corresponding unit eigenvectors of the sample covariance matrix \\(\\mathbf{S}\\). We should reject \\(H_0: \\boldsymbol{\\mu}=\\boldsymbol{\\mu}_0\\) if \\(\\boldsymbol{\\mu}_0\\) is outside the confidence region, i.e., if the distance \\[ (\\mathbf{\\bar x}-\\boldsymbol{\\mu}_0)^T\\mathbf{S}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu}_0) &gt;\\frac{p(n-1)}{n(n-p)} F_{p, n-p}(\\alpha). \\] \\(\\textbf{Side note}\\): The derivation of the distribution of \\(T^2\\) is based on the Wishart distribution and its properties, which is outside the scope of this course. Some more details are provided here and are not required for exams. If \\(\\mathbf{y}_1, \\mathbf{y}_2, \\cdots, \\mathbf{y}_n\\) are identically independently follow a multivariate normal distribution \\(N_p(\\mathbf{0}, \\boldsymbol{\\Sigma})\\), then \\(\\mathbf{A}=\\sum_{i=1}^n \\mathbf{y}_i \\mathbf{y}_i^T\\sim W_p(n, \\boldsymbol{\\Sigma})\\), a \\(p\\) dimensional Wishart distribution with parameters \\(n\\) and \\(\\boldsymbol{\\Sigma}\\). If \\(\\mathbf{A} \\sim W_p(n, \\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\) is a \\(p\\times 1\\) random vector independent of \\(\\mathbf{A}\\), then \\[ \\frac{\\mathbf{y}^T\\boldsymbol{\\Sigma}^{-1} \\mathbf{y}}{\\mathbf{y}^T\\mathbf{A}^{-1} \\mathbf{y}}\\sim \\chi_{n-(p-1)}^2 \\mbox{ and independent of $\\mathbf{y}$.} \\] Given the definition of Wishart distribution and its property, the distribution of \\(T^2\\) can be obtained as follows: Suppose \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_n\\) are identically independently follow a multivariate normal distribution \\(N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then the sample mean vector \\(\\mathbf{\\bar x}\\) and the sample covariance matrix \\(\\mathbf{S}\\) have the following distributions \\[ \\mathbf{\\bar x}\\sim N_p\\left(\\boldsymbol{\\mu}, \\frac{\\boldsymbol{\\Sigma}}{n}\\right), \\quad \\quad (n-1)\\mathbf{S}=\\sum_{i=1}^n (\\mathbf{x}_i-\\mathbf{\\bar x})(\\mathbf{x}_i-\\mathbf{\\bar x})^T\\sim W_p(n-1, \\boldsymbol{\\Sigma}) \\] and the sample mean vector \\(\\mathbf{\\bar x}\\) and the sample covariance matrix \\(\\mathbf{S}\\) are independent. This implies random vector \\(\\mathbf{\\bar x}-\\boldsymbol{\\mu}\\sim N_p\\left(\\boldsymbol{0}, \\frac{\\boldsymbol{\\Sigma}}{n}\\right)\\) and independent of \\(\\mathbf{S}\\). Since \\[ \\begin{aligned} U&amp;=(\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T\\left(\\frac{\\boldsymbol{\\Sigma}}{n}\\right)^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})=n(\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})\\sim \\chi_p^2\\\\ V&amp;=\\frac{(\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})}{(\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T[(n-1)\\mathbf{S}]^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})}=\\frac{(n-1)(\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})}{(\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T\\mathbf{S}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})}\\sim \\chi_{(n-1)-(p-1)}^2 \\end{aligned} \\] and \\(U\\) and \\(V\\) are independent, \\[ \\frac{U/p}{V/(n-p)}\\sim F_{p, n-p}\\Longrightarrow \\left(\\frac{n-p}{p}\\right)\\left(\\frac{n}{n-1}\\right)(\\mathbf{\\bar x}-\\boldsymbol{\\mu})^T\\mathbf{S}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu})=\\frac{n-p}{p(n-1)}T^2\\sim F_{p, n-p}. \\] One-sample Hotelling’s \\(T^2\\) Confidence Interval Recall the one-way ANOVA F test, we will follow up with a posthoc test to figure out which pairs are significantly different if we reject the null hypothesis that all means are equal. Similarly, for a one-mean Hotelling’s \\(T^2\\) test, if we reject \\(H_0: \\boldsymbol{\\mu}=\\boldsymbol{\\mu}_0\\), we would like to figure out in which variables the means are different. A \\((1-\\alpha)\\times 100 \\%\\) simultaneous confidence interval for the \\(i\\)th variable \\(X_i\\) is given by \\[ \\bar x_i \\pm \\sqrt{\\frac{p(n-1)}{(n-p)} F_{p, n-p}(\\alpha)}\\sqrt{\\frac{s_i^2}{n}}, i=1, 2, \\cdots, p \\] where \\(\\bar x_i\\) and \\(s_i^2\\) are the sample mean and sample variance of \\(X_i\\). A \\((1-\\alpha)\\times 100 \\%\\) Bonferroni interval\\ The problem with the simultaneous confidence intervals is that if we are not interested in all \\(p\\) variables, the simultaneous confidence interval may be too wide, and, hence, too conservative. This is called the {} problem. We can adopt the Bonferroni adjustment to fix this problem. A \\((1-\\alpha)\\times 100 \\%\\) Bonferroni interval is given by \\[ \\bar x_i \\pm t_{n-1}(\\frac{\\alpha}{2p})\\sqrt{\\frac{s_i^2}{n}}, i=1, 2, \\cdots, p, \\] where \\(t_{n-1}(\\frac{\\alpha}{2p})\\) is the \\(\\frac{\\alpha}{2p}\\times 100\\)th upper-tailed quantile of the \\(t\\) distribution with \\(df=n-1\\).The mean vector differs in \\(X_i\\) if \\(\\mu_{0,i}\\) is outside the interval. \\(\\textbf{Example: One-sample Hotelling&#39;s } \\boldsymbol{T^2} \\textbf{ Test}\\) Given the data matrix \\[ \\mathbf{X}= \\left[ \\begin{array}{cc} 6&amp; 9\\\\ 10&amp; 6\\\\ 8&amp;3 \\end{array} \\right] \\] Find the sample mean vector and covariance matrix. Test at the 5% significance level \\[ H_0: \\boldsymbol{\\mu}=\\left[ \\begin{array}{c} 9\\\\ 5 \\end{array} \\right] \\text{versus } H_a: \\boldsymbol{\\mu}\\ne\\left[ \\begin{array}{c} 9\\\\ 5 \\end{array} \\right]. \\] import numpy as np from scipy.stats import f xmat = np.array([[6, 9], [10, 6], [8, 3]]) mu0 = np.array([9, 5]) # manually in python as there is no one package for all tests n, p = xmat.shape diff = np.mean(xmat, axis=0) - mu0 inv_cov = np.linalg.inv(np.cov(xmat, rowvar=False, ddof=1)) t_squared = n * diff @ inv_cov @ diff # formula for T squared f_stat = (n - p) / (p * (n - 1)) * t_squared # formula for F statistic p_value = 1 - f.cdf(f_stat, p, n - p) # formula for p-value print(f&#39;T² statistic: {t_squared:.5f}&#39;) ## T² statistic: 0.77778 print(f&#39;F statistic: {f_stat:.5f} (df1 = {p}, df2 = {n-p})&#39;) ## F statistic: 0.19444 (df1 = 2, df2 = 1) print(f&#39;p-value: {p_value:.4f}&#39;) ## p-value: 0.8485 4.1.3 Evaluating Multivariate Normality One of the assumptions for a one-sample \\(t\\) test in the univariate case is that we either have a normal population or a large sample. A normal probability plot is used to check the normality assumption. If the distribution of the data is roughly normal, the points will roughly fall on a straight line. Deviations from a straight line indicate that the underlying distribution is not normal. The rationale behind the normal probability plot is as follows. If the data follow a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), their \\(z\\)-scores given by \\[ z_i=\\frac{y_i-\\mu}{\\sigma}=-\\frac{\\mu}{\\sigma}+\\frac{1}{\\sigma}y_i \\] should follow a standard normal distribution. If we plot the normal scores \\(z_i\\) versus the observed values \\(y_i\\), the data are roughly on a straight line with intercept \\(-\\frac{\\mu}{\\sigma}\\) and slope \\(\\frac{1}{\\sigma}\\). Therefore, if the data are from a normal population, plotting the normal scores (theoretical quantiles) obtained from Table III in the appendix of the recommended textbook (Weiss) versus the observations (observed quantiles) gives a straight line. The normal probability plot is also called the normal Q-Q plot; “Q” stands for “quantile”. The following example shows the steps to construct a normal probability plot. Suppose the data are 75, 80, 90, 85, 75, and 40. Here is the code for a normal Q-Q plot import scipy.stats as stats import matplotlib.pyplot as plt x = np.array([75, 80, 90, 85, 75, 40]) stats.probplot(x, dist=&quot;norm&quot;, plot=plt) ## ((array([-1.23132171, -0.63003387, -0.19819716, 0.19819716, 0.63003387, ## 1.23132171]), array([40, 75, 75, 80, 85, 90])), (np.float64(17.634241917611398), np.float64(74.16666666666667), np.float64(0.8792020358303791))) plt.title(&quot;Normal QQ Plot of Grade&quot;) plt.show() plt.close() The six points do not fall in a straight line; the data do not seem to come from a normal distribution. The point on the lower left corner might be an outlier. If we remove this potential outlier, the other five points roughly fall on a straight line. ## (np.float64(-0.5), np.float64(767.5), np.float64(811.5), np.float64(-0.5)) The following section explains how the normal scores given in Table III were calculated. Sort the observations \\(x_1, x_2 \\cdots, x_n\\) from the smallest to the largest, we have \\(x_{(1)}, x_{(2)}, \\cdots, x_{(n)}\\), where \\(x_{(1)}\\) denotes the smallest observation, \\(x_{(2)}\\) the second smallest and \\(x_{(n)}\\) the largest value. Let \\[ a_i=\\frac{i-\\frac{3}{8}}{n+\\frac{1}{4}}, i=1, 2, \\ldots, n. \\] As we can see, \\(a_i\\) is roughly the percentage of values that lie below the \\(i\\)th smallest, the fractions \\(\\frac{3}{8}\\) and \\(\\frac{1}{4}\\) is for continuity correction. Using Table II to find the normal scores \\(z_1, z_2, \\cdots, z_n\\), such that \\[ P(Z\\le z_i)=\\mbox{Area}(Z\\le z_i)=a_i, i=1, 2, \\cdots, n. \\] Use the previous example with six grades, the normal scores for \\(n=6\\) are: \\[\\begin{array}{c|c|c|c} \\hline \\text{Grade} (x_{(i)}) &amp;i&amp;a_i=\\frac{i-\\frac{3}{8}}{n+\\frac{1}{4}}&amp;\\text{Normal score} (z_i)\\\\ \\hline 40&amp;1&amp;(1-3/8)/(6+1/4)=0.10&amp;-1.28\\\\ 75&amp;2&amp;(2-3/8)/(6+1/4)=0.26&amp;-0.64\\\\ 75&amp;3&amp;(3-3/8)/(6+1/4)=0.42&amp;-0.20\\\\ 80&amp;4&amp;(4-3/8)/(6+1/4)=0.58&amp;0.20\\\\ 85&amp;5&amp;(5-3/8)/(6+1/4)=0.74&amp;0.64\\\\ 90&amp;6&amp;(6-3/8)/(6+1/4)=0.90&amp;1.28\\\\ \\hline \\end{array}\\] from scipy.stats import norm # First example: n = 6 i = np.arange(1, 7) n = len(i) a = (i - 3/8) / (n + 0.25) z = norm.ppf(a) print(np.round(z, 2)) ## [-1.28 -0.64 -0.2 0.2 0.64 1.28] # Second example: n = 10 i2 = np.arange(1, 11) n2 = len(i2) a2 = (i2 - 3/8) / (n2 + 0.25) z2 = norm.ppf(a2) print(np.round(z2, 2)) ## [-1.55 -1. -0.66 -0.38 -0.12 0.12 0.38 0.66 1. 1.55] \\(\\textbf{Note: }\\) the normal probability plots in the Stat 151 textbook are generated by Minitab, which plots the normal scores versus the sorted observations. It does not matter whether we plot the observed data in the \\(x\\)-axis or in the \\(y\\)-axis. The points will fall on a straight line if the data are from a normal population. Different formulas can be used to calculate \\(a_i\\). Another more popular choice is \\(a_i=\\frac{i-0.5}{n}\\), which is used in \\(\\textsf{R.}\\) A similar idea can be applied to the multivariate case. Suppose \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_n\\) is a simple random sample of size \\(n\\) from a multivariate normal distribution \\(N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then \\((\\mathbf{x}_i-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_i-\\boldsymbol{\\mu})\\sim \\chi_p^2\\). If the observations follow a multivariate normal distribution \\(N_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), \\((\\mathbf{x}_i-\\mathbf{\\bar x})^T\\mathbf{S}^{-1}(\\mathbf{x}_i-\\mathbf{\\bar x})\\) should also roughly follows a \\(\\chi_p^2\\) distribution. Therefore, we could use the theoretical quantiles from a \\(\\chi_p^2\\) distribution to test multivariate normality. The steps are as follows: Calculate the sample mean vector \\(\\mathbf{\\bar x}\\) and the sample covariance matrix \\(\\mathbf{S}\\). Calculate the Mahalanobis distance from each observation \\(\\mathbf{x}_i\\) to the sample mean \\(\\mathbf{\\bar x}\\), denoted as \\(d_i^2\\). Sort the distances \\(d_i^2\\) from the smallest to the largest, denoted as \\(d_{(1)}^2, d_{(2)}^2, \\cdots, d_{(n)}^2\\). These are the observed quantiles. Obtain the theoretical quantiles from a \\(\\chi_p^2\\) distribution, denoted as \\(q_{(i)}=\\chi_p^2(1-\\frac{i-0.5}{n})\\), i.e., the \\(\\frac{i-0.5}{n}\\times 100\\)th percentile. Draw a scatter plot of the observed quantiles \\(d_{(i)}^2\\) versus the theoretical quantiles \\(q_{(i)}\\). If the data are from a multivariate normal distribution, the data points should roughly fall on a 45-degree straight line. Figure 4.1 is the chi-square probability plot on the 50 iris (setosa) flowers and the table to its right compares the Mahanalobis distance (observed quantiles) and the theoretical quantiles obtained from a \\(\\chi_4^2\\) distribution. The sample mean vector is \\(\\mathbf{\\bar x}=[5.006, 3.428, 1.462, 0.246]^T\\). Except for the last ten observations, most of the points are roughly on the 45-degree line. There is no strong evidence against the null hypothesis: data follow a chi-square distribution. import pandas as pd from scipy.stats import chi2 from scipy.spatial.distance import mahalanobis from sklearn.datasets import load_iris iris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) # subsetting setosa x = df.iloc[:50, :].values n, p = x.shape mean_vec = np.mean(x, axis=0) cov_mat = np.cov(x, rowvar=False) cov_inv = np.linalg.inv(cov_mat) # mahalanobis distances D2 = np.array([mahalanobis(obs, mean_vec, cov_inv)**2 for obs in x]) D2_sorted = np.sort(D2) q_vec = (np.arange(1, n+1) - 0.5) / n q_theo = chi2.ppf(q_vec, df=p) # Plot plt.figure(figsize=(7, 5)) plt.plot(q_theo, D2_sorted, &#39;ko&#39;, markersize=4) plt.plot([0, max(q_theo)], [0, max(q_theo)], &#39;r-&#39;, linewidth=2) plt.xlabel(&quot;Theoretical Quantile&quot;) plt.ylabel(&quot;Observed Quantile&quot;) plt.title(&quot;Chi-square Q-Q Plot&quot;) plt.grid(True, linestyle=&#39;--&#39;, alpha=0.3) plt.tight_layout() plt.show() Figure 4.1: Chi-squre Q-Q Plot for Setosa plt.close() We can apply the Kolmogorov-Smirnov (KS) test to check whether the Mahanalobis distances follow a chi-square distribution with \\(df=4\\). The Kolmogorov-Smirnov is a non-parametric statistical test used to determine if a sample of data follows a specified probability distribution or if two samples come from the same underlying distribution. The main idea behind the KS test is to compare the empirical cumulative distribution function (ECDF) of the sample data to the cumulative distribution function (CDF) of the target distribution. Since the p-value of the test is 0.4337, there is no strong evidence against the null hypothesis: data follow a chi-square distribution. from scipy.stats import kstest ks_stat, p_value = kstest(D2, cdf=&#39;chi2&#39;, args=(4,)) print(f&quot;KS statistic (D): {ks_stat:.5f}, p-value: {p_value:.4f}&quot;) ## KS statistic (D): 0.12001, p-value: 0.4337 The following table compares the observed and theoretical quantiles. \\[\\begin{array}{c|c|c|c|c|c} \\hline \\text{Sepal} &amp; \\text{Sepal} &amp; \\text{Petal}&amp; \\text{Petal}&amp; \\text{Observed} &amp; \\text{Theoretical}\\\\ \\text{Length}&amp;\\text{Width}&amp;\\text{Length}&amp;\\text{Width}&amp;\\text{Quantiles} d_{(i)}^2 &amp;\\text{Quantiles} q_{(i)}\\\\ \\hline 5 &amp; 3.4 &amp; 1.5 &amp; 0.2 &amp; 0.343 &amp; 0.297 \\\\ 5.1 &amp; 3.5 &amp; 1.4 &amp; 0.2 &amp; 0.449 &amp; 0.535 \\\\ 5 &amp; 3.3 &amp; 1.4 &amp; 0.2 &amp; 0.495 &amp; 0.711 \\\\ 5.1 &amp; 3.4 &amp; 1.5 &amp; 0.2 &amp; 0.589 &amp; 0.862 \\\\ 5.1 &amp; 3.5 &amp; 1.4 &amp; 0.3 &amp; 0.636 &amp; 0.999 \\\\ 5 &amp; 3.6 &amp; 1.4 &amp; 0.2 &amp; 0.762 &amp; 1.127 \\\\ 5.2 &amp; 3.5 &amp; 1.5 &amp; 0.2 &amp; 0.829 &amp; 1.249 \\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 5.2 &amp; 4.1 &amp; 1.5 &amp; 0.1 &amp; 7.699 &amp; 7.114 \\\\ 5.1 &amp; 3.8 &amp; 1.9 &amp; 0.4 &amp; 8.601 &amp; 7.539 \\\\ 4.8 &amp; 3.4 &amp; 1.9 &amp; 0.2 &amp; 9.748 &amp; 8.043 \\\\ 5.8 &amp; 4 &amp; 1.2 &amp; 0.2 &amp; 10.222 &amp; 8.666 \\\\ 4.6 &amp; 3.6 &amp; 1 &amp; 0.2 &amp; 11.044 &amp; 9.488 \\\\ 5 &amp; 3.5 &amp; 1.6 &amp; 0.6 &amp; 12.310 &amp; 10.712 \\\\ 4.5 &amp; 2.3 &amp; 1.3 &amp; 0.3 &amp; 12.328 &amp; 13.277 \\\\ \\hline \\end{array}\\] We can also employ the following diagnostic procedures to assess the multivariate normal assumption in a more casual way. Produce a normal probability plot (Q-Q plot ) for each variable. What we should look for is whether the points are roughly on a straight line. Produce scatter plots for each pair of variables. Under multivariate normality, we should see an elliptical cloud of points. Produce a three-dimensional rotating scatter plot. Again, we should see an elliptical cloud of points. Figure 4.2 shows a normal Q-Q plot for each of the four variables of the Setosa Iris data. Except for Petal Width which has only six distinct values, all other three variables roughly follow normal distributions. import seaborn as sns iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df[&#39;species&#39;] = iris.target # Filter for Setosa (species = 0) setosa_df = iris_df[iris_df[&#39;species&#39;] == 0] # Plot Q-Q plots for each variable fig, axes = plt.subplots(2, 2, figsize=(10, 8)) axes = axes.flatten() columns = setosa_df.columns[:-1] for i, col in enumerate(columns): stats.probplot(setosa_df[col], dist=&quot;norm&quot;, plot=axes[i]) axes[i].set_title(f&quot;QQ Plot for {col}&quot;) plt.subplots_adjust(hspace=0.6, wspace=0.3) # spacing plt.show() Figure 4.2: Normal Q-Q Plot for Setosa plt.close() Figure 4.3 is the scatter plots matrix of those 50 Setosa iris flowers. Sepal Length and Sepal Width form an elliptical cloud. Sepal Length and Petal Length, Sepal Width and Petal Length form a roughly elliptical cloud. Due to the sparse values of Petal Width, it does not form elliptical cloud of points with any other variables. setosa_df.columns = [&#39;Sepal Length&#39;, &#39;Sepal Width&#39;, &#39;Petal Length&#39;, &#39;Petal Width&#39;, &#39;Species&#39;] setosa_plot_df = setosa_df.drop(columns=&#39;Species&#39;) sns.pairplot(setosa_plot_df, diag_kind=&quot;hist&quot;, corner=False) Figure 4.3: Scatter Plot Matrix for Setosa plt.show() Figure 4.3: Scatter Plot Matrix for Setosa plt.close() \\(\\textbf{Example: Inference on One Mean Vector with Admission Data}\\) The admission data set contains 397 graduate school admissions decisions, among which \\(n_1=271\\) were not admitted and \\(n_2=126\\) were admitted. The explanatory variables are the student’s GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution (values from 1 to 4 with 1 having the highest prestige). The response variable, admit/don’t admit, is a binary variable with 1=admitted and 0=not admitted. Let’s focus on the admitted students with the first two quantitative explanatory variables, GRE and GPA. This creates a one-sample scenario. Given that the sample mean vector \\(\\mathbf{\\bar x}=[618.571, 3.489]^T\\) and the sample covariance matrix and its inverse \\[ \\mathbf{S}=\\left[ \\begin{array}{cc} 11937.143&amp; 9.452\\\\ 9.452&amp; 0.138 \\end{array} \\right]; \\quad \\quad \\mathbf{S}^{-1}=\\left[ \\begin{array}{cc} 8.857592\\times 10^{-5} &amp; -0.006066809\\\\ -0.006066809 &amp; 7.661909264 \\end{array} \\right] \\] We could check whether the two variables GRE and GPA are bivariate normally distributed based on the following four graphs. from scipy.stats import probplot df = pd.read_csv(&quot;data/admission.csv&quot;) df = df.dropna() xx = df[[&#39;gre&#39;, &#39;gpa&#39;]].to_numpy() n, p = xx.shape mean_vec = np.mean(xx, axis=0) cov_mat = np.cov(xx.T) inv_covmat = np.linalg.inv(cov_mat) D2 = np.array([mahalanobis(x, mean_vec, inv_covmat)**2 for x in xx]) # theoretical chi-square quantiles qvec = ((np.arange(1, n + 1) - 0.5) / n) q_theo = chi2.ppf(qvec, df=p) q_obs = np.sort(D2) fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Scatter plot of GRE vs GPA axes[0, 0].scatter(df[&#39;gre&#39;], df[&#39;gpa&#39;], c=&#39;black&#39;) axes[0, 0].set_title(&quot;Scatter Plot of GPA vs GRE&quot;) axes[0, 0].set_xlabel(&quot;GRE&quot;) axes[0, 0].set_ylabel(&quot;GPA&quot;) # QQ plot for GRE probplot(df[&#39;gre&#39;], dist=&quot;norm&quot;, plot=axes[0, 1]) ## ((array([-2.9210196 , -2.63280541, -2.47026126, -2.35476152, -2.26410345, ## -2.18894403, -2.12443167, -2.06771086, -2.0169534 , -1.97091559, ## -1.9287115 , -1.88968692, -1.85334448, -1.81929684, -1.78723626, ## -1.75691401, -1.72812608, -1.70070296, -1.67450224, -1.64940305, ## -1.6253019 , -1.60210948, -1.57974813, -1.55814991, -1.53725502, ## -1.51701052, -1.49736934, -1.47828938, -1.45973291, -1.44166589, ## -1.42405757, -1.40688004, -1.39010789, -1.37371791, -1.35768887, ## -1.34200126, -1.32663715, -1.31157996, -1.2968144 , -1.28232629, ## -1.26810245, -1.25413065, -1.24039947, -1.22689826, -1.21361706, ## -1.20054655, -1.18767798, -1.17500315, -1.16251432, -1.15020422, ## -1.138066 , -1.12609318, -1.11427965, -1.10261961, -1.09110758, ## -1.07973837, -1.06850703, -1.05740889, -1.04643948, -1.03559457, ## -1.0248701 , -1.01426223, -1.00376729, -0.99338175, -0.98310227, ## -0.97292563, -0.96284878, -0.95286875, -0.94298274, -0.93318804, ## -0.92348206, -0.91386231, -0.90432639, -0.89487201, -0.88549694, ## -0.87619907, -0.86697633, -0.85782675, -0.84874843, -0.83973953, ## -0.83079827, -0.82192294, -0.81311188, -0.80436351, -0.79567627, ## -0.78704866, -0.77847925, -0.76996662, -0.76150943, -0.75310636, ## -0.74475613, -0.73645751, -0.7282093 , -0.72001034, -0.7118595 , ## -0.70375568, -0.69569781, -0.68768487, -0.67971583, -0.67178974, ## -0.66390562, -0.65606255, -0.64825964, -0.640496 , -0.63277078, ## -0.62508313, -0.61743225, -0.60981734, -0.60223763, -0.59469237, ## -0.58718081, -0.57970224, -0.57225594, -0.56484125, -0.55745748, ## -0.55010398, -0.5427801 , -0.53548523, -0.52821874, -0.52098003, ## -0.51376852, -0.50658363, -0.4994248 , -0.49229147, -0.48518311, ## -0.47809918, -0.47103915, -0.46400253, -0.45698881, -0.4499975 , ## -0.44302812, -0.43608018, -0.42915324, -0.42224683, -0.4153605 , ## -0.40849381, -0.40164632, -0.39481762, -0.38800728, -0.38121489, ## -0.37444004, -0.36768234, -0.36094138, -0.35421679, -0.34750817, ## -0.34081516, -0.33413738, -0.32747447, -0.32082607, -0.31419181, ## -0.30757136, -0.30096436, -0.29437047, -0.28778936, -0.28122069, ## -0.27466412, -0.26811935, -0.26158604, -0.25506388, -0.24855255, ## -0.24205174, -0.23556114, -0.22908045, -0.22260937, -0.2161476 , ## -0.20969484, -0.2032508 , -0.19681519, -0.19038772, -0.1839681 , ## -0.17755606, -0.17115131, -0.16475358, -0.15836258, -0.15197804, ## -0.14559969, -0.13922726, -0.13286048, -0.12649907, -0.12014279, ## -0.11379135, -0.1074445 , -0.10110198, -0.09476352, -0.08842887, ## -0.08209776, -0.07576994, -0.06944516, -0.06312315, -0.05680366, ## -0.05048644, -0.04417124, -0.03785779, -0.03154586, -0.02523518, ## -0.01892551, -0.01261659, -0.00630817, 0. , 0.00630817, ## 0.01261659, 0.01892551, 0.02523518, 0.03154586, 0.03785779, ## 0.04417124, 0.05048644, 0.05680366, 0.06312315, 0.06944516, ## 0.07576994, 0.08209776, 0.08842887, 0.09476352, 0.10110198, ## 0.1074445 , 0.11379135, 0.12014279, 0.12649907, 0.13286048, ## 0.13922726, 0.14559969, 0.15197804, 0.15836258, 0.16475358, ## 0.17115131, 0.17755606, 0.1839681 , 0.19038772, 0.19681519, ## 0.2032508 , 0.20969484, 0.2161476 , 0.22260937, 0.22908045, ## 0.23556114, 0.24205174, 0.24855255, 0.25506388, 0.26158604, ## 0.26811935, 0.27466412, 0.28122069, 0.28778936, 0.29437047, ## 0.30096436, 0.30757136, 0.31419181, 0.32082607, 0.32747447, ## 0.33413738, 0.34081516, 0.34750817, 0.35421679, 0.36094138, ## 0.36768234, 0.37444004, 0.38121489, 0.38800728, 0.39481762, ## 0.40164632, 0.40849381, 0.4153605 , 0.42224683, 0.42915324, ## 0.43608018, 0.44302812, 0.4499975 , 0.45698881, 0.46400253, ## 0.47103915, 0.47809918, 0.48518311, 0.49229147, 0.4994248 , ## 0.50658363, 0.51376852, 0.52098003, 0.52821874, 0.53548523, ## 0.5427801 , 0.55010398, 0.55745748, 0.56484125, 0.57225594, ## 0.57970224, 0.58718081, 0.59469237, 0.60223763, 0.60981734, ## 0.61743225, 0.62508313, 0.63277078, 0.640496 , 0.64825964, ## 0.65606255, 0.66390562, 0.67178974, 0.67971583, 0.68768487, ## 0.69569781, 0.70375568, 0.7118595 , 0.72001034, 0.7282093 , ## 0.73645751, 0.74475613, 0.75310636, 0.76150943, 0.76996662, ## 0.77847925, 0.78704866, 0.79567627, 0.80436351, 0.81311188, ## 0.82192294, 0.83079827, 0.83973953, 0.84874843, 0.85782675, ## 0.86697633, 0.87619907, 0.88549694, 0.89487201, 0.90432639, ## 0.91386231, 0.92348206, 0.93318804, 0.94298274, 0.95286875, ## 0.96284878, 0.97292563, 0.98310227, 0.99338175, 1.00376729, ## 1.01426223, 1.0248701 , 1.03559457, 1.04643948, 1.05740889, ## 1.06850703, 1.07973837, 1.09110758, 1.10261961, 1.11427965, ## 1.12609318, 1.138066 , 1.15020422, 1.16251432, 1.17500315, ## 1.18767798, 1.20054655, 1.21361706, 1.22689826, 1.24039947, ## 1.25413065, 1.26810245, 1.28232629, 1.2968144 , 1.31157996, ## 1.32663715, 1.34200126, 1.35768887, 1.37371791, 1.39010789, ## 1.40688004, 1.42405757, 1.44166589, 1.45973291, 1.47828938, ## 1.49736934, 1.51701052, 1.53725502, 1.55814991, 1.57974813, ## 1.60210948, 1.6253019 , 1.64940305, 1.67450224, 1.70070296, ## 1.72812608, 1.75691401, 1.78723626, 1.81929684, 1.85334448, ## 1.88968692, 1.9287115 , 1.97091559, 2.0169534 , 2.06771086, ## 2.12443167, 2.18894403, 2.26410345, 2.35476152, 2.47026126, ## 2.63280541, 2.9210196 ]), array([220., 300., 300., 300., 340., 340., 340., 340., 360., 360., 360., ## 360., 380., 380., 380., 380., 380., 380., 380., 380., 400., 400., ## 400., 400., 400., 400., 400., 400., 400., 400., 400., 420., 420., ## 420., 420., 420., 420., 420., 440., 440., 440., 440., 440., 440., ## 440., 440., 440., 440., 460., 460., 460., 460., 460., 460., 460., ## 460., 460., 460., 460., 460., 460., 480., 480., 480., 480., 480., ## 480., 480., 480., 480., 480., 480., 480., 480., 480., 480., 480., ## 500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 500., ## 500., 500., 500., 500., 500., 500., 500., 500., 500., 500., 520., ## 520., 520., 520., 520., 520., 520., 520., 520., 520., 520., 520., ## 520., 520., 520., 520., 520., 520., 520., 520., 520., 520., 520., ## 520., 540., 540., 540., 540., 540., 540., 540., 540., 540., 540., ## 540., 540., 540., 540., 540., 540., 540., 540., 540., 540., 540., ## 540., 540., 540., 540., 540., 540., 560., 560., 560., 560., 560., ## 560., 560., 560., 560., 560., 560., 560., 560., 560., 560., 560., ## 560., 560., 560., 560., 560., 560., 560., 560., 580., 580., 580., ## 580., 580., 580., 580., 580., 580., 580., 580., 580., 580., 580., ## 580., 580., 580., 580., 580., 580., 580., 580., 580., 580., 580., ## 580., 580., 580., 600., 600., 600., 600., 600., 600., 600., 600., ## 600., 600., 600., 600., 600., 600., 600., 600., 600., 600., 600., ## 600., 600., 600., 600., 620., 620., 620., 620., 620., 620., 620., ## 620., 620., 620., 620., 620., 620., 620., 620., 620., 620., 620., ## 620., 620., 620., 620., 620., 620., 620., 620., 620., 620., 620., ## 620., 640., 640., 640., 640., 640., 640., 640., 640., 640., 640., ## 640., 640., 640., 640., 640., 640., 640., 640., 640., 640., 640., ## 660., 660., 660., 660., 660., 660., 660., 660., 660., 660., 660., ## 660., 660., 660., 660., 660., 660., 660., 660., 660., 660., 660., ## 660., 680., 680., 680., 680., 680., 680., 680., 680., 680., 680., ## 680., 680., 680., 680., 680., 680., 680., 680., 680., 680., 700., ## 700., 700., 700., 700., 700., 700., 700., 700., 700., 700., 700., ## 700., 700., 700., 700., 700., 700., 700., 700., 700., 700., 720., ## 720., 720., 720., 720., 720., 720., 720., 720., 720., 720., 740., ## 740., 740., 740., 740., 740., 740., 740., 740., 740., 740., 760., ## 760., 760., 760., 760., 780., 780., 780., 780., 780., 800., 800., ## 800., 800., 800., 800., 800., 800., 800., 800., 800., 800., 800., ## 800., 800., 800., 800., 800., 800., 800., 800., 800., 800., 800., ## 800.])), (np.float64(115.55472653820884), np.float64(587.8589420654912), np.float64(0.9936469932510308))) axes[0, 1].set_title(&quot;Normal Q-Q Plot for GRE&quot;) # QQ plot for GPA probplot(df[&#39;gpa&#39;], dist=&quot;norm&quot;, plot=axes[1, 0]) ## ((array([-2.9210196 , -2.63280541, -2.47026126, -2.35476152, -2.26410345, ## -2.18894403, -2.12443167, -2.06771086, -2.0169534 , -1.97091559, ## -1.9287115 , -1.88968692, -1.85334448, -1.81929684, -1.78723626, ## -1.75691401, -1.72812608, -1.70070296, -1.67450224, -1.64940305, ## -1.6253019 , -1.60210948, -1.57974813, -1.55814991, -1.53725502, ## -1.51701052, -1.49736934, -1.47828938, -1.45973291, -1.44166589, ## -1.42405757, -1.40688004, -1.39010789, -1.37371791, -1.35768887, ## -1.34200126, -1.32663715, -1.31157996, -1.2968144 , -1.28232629, ## -1.26810245, -1.25413065, -1.24039947, -1.22689826, -1.21361706, ## -1.20054655, -1.18767798, -1.17500315, -1.16251432, -1.15020422, ## -1.138066 , -1.12609318, -1.11427965, -1.10261961, -1.09110758, ## -1.07973837, -1.06850703, -1.05740889, -1.04643948, -1.03559457, ## -1.0248701 , -1.01426223, -1.00376729, -0.99338175, -0.98310227, ## -0.97292563, -0.96284878, -0.95286875, -0.94298274, -0.93318804, ## -0.92348206, -0.91386231, -0.90432639, -0.89487201, -0.88549694, ## -0.87619907, -0.86697633, -0.85782675, -0.84874843, -0.83973953, ## -0.83079827, -0.82192294, -0.81311188, -0.80436351, -0.79567627, ## -0.78704866, -0.77847925, -0.76996662, -0.76150943, -0.75310636, ## -0.74475613, -0.73645751, -0.7282093 , -0.72001034, -0.7118595 , ## -0.70375568, -0.69569781, -0.68768487, -0.67971583, -0.67178974, ## -0.66390562, -0.65606255, -0.64825964, -0.640496 , -0.63277078, ## -0.62508313, -0.61743225, -0.60981734, -0.60223763, -0.59469237, ## -0.58718081, -0.57970224, -0.57225594, -0.56484125, -0.55745748, ## -0.55010398, -0.5427801 , -0.53548523, -0.52821874, -0.52098003, ## -0.51376852, -0.50658363, -0.4994248 , -0.49229147, -0.48518311, ## -0.47809918, -0.47103915, -0.46400253, -0.45698881, -0.4499975 , ## -0.44302812, -0.43608018, -0.42915324, -0.42224683, -0.4153605 , ## -0.40849381, -0.40164632, -0.39481762, -0.38800728, -0.38121489, ## -0.37444004, -0.36768234, -0.36094138, -0.35421679, -0.34750817, ## -0.34081516, -0.33413738, -0.32747447, -0.32082607, -0.31419181, ## -0.30757136, -0.30096436, -0.29437047, -0.28778936, -0.28122069, ## -0.27466412, -0.26811935, -0.26158604, -0.25506388, -0.24855255, ## -0.24205174, -0.23556114, -0.22908045, -0.22260937, -0.2161476 , ## -0.20969484, -0.2032508 , -0.19681519, -0.19038772, -0.1839681 , ## -0.17755606, -0.17115131, -0.16475358, -0.15836258, -0.15197804, ## -0.14559969, -0.13922726, -0.13286048, -0.12649907, -0.12014279, ## -0.11379135, -0.1074445 , -0.10110198, -0.09476352, -0.08842887, ## -0.08209776, -0.07576994, -0.06944516, -0.06312315, -0.05680366, ## -0.05048644, -0.04417124, -0.03785779, -0.03154586, -0.02523518, ## -0.01892551, -0.01261659, -0.00630817, 0. , 0.00630817, ## 0.01261659, 0.01892551, 0.02523518, 0.03154586, 0.03785779, ## 0.04417124, 0.05048644, 0.05680366, 0.06312315, 0.06944516, ## 0.07576994, 0.08209776, 0.08842887, 0.09476352, 0.10110198, ## 0.1074445 , 0.11379135, 0.12014279, 0.12649907, 0.13286048, ## 0.13922726, 0.14559969, 0.15197804, 0.15836258, 0.16475358, ## 0.17115131, 0.17755606, 0.1839681 , 0.19038772, 0.19681519, ## 0.2032508 , 0.20969484, 0.2161476 , 0.22260937, 0.22908045, ## 0.23556114, 0.24205174, 0.24855255, 0.25506388, 0.26158604, ## 0.26811935, 0.27466412, 0.28122069, 0.28778936, 0.29437047, ## 0.30096436, 0.30757136, 0.31419181, 0.32082607, 0.32747447, ## 0.33413738, 0.34081516, 0.34750817, 0.35421679, 0.36094138, ## 0.36768234, 0.37444004, 0.38121489, 0.38800728, 0.39481762, ## 0.40164632, 0.40849381, 0.4153605 , 0.42224683, 0.42915324, ## 0.43608018, 0.44302812, 0.4499975 , 0.45698881, 0.46400253, ## 0.47103915, 0.47809918, 0.48518311, 0.49229147, 0.4994248 , ## 0.50658363, 0.51376852, 0.52098003, 0.52821874, 0.53548523, ## 0.5427801 , 0.55010398, 0.55745748, 0.56484125, 0.57225594, ## 0.57970224, 0.58718081, 0.59469237, 0.60223763, 0.60981734, ## 0.61743225, 0.62508313, 0.63277078, 0.640496 , 0.64825964, ## 0.65606255, 0.66390562, 0.67178974, 0.67971583, 0.68768487, ## 0.69569781, 0.70375568, 0.7118595 , 0.72001034, 0.7282093 , ## 0.73645751, 0.74475613, 0.75310636, 0.76150943, 0.76996662, ## 0.77847925, 0.78704866, 0.79567627, 0.80436351, 0.81311188, ## 0.82192294, 0.83079827, 0.83973953, 0.84874843, 0.85782675, ## 0.86697633, 0.87619907, 0.88549694, 0.89487201, 0.90432639, ## 0.91386231, 0.92348206, 0.93318804, 0.94298274, 0.95286875, ## 0.96284878, 0.97292563, 0.98310227, 0.99338175, 1.00376729, ## 1.01426223, 1.0248701 , 1.03559457, 1.04643948, 1.05740889, ## 1.06850703, 1.07973837, 1.09110758, 1.10261961, 1.11427965, ## 1.12609318, 1.138066 , 1.15020422, 1.16251432, 1.17500315, ## 1.18767798, 1.20054655, 1.21361706, 1.22689826, 1.24039947, ## 1.25413065, 1.26810245, 1.28232629, 1.2968144 , 1.31157996, ## 1.32663715, 1.34200126, 1.35768887, 1.37371791, 1.39010789, ## 1.40688004, 1.42405757, 1.44166589, 1.45973291, 1.47828938, ## 1.49736934, 1.51701052, 1.53725502, 1.55814991, 1.57974813, ## 1.60210948, 1.6253019 , 1.64940305, 1.67450224, 1.70070296, ## 1.72812608, 1.75691401, 1.78723626, 1.81929684, 1.85334448, ## 1.88968692, 1.9287115 , 1.97091559, 2.0169534 , 2.06771086, ## 2.12443167, 2.18894403, 2.26410345, 2.35476152, 2.47026126, ## 2.63280541, 2.9210196 ]), array([2.26, 2.42, 2.42, 2.48, 2.52, 2.55, 2.56, 2.62, 2.62, 2.63, 2.65, ## 2.67, 2.67, 2.68, 2.69, 2.7 , 2.7 , 2.71, 2.71, 2.73, 2.76, 2.78, ## 2.78, 2.79, 2.79, 2.81, 2.81, 2.81, 2.82, 2.82, 2.83, 2.84, 2.85, ## 2.85, 2.86, 2.86, 2.88, 2.9 , 2.9 , 2.9 , 2.9 , 2.91, 2.91, 2.91, ## 2.92, 2.92, 2.93, 2.93, 2.93, 2.93, 2.93, 2.94, 2.94, 2.94, 2.95, ## 2.96, 2.96, 2.97, 2.97, 2.98, 2.98, 2.98, 2.98, 2.98, 2.98, 3. , ## 3. , 3. , 3. , 3.01, 3.01, 3.02, 3.02, 3.02, 3.02, 3.03, 3.04, ## 3.04, 3.05, 3.05, 3.05, 3.06, 3.07, 3.07, 3.07, 3.07, 3.08, 3.08, ## 3.08, 3.08, 3.09, 3.1 , 3.11, 3.12, 3.12, 3.12, 3.12, 3.13, 3.13, ## 3.13, 3.13, 3.13, 3.14, 3.14, 3.14, 3.14, 3.15, 3.15, 3.15, 3.15, ## 3.15, 3.15, 3.15, 3.16, 3.16, 3.17, 3.17, 3.17, 3.17, 3.17, 3.18, ## 3.19, 3.19, 3.19, 3.19, 3.19, 3.2 , 3.2 , 3.21, 3.22, 3.22, 3.22, ## 3.22, 3.22, 3.23, 3.23, 3.23, 3.24, 3.24, 3.25, 3.25, 3.27, 3.27, ## 3.27, 3.28, 3.28, 3.28, 3.28, 3.29, 3.29, 3.3 , 3.3 , 3.3 , 3.3 , ## 3.31, 3.31, 3.31, 3.31, 3.31, 3.31, 3.31, 3.31, 3.32, 3.32, 3.32, ## 3.32, 3.33, 3.33, 3.33, 3.33, 3.33, 3.34, 3.34, 3.34, 3.34, 3.34, ## 3.35, 3.35, 3.35, 3.35, 3.35, 3.35, 3.35, 3.36, 3.36, 3.36, 3.36, ## 3.37, 3.37, 3.37, 3.38, 3.38, 3.38, 3.38, 3.38, 3.39, 3.39, 3.39, ## 3.4 , 3.4 , 3.4 , 3.4 , 3.4 , 3.4 , 3.4 , 3.41, 3.42, 3.43, 3.43, ## 3.43, 3.43, 3.43, 3.44, 3.44, 3.44, 3.45, 3.45, 3.45, 3.45, 3.45, ## 3.45, 3.45, 3.46, 3.46, 3.46, 3.46, 3.46, 3.47, 3.47, 3.47, 3.48, ## 3.48, 3.48, 3.49, 3.49, 3.49, 3.49, 3.5 , 3.5 , 3.5 , 3.5 , 3.51, ## 3.51, 3.51, 3.51, 3.51, 3.52, 3.52, 3.52, 3.52, 3.53, 3.53, 3.54, ## 3.54, 3.54, 3.55, 3.56, 3.56, 3.56, 3.57, 3.57, 3.57, 3.58, 3.58, ## 3.58, 3.58, 3.58, 3.59, 3.59, 3.59, 3.59, 3.59, 3.6 , 3.6 , 3.6 , ## 3.61, 3.61, 3.61, 3.62, 3.62, 3.63, 3.63, 3.63, 3.63, 3.63, 3.63, ## 3.64, 3.64, 3.64, 3.64, 3.64, 3.65, 3.65, 3.65, 3.65, 3.66, 3.67, ## 3.67, 3.67, 3.67, 3.69, 3.69, 3.69, 3.7 , 3.7 , 3.7 , 3.71, 3.71, ## 3.72, 3.73, 3.73, 3.74, 3.74, 3.74, 3.74, 3.75, 3.75, 3.76, 3.76, ## 3.77, 3.77, 3.77, 3.77, 3.77, 3.78, 3.78, 3.78, 3.78, 3.8 , 3.8 , ## 3.81, 3.81, 3.81, 3.82, 3.83, 3.84, 3.84, 3.85, 3.86, 3.86, 3.87, ## 3.88, 3.88, 3.88, 3.89, 3.89, 3.89, 3.9 , 3.9 , 3.9 , 3.91, 3.92, ## 3.92, 3.93, 3.94, 3.94, 3.94, 3.94, 3.94, 3.95, 3.95, 3.95, 3.95, ## 3.95, 3.97, 3.98, 3.99, 3.99, 3.99, 4. , 4. , 4. , 4. , 4. , ## 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , ## 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , 4. , ## 4. ])), (np.float64(0.37814377982297576), np.float64(3.3922418136020154), np.float64(0.9896460740588751))) axes[1, 0].set_title(&quot;Normal Q-Q Plot for GPA&quot;) # Chi-square Q-Q Plot axes[1, 1].plot(q_theo, q_obs, &#39;ko&#39;) axes[1, 1].plot(q_theo, q_theo, &#39;r-&#39;, linewidth=2) axes[1, 1].set_title(&quot;Chi-square Q-Q Plot for Admission Data&quot;) axes[1, 1].set_xlabel(&quot;Theoretical Quantile&quot;) axes[1, 1].set_ylabel(&quot;Observed Quantile&quot;) plt.subplots_adjust(hspace=0.6, wspace=0.3) plt.show() plt.close() Based on graphs above, address whether the bivariate normality assumption is satisfied. Test at the 5% significance level \\[ H_0: \\boldsymbol{\\mu}=\\left[ \\begin{array}{c} 600\\\\ 3.4 \\end{array} \\right] \\text{ versus } H_a: \\boldsymbol{\\mu}\\ne\\left[ \\begin{array}{c} 600\\\\ 3.4 \\end{array} \\right]. \\] Use \\(T^2=(\\mathbf{\\bar x}-\\boldsymbol{\\mu}_0)^T\\mathbf{S}^{-1}(\\mathbf{\\bar x}-\\boldsymbol{\\mu}_0)=0.07137872\\). Shall we follow up with a post hoc test at 5% significance level? Obtain a 95% confidence interval for GRE and GPA respectively using the Bonferroni method. Obtain a 95% simultaneous confidence interval for GRE and GPA respectively. Compare the intervals with Bonferroni intervals above. We can use to confirm the Bonferroni interval and the simultaneous confidence interval. from scipy.stats import t, f def bonferroni_intervals(x, alpha=0.05): &quot;&quot;&quot; Bonferroni corrected confidence intervals for each variable in x. x: n by p numpy array alpha: significance level &quot;&quot;&quot; n, p = x.shape mvec = np.mean(x, axis=0) svec = np.std(x, axis=0, ddof=1) tscore = t.ppf(1 - alpha/(2*p), df=n-1) lvec = mvec - tscore * svec / np.sqrt(n) uvec = mvec + tscore * svec / np.sqrt(n) return np.column_stack((lvec, uvec)) def simultaneous_intervals(x, alpha=0.05): &quot;&quot;&quot; Simultaneous confidence intervals for each variable in x. x: n by p numpy array alpha: significance level &quot;&quot;&quot; n, p = x.shape mvec = np.mean(x, axis=0) svec = np.std(x, axis=0, ddof=1) fscore = f.ppf(1 - alpha, dfn=p, dfd=n-p) multiplier = np.sqrt(fscore * p * (n - 1) / (n - p)) lvec = mvec - multiplier * svec / np.sqrt(n) uvec = mvec + multiplier * svec / np.sqrt(n) return np.column_stack((lvec, uvec)) np.random.seed(4061) axx_df = df.loc[df[&#39;admit&#39;] == 1, [&#39;gre&#39;, &#39;gpa&#39;]] axx = axx_df.to_numpy() cib = bonferroni_intervals(axx, 0.05) cis = simultaneous_intervals(axx, 0.05) print(&quot;Bonferroni intervals:\\n&quot;, cib) ## Bonferroni intervals: ## [[596.4890261 640.65383104] ## [ 3.41408974 3.56432296]] print(&quot;Simultaneous intervals:\\n&quot;, cis) ## Simultaneous intervals: ## [[594.35875229 642.78410486] ## [ 3.40684329 3.57156941]] We can construct simultaneous confidence region, simultaneous confidence interval, and Bonferroni interval to compare the two intervals. The confidence region is an ellipsoid centered at the mean vector. Since the hypothesized value \\(\\boldsymbol{\\mu}_0=[600, 3.4]^T\\) is outside the confidence region, we should reject the null hypothesis. from numpy.linalg import inv, eigvals # Manual ellipse in python, unlike R def confidence_ellipse(cov, mean, n, p, alpha=0.05, points=100): &quot;&quot;&quot; Generate points on the confidence ellipse. cov: covariance matrix (p x p) mean: mean vector (length p) n: sample size p: dimension alpha: significance level points: number of points on ellipse Returns: array of shape (points, p) with ellipse coordinates &quot;&quot;&quot; f_score = f.ppf(1 - alpha, dfn=p, dfd=n-p) radius = np.sqrt(p * (n - 1) / (n * (n - p)) * f_score) vals, vecs = np.linalg.eigh(cov) order = vals.argsort()[::-1] vals = vals[order] vecs = vecs[:, order] theta = np.linspace(0, 2 * np.pi, points) circle = np.array([np.cos(theta), np.sin(theta)]) # unit circle points axis_lengths = radius * np.sqrt(vals) ellipse = (vecs @ np.diag(axis_lengths)) @ circle ellipse = ellipse.T + mean return ellipse n, p = axx.shape mean_vec = np.mean(axx, axis=0) cov_mat = np.cov(axx, rowvar=False) alpha = 0.05 mu0 = np.array([600, 3.4]) # hypothesized mean ellip_points = confidence_ellipse(cov_mat, mean_vec, n, p, alpha) # Plot plt.figure(figsize=(8,6)) plt.plot(ellip_points[:,0], ellip_points[:,1], label=&#39;Simultaneous Confidence Region (Ellipsoid)&#39;) plt.scatter(*mu0, color=&#39;red&#39;, label=r&#39;$\\mu_0$ (Hypothesized Mean)&#39;, zorder=5) plt.scatter(*mean_vec, color=&#39;green&#39;, label=&#39;Sample Mean&#39;, zorder=5) # Plot simultaneous confidence intervals (from previous function) cis = simultaneous_intervals(axx, alpha) plt.axvline(cis[0,0], color=&#39;black&#39;, linestyle=&#39;-&#39;, linewidth=2) plt.axvline(cis[0,1], color=&#39;black&#39;, linestyle=&#39;-&#39;, linewidth=2) plt.axhline(cis[1,0], color=&#39;black&#39;, linestyle=&#39;-&#39;, linewidth=2) plt.axhline(cis[1,1], color=&#39;black&#39;, linestyle=&#39;-&#39;, linewidth=2) # Plot Bonferroni intervals (from previous function) cib = bonferroni_intervals(axx, alpha) plt.axvline(cib[0,0], color=&#39;blue&#39;, linestyle=&#39;--&#39;, linewidth=2) plt.axvline(cib[0,1], color=&#39;blue&#39;, linestyle=&#39;--&#39;, linewidth=2) plt.axhline(cib[1,0], color=&#39;blue&#39;, linestyle=&#39;--&#39;, linewidth=2) plt.axhline(cib[1,1], color=&#39;blue&#39;, linestyle=&#39;--&#39;, linewidth=2) plt.xlabel(&#39;GRE&#39;) plt.ylabel(&#39;GPA&#39;) plt.title(&#39;Simultaneous Confidence Region and Intervals&#39;) plt.legend() plt.grid(True) plt.show() plt.close() 4.2 Hypothesis Test for Two Mean Vectors Before diving to the multivariate case, let’s review how we compare two population means in the univariate case. 4.2.1 Univariate Case Based on Two Independent Samples Suppose there are two populations; we want to compare whether the two population means are the same or not based on two independent samples. ## (np.float64(-0.5), np.float64(775.5), np.float64(547.5), np.float64(-0.5)) \\(\\textbf{Two-sample t Tests}\\) Hypotheses: \\(H_0: \\mu_1-\\mu_2=\\Delta_0 \\mbox{ versus } H_a: \\mu_1-\\mu_2\\ne \\Delta_0\\) [ \\[\\begin{array}{c|c} \\text{Two-sample Pooled t Test} &amp; \\text{Two-sample Non-pooled t Test} \\\\ ----- &amp; ----- \\\\ \\text{Assumptions} &amp; \\text{Assumptions} \\\\ \\text{- Simple Random Samples} &amp; \\text{- Simple Random Samples} \\\\ \\text{- Two independent samples} &amp; \\text{- Two independent samples} \\\\ \\text{- Normal populations or large samples} &amp; \\text{- Normal populations or large samples} \\\\ \\frac{\\max\\{s_1, s_2\\}}{\\min\\{s_1,s_2\\}}&lt;2 &amp; \\\\ \\bullet{\\text{ Test Statistic:}} &amp; \\bullet{\\text{ Test Statistic:}} \\\\ t_o=\\frac{(\\bar x_1-\\bar x_2)-\\Delta_0}{\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}, df=n_1+n_2-2 &amp; t_o=\\frac{(\\bar x_1-\\bar x_2)-\\Delta_0}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}} \\\\ \\\\ \\text{Reject $H_0$ if $|t_o|\\ge t_{n1+n2-2}(\\alpha/2)$.} &amp; \\text{Reject $H_0$ if $|t_o|\\ge t_{df^{\\ast}}(\\alpha/2)$.} \\\\ \\end{array}\\] ] 4.2.2 Multivariate Case Based on Two Independent Samples We generalize the ideas to the multivariate case, we want to compare whether the two mean vectors are the same, that is, \\[ H_0: \\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2}=\\boldsymbol{\\Delta_0} \\mbox{ versus } \\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2}=\\boldsymbol{\\Delta_0}. \\] If the two populations have the same covariance matrix, we could pool the two samples and obtain a better estimate of the common covariance matrix \\[ \\mathbf{S}_{\\mbox{pooled}}=\\frac{(n_1-1)\\mathbf{S_1}+(n_2-1)\\mathbf{S_2}}{n_1+n_2-2} \\] As a result, \\[ \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\mathbf{S}_{\\mbox{pooled}} \\] is an estimator of \\(\\boldsymbol{\\Sigma}_1=\\boldsymbol{\\Sigma}_2=\\boldsymbol{\\Sigma}=\\text{Var}(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})\\). And we can use the pooled Hotelling’s \\(T^2\\) test. \\(\\textbf{Assumptions}:\\) Simple random samples Two independent samples Multivariate normal populations or large samples. We could use the chi-square probability plot to assess the multivariate normality assumption. One plot for one sample. If the multivariate normal populations’ assumption is satisfied, the data points should roughly fall on a 45-degree straight line for both samples. Note that the Central Limit Theorem implies that the sample mean vectors are going to be approximately multivariate normally distributed regardless of the distribution of the original variables when the sample sizes are large enough. Therefore, in general, Hotelling’s \\(T^2\\) is not going to be sensitive to violations of the multivariate normal assumption. * \\(\\boldsymbol{\\Sigma}_1=\\boldsymbol{\\Sigma}_2\\). This assumption may be assessed using Barlett’s Test. The hypotheses are \\(H_0: \\boldsymbol{\\Sigma}_1=\\boldsymbol{\\Sigma}_2\\) versus \\(H_a: \\boldsymbol{\\Sigma}_1\\ne \\boldsymbol{\\Sigma}_2\\). The test statistic for Bartlett’s Test is given by L prime as shown below: \\[ L^{&#39;}=c\\{(n_1+n_2-2)\\log|\\mathbf{S}_{\\mbox{pooled}}|-(n_1-1)\\log|\\boldsymbol{\\Sigma}_1|-(n_2-1)\\log|\\boldsymbol{\\Sigma}_2|\\} \\mbox{ with} \\] \\[ c=1-\\frac{2p^2+3p-1}{6(p-1)}\\left[\\frac{1}{n_1-1}+\\frac{1}{n_2-1}-\\frac{1}{n_1+n_2-2}\\right]. \\] Under the null hypothesis, the Bartlett’s test statistic is approximately chi-square distributed with \\(df=\\frac{p(p+1)}{2}\\). Therefore, we reject \\(H_0: \\boldsymbol{\\Sigma}_1=\\boldsymbol{\\Sigma}_2\\) at significance level \\(\\alpha\\) if \\(L^{&#39;}\\ge \\chi_{\\frac{p(p+1)}{2}}^2 (\\alpha)\\). Note that Bartlett’s test is not robust to the violations of multivariate assumption and should not be used if there is any indication that the data are not multivariate normally distributed. The test statistic of the Hotelling’s \\(T^2\\) test for comparing two mean vectors is \\[ T^2=[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta}_0]^{T}\\left[\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\mathbf{S}_{\\mbox{pooled}}\\right]^{-1}[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta}_0] \\] with \\[ \\widehat{\\boldsymbol{\\Sigma}}=\\mathbf{S}_{\\mbox{pooled}}=\\frac{(n_1-1)\\mathbf{S}_1+(n_2-1)\\mathbf{S}_2}{n_1+n_2-2}. \\] It can be shown that \\(T^2\\sim \\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p, n_1+n_2-p-1}\\). Therefore, we can reject \\(H_0\\) if \\[ \\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2\\ge F_{p, n_1+n_2-p-1}(\\alpha). \\] 4.2.3 Two-sample Non-pooled Hotelling’s \\(T^2\\) Test If \\(\\boldsymbol{\\Sigma}_1\\ne \\boldsymbol{\\Sigma}_2\\) and the sample sizes \\(n_1\\), \\(n_2\\) are not large enough, the test statistic becomes \\[ T^2=[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta}_0]^{T}\\left[\\frac{1}{n_1}\\mathbf{S}_1+\\frac{1}{n_2}\\mathbf{S}_2\\right]^{-1}[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta}_0]. \\] It can be shown that \\(T^2\\sim \\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p, \\gamma}\\), where \\[ \\frac{1}{\\gamma}=\\sum_{i=1}^2 \\frac{1}{n_i-1}\\left\\{\\frac{[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta}_0]^{T}\\mathbf{S}_T^{-1}(\\frac{1}{n_i} \\mathbf{S}_i) \\mathbf{S}_T^{-1} [(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta}_0]}{T^2}\\right\\}^2 \\] where \\(\\mathbf{S}_T=\\frac{1}{n_1}\\mathbf{S}_1+\\frac{1}{n_2}\\mathbf{S}_2\\). Therefore, we can reject \\(H_0\\) if \\[ \\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2\\ge F_{p, \\gamma}(\\alpha). \\] If sample sizes \\(n_1\\) and \\(n_2\\) are large enough such that \\(n_1-p\\) and \\(n_2-p\\) are large, then we should reject \\(H_0: \\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2}=\\boldsymbol{\\Delta_0}\\) if \\[ [(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta_0}]^{T}\\left[\\frac{1}{n_1}\\mathbf{S_1}+\\frac{1}{n_2}\\mathbf{S_2}\\right]^{-1}[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-\\boldsymbol{\\Delta_0}]\\ge \\chi^2_{p}(\\alpha). \\] 4.2.4 Two-sample Hotelling’s \\(T^2\\) Confidence Interval If we reject \\(H_0: \\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2=\\boldsymbol{\\Delta}_0\\), we would like to figure out in which variables the means are different. A \\((1-\\alpha)\\times 100 \\%\\) simultaneous confidence interval for the \\(i\\)th variable \\(X_i\\) is given by \\[ (\\bar x_{1i}-\\bar x_{2i}) \\pm \\sqrt{\\frac{(n_1+n_2-2)p}{n_1+n_2-p-1}F_{p, n_1+n_2-p-1}(\\alpha)}\\sqrt{s_{\\mbox{ ${pooled}$ },i}^2(\\frac{1}{n_1}+\\frac{1}{n_2})}, i=1, 2, \\cdots, p \\] where \\(\\bar x_{1i}\\) and \\(\\bar x_{2i}\\) are the sample mean of \\(X_i\\) based on the first and the second sample respectively. That is \\[ \\bar x_{1i}=\\frac{1}{n_1}\\sum_{k=1}^{n_1} x_{1ik}, \\bar x_{2i}=\\frac{1}{n_2}\\sum_{k=1}^{n_2} x_{2ik}. \\] And $s_{,i}^2$ is the pooled sample variance of the \\(X_i\\). That is \\[ \\small s_{\\mbox{ ${pooled}$ },i}^2=\\frac{(n_1-1)s_{1i}^2+(n_2-1)s_{2i}^2}{n_1+n_2-2}, s_{1i}^2=\\frac{1}{n_1-1}\\sum_{k=1}^{n_1}(x_{1ik}-\\bar x_{1i})^2, s_{2i}^2=\\frac{1}{n_2-1}\\sum_{k=1}^{n_2}(x_{2ik}-\\bar x_{2i})^2. \\] A \\((1-\\alpha)\\times 100 \\%\\) Bonferroni interval To address the multiple comparisons problem, a \\((1-\\alpha)\\times 100 \\%\\) Bonferroni interval is given by \\[ (\\bar x_{1i}-\\bar x_{2i}) \\pm t_{n_1+n_2-2}(\\frac{\\alpha}{2p})\\sqrt{s_{\\mbox{ ${pooled}$ },i}^2(\\frac{1}{n_1}+\\frac{1}{n_2})}, i=1, 2, \\cdots, p, \\] where \\(t_{n_1+n_2-2}(\\frac{\\alpha}{2p})\\) is the \\(\\frac{\\alpha}{2p}\\times 100\\)th upper-tailed quantile of the \\(t\\) distribution with \\(df=n_1+n_2-2\\). The mean vector is different in \\(X_i\\) by \\(\\Delta_{0, i}\\) if \\(\\Delta_{0,i}\\) is outside the interval. \\(\\textbf{Example: Comparing Two Mean Vectors with Birds Data}\\) For Bumpus’s bird data, at the 5% significance level, test whether the survivors and non-survivors are different in the five measurements. Hypotheses. \\(H_0: \\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2}=\\mathbf{0}\\), \\(H_a: \\mbox{at least one } \\mu_{1i}\\ne \\mu_{2i}, i=1, 2, \\cdots, p\\). Find the sample mean vectors and covariance matrices for the survivors and non-survivors. \\[ \\mathbf{\\bar x_1}=\\left[ \\begin{array}{c} 157.381\\\\ 241.000\\\\ 31.433\\\\ 18.500\\\\ 20.810 \\end{array} \\right], \\mathbf{S_1}=\\left[ \\begin{array}{rrrrr} 11.048 &amp; 9.10&amp; 1.557&amp; 0.870&amp; 1.286\\\\ 9.100&amp; 17.50&amp; 1.910&amp; 1.310&amp; 0.880\\\\ 1.557 &amp; 1.91&amp; 0.531&amp; 0.189&amp; 0.240\\\\ 0.870&amp; 1.31&amp; 0.189&amp; 0.176&amp; 0.133\\\\ 1.286 &amp; 0.88&amp; 0.240&amp; 0.133&amp; 0.575 \\end{array} \\right] \\] \\[ \\mathbf{\\bar x_2}=\\left[ \\begin{array}{c} 158.429\\\\ 241.571\\\\ 31.479\\\\ 18.446\\\\ 20.839 \\end{array} \\right], \\mathbf{S_2}=\\left[ \\begin{array}{rrrrr} 15.069&amp; 17.190&amp; 2.243&amp; 1.746&amp; 2.931\\\\ 17.190&amp; 32.550&amp; 3.398&amp; 2.950 &amp;4.066\\\\ 2.243 &amp; 3.398&amp; 0.728 &amp;0.470 &amp;0.559\\\\ 1.746 &amp; 2.950&amp; 0.470&amp; 0.434&amp; 0.506\\\\ 2.931&amp; 4.066&amp; 0.559&amp; 0.506 &amp;1.321 \\end{array} \\right] \\] Calculate the observed value of the test statistic \\(T^2\\). The pooled covariance matrix is \\[ \\mathbf{S}_{\\mbox{pooled}}=\\left[ \\begin{array}{rrrrr} 13.358&amp; 13.747&amp; 1.951&amp; 1.373&amp; 2.231\\\\ 13.747&amp; 26.146&amp; 2.765&amp; 2.252&amp; 2.710\\\\ 1.951&amp; 2.765&amp; 0.644&amp; 0.350&amp; 0.423\\\\ 1.373&amp; 2.252&amp;0.350&amp; 0.324&amp; 0.347\\\\ 2.231&amp; 2.710&amp;0.423&amp; 0.347&amp; 1.004 \\end{array} \\right] \\] and \\[ \\left[\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\mathbf{S}_{\\mbox{pooled}}\\right]^{-1}=\\left[ \\begin{array}{rrrrr} 2.473&amp; -0.832&amp; -2.883 &amp; 0.950&amp; -2.362\\\\ -0.832&amp; 1.482&amp; -0.448&amp; -6.648&amp; 0.337\\\\ -2.883&amp; -0.448&amp; 50.793&amp; -39.335&amp; -0.190\\\\ 0.950&amp; -6.648&amp; -39.335&amp; 138.136&amp; -15.336\\\\ -2.362&amp; 0.337&amp; -0.190&amp; -15.336&amp; 21.673 \\end{array} \\right], \\] \\[ \\small \\begin{aligned} T^2&amp;=[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-(\\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2})]^{T}\\left[\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\mathbf{S}_{\\mbox{pooled}}\\right]^{-1}[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-(\\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2})]\\\\ &amp;=[-1.048, -0.571, -0.046, 0.054, -0.029]\\left[ \\begin{array}{rrrrr} 2.473&amp; -0.832&amp; -2.883 &amp; 0.950&amp; -2.362\\\\ -0.832&amp; 1.482&amp; -0.448&amp; -6.648&amp; 0.337\\\\ -2.883&amp; -0.448&amp; 50.793&amp; -39.335&amp; -0.190\\\\ 0.950&amp; -6.648&amp; -39.335&amp; 138.136&amp; -15.336\\\\ -2.362&amp; 0.337&amp; -0.190&amp; -15.336&amp; 21.673 \\end{array} \\right] \\left[ \\begin{array}{c} -1.048\\\\ -0.571\\\\ -0.046\\\\ 0.054\\\\ -0.029 \\end{array} \\right]\\\\ &amp;=2.843 \\end{aligned} \\] Since \\(F_o=\\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2=\\frac{21+28-5-1}{(21+28-2)5}\\times 2.843=0.52&lt;F_{p, n_1+n_2-p-1}(\\alpha)=F_{5, 43}(0.05)=2.432\\), there is no sufficient evidence to conclude that the survivors and non-survivors are different in the five body measurements at the 5% significance level. Use \\(\\textsf{R}\\) to confirm the result. # manual hotelling def hotellings_t2(x1, x2): &quot;&quot;&quot; Hotelling&#39;s T-squared test for two multivariate samples. Returns the T² statistic and the corresponding F-statistic and p-value. &quot;&quot;&quot; n1, p = x1.shape n2, _ = x2.shape # Mean vectors mean1 = np.mean(x1, axis=0) mean2 = np.mean(x2, axis=0) # Pooled covariance matrix s1 = np.cov(x1, rowvar=False) s2 = np.cov(x2, rowvar=False) spooled = ((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2) # T² statistic diff = mean1 - mean2 t2 = (n1 * n2) / (n1 + n2) * diff.T @ np.linalg.inv(spooled) @ diff # Convert to F-statistic f_stat = (n1 + n2 - p - 1) * t2 / (p * (n1 + n2 - 2)) df1 = p df2 = n1 + n2 - p - 1 p_value = 1 - f.cdf(f_stat, df1, df2) return { &#39;T2&#39;: t2, &#39;F&#39;: f_stat, &#39;df1&#39;: df1, &#39;df2&#39;: df2, &#39;p_value&#39;: p_value } bird = pd.read_csv(&quot;data/bumpus.txt&quot;, sep=&quot;\\t&quot;) # or sep=&quot;,&quot; depending on file xmat1 = bird.iloc[0:21, 1:].to_numpy() # survivors: exclude ID column xmat2 = bird.iloc[21:, 1:].to_numpy() # non-survivors result = hotellings_t2(xmat1, xmat2) print(f&quot;Hotelling&#39;s T²: {result[&#39;T2&#39;]:.4f}&quot;) ## Hotelling&#39;s T²: 0.0000 print(f&quot;F-statistic: {result[&#39;F&#39;]:.4f}&quot;) ## F-statistic: nan print(f&quot;Degrees of freedom: ({result[&#39;df1&#39;]}, {result[&#39;df2&#39;]})&quot;) ## Degrees of freedom: (0, 48) print(f&quot;P-value: {result[&#39;p_value&#39;]:.4f}&quot;) ## P-value: nan The admission data set contains 397 graduate school admissions decisions, among which \\(n_1=271\\) were not admitted and \\(n_2=126\\) were admitted. The explanatory variables are the student’s GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution (values from 1 to 4 with 1 having the highest prestige). The response variable, admit/don’t admit, is a binary variable with 1=admitted and 0=not admitted. Test at the 1% significance level whether the non-admitted and admitted students have different GRE and/or GPA scores. We first check the assumptions of the test. def chi_squared_qq(x, ax=None, title=&quot;&quot;): n, p = x.shape pvec = (np.arange(1, n + 1) - 0.5) / n qvec = chi2.ppf(pvec, df=p) mvec = x.mean(axis=0) smat = np.cov(x, rowvar=False) inv_smat = np.linalg.inv(smat) dvec = np.array([mahalanobis(row, mvec, inv_smat)**2 for row in x]) dvec_sorted = np.sort(dvec) if ax is None: ax = plt.gca() ax.scatter(dvec_sorted, qvec, c=&#39;black&#39;) ax.plot([0, max(dvec_sorted)], [0, max(qvec)], color=&#39;red&#39;, linewidth=2) ax.set_xlabel(&quot;Observed Quantile&quot;) ax.set_ylabel(&quot;Theoretical Quantile&quot;) ax.set_title(title) df = pd.read_csv(&quot;data/admission.csv&quot;) df = df.dropna() adf0 = df[df[&#39;admit&#39;] == 0][[&#39;gre&#39;, &#39;gpa&#39;]] # Non-admitted adf1 = df[df[&#39;admit&#39;] == 1][[&#39;gre&#39;, &#39;gpa&#39;]] # Admitted fig, axes = plt.subplots(1, 2, figsize=(12, 5)) chi_squared_qq(adf0.to_numpy(), ax=axes[0], title=&quot;Chi-square Q-Q Plot for Non-Admitted&quot;) chi_squared_qq(adf1.to_numpy(), ax=axes[1], title=&quot;Chi-square Q-Q Plot for Admitted&quot;) plt.tight_layout() plt.show() plt.close() Both chi-square Q-Q plots do not show strong evidence against the bivariate normality assumption for both the non-admitted and admitted groups. We use Box’s M-test for Homogeneity of Covariance Matrices with \\(H_0: \\boldsymbol{\\Sigma}_1=\\boldsymbol{\\Sigma}_2\\). # manual box test def box_m_test(df, group_col): groups = df[group_col].unique() k = len(groups) n_total = len(df) group_data = [df[df[group_col] == g].drop(columns=group_col).values for g in groups] ns = [len(g) for g in group_data] covs = [np.cov(g, rowvar=False) for g in group_data] pooled_cov = sum([(ns[i] - 1) * covs[i] for i in range(k)]) / (n_total - k) p = pooled_cov.shape[0] M = sum([(ns[i] - 1) * np.log(np.linalg.det(covs[i])) for i in range(k)]) - (n_total - k) * np.log(np.linalg.det(pooled_cov)) C = (1 + (1 / (3 * (k - 1))) * (sum([1 / (ns[i] - 1) for i in range(k)]) - 1 / (n_total - k))) # Correction factor chi2_stat = M * C df_box = (p * (p + 1) * (k - 1)) // 2 p_value = 1 - chi2.cdf(chi2_stat, df_box) return { &quot;Chi-squared&quot;: chi2_stat, &quot;df&quot;: df_box, &quot;p-value&quot;: p_value } subset_df = df[[&#39;gre&#39;, &#39;gpa&#39;, &#39;admit&#39;]] result = box_m_test(subset_df, group_col=&#39;admit&#39;) print(&quot;Chi-squared (approx.):&quot;, result[&quot;Chi-squared&quot;]) ## Chi-squared (approx.): -3.93145712583418 print(&quot;df:&quot;, result[&quot;df&quot;]) ## df: 3 print(&quot;p-value:&quot;, result[&quot;p-value&quot;]) ## p-value: 1.0 The P-value of the Box’s M-test is \\(0.2732\\), so there is not sufficient evidence to reject \\(H_0\\), we could use the pooled procedure. Steps: Hypotheses. \\(H_0: \\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2}=\\mathbf{0}\\), \\(H_a: \\mbox{at least one } \\mu_{1i}\\ne \\mu_{2i}, i=1, 2\\). Significance level \\(\\alpha=0.01\\). Test statistic. First find the sample mean vectors and sample covariance matrices. \\[ \\mathbf{\\bar x_1}=\\left[ \\begin{array}{c} 573.579\\\\ 3.347 \\end{array} \\right], \\mathbf{S_1}=\\left[ \\begin{array}{rrrrr} 3468.252&amp; 18.265\\\\ 18.265 &amp; 0.1412 \\end{array} \\right] \\] \\[ \\mathbf{\\bar x_2}=\\left[ \\begin{array}{c} 618.571\\\\ 3.4892 \\end{array} \\right], \\mathbf{S_2}=\\left[ \\begin{array}{rrrrr} 11937.143&amp; 9.452\\\\ 9.452&amp; 0.138 \\end{array} \\right] \\] The pooled covariance matrix is \\[ \\mathbf{S}_{\\mbox{pooled}}=\\left[ \\begin{array}{rrrrr} 12983.724&amp; 15.476\\\\ 15.476&amp; 0.141 \\end{array} \\right] \\] and \\[ \\left[\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\mathbf{S}_{\\mbox{pooled}}\\right]^{-1}=\\left[ \\begin{array}{rrrrr} 0.007625356&amp; -0.8397307\\\\ -0.839730679&amp; 704.5116072 \\end{array} \\right], \\] \\[ \\begin{aligned} T^2&amp;=[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-(\\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2})]^{T}\\left[\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)\\mathbf{S}_{\\mbox{pooled}}\\right]^{-1}[(\\mathbf{\\bar x_1}-\\mathbf{\\bar x_2})-(\\boldsymbol{\\mu_1}-\\boldsymbol{\\mu_2})]\\\\ &amp;=[-44.992, -0.142]\\left[ \\begin{array}{rrrrr} 0.007625356&amp; -0.8397307\\\\ -0.839730679&amp; 704.5116072 \\end{array} \\right] \\left[ \\begin{array}{c} -44.992\\\\ -0.142 \\end{array} \\right]\\\\ &amp;=18.918 \\end{aligned} \\] \\(F_o=\\frac{n_1+n_2-p-1}{(n_1+n_2-2)p}T^2=\\hspace{10cm}\\). P-value or rejection region. Decision: Conclusion: Double check with \\(\\textsf{R}.\\) X = adf0[[&#39;gre&#39;, &#39;gpa&#39;]].values # Non-admitted Y = adf1[[&#39;gre&#39;, &#39;gpa&#39;]].values # Admitted result = hotellings_t2(X, Y) # previous function print(&quot;T²:&quot;, result[&#39;T2&#39;], &quot;\\nF-statistic:&quot;, result[&#39;F&#39;],&quot;\\ndf1:&quot;, result[&#39;df1&#39;],&quot;\\ndf2:&quot;, result[&#39;df2&#39;],&quot;\\np-value:&quot;, result[&#39;p_value&#39;]) ## T²: 18.91775855050124 ## F-statistic: 9.434932745439859 ## df1: 2 ## df2: 394 ## p-value: 9.943930954481317e-05 Obtain 99% Bonferroni intervals for the mean difference in GRE and GPA. \\[ (\\bar x_{1i}-\\bar x_{2i}) \\pm t_{n_1+n_2-2}(\\frac{\\alpha}{2p})\\sqrt{s_{\\mbox{ ${pooled}$ },i}^2(\\frac{1}{n_1}+\\frac{1}{n_2})}, i=1, 2 \\] We can use \\(\\textsf{R}\\) to obtain a Bonferroni interval. def simultaneous_intervals_two_sample(x, y, alpha=0.05): &quot;&quot;&quot; Simultaneous confidence intervals for the difference between two mean vectors (pooled). x, y: n1 x p and n2 x p numpy arrays &quot;&quot;&quot; n1, p = x.shape n2, _ = y.shape xm = np.mean(x, axis=0) ym = np.mean(y, axis=0) xs = np.std(x, axis=0, ddof=1) ys = np.std(y, axis=0, ddof=1) sp = np.sqrt(((n1 - 1) * xs**2 + (n2 - 1) * ys**2) / (n1 + n2 - 2)) fscore = f.ppf(1 - alpha, dfn=p, dfd=n1 + n2 - p - 1) multiplier = np.sqrt(fscore * (n1 + n2 - 2) * p / (n1 + n2 - p - 1)) margin = multiplier * sp * np.sqrt(1/n1 + 1/n2) diff_mean = xm - ym lvec = diff_mean - margin uvec = diff_mean + margin return np.column_stack((lvec, uvec)) def bonferroni_intervals_two_sample(x, y, alpha=0.05): &quot;&quot;&quot; Bonferroni corrected confidence intervals for the difference between two mean vectors. Returns pooled and non-pooled versions. &quot;&quot;&quot; n1, p = x.shape n2, _ = y.shape xm = np.mean(x, axis=0) ym = np.mean(y, axis=0) xs = np.std(x, axis=0, ddof=1) ys = np.std(y, axis=0, ddof=1) # Pooled version sp = np.sqrt(((n1 - 1) * xs**2 + (n2 - 1) * ys**2) / (n1 + n2 - 2)) tscore_pool = t.ppf(1 - alpha / (2 * p), df=n1 + n2 - 2) margin_pool = tscore_pool * sp * np.sqrt(1/n1 + 1/n2) diff_mean = xm - ym lvec1 = diff_mean - margin_pool uvec1 = diff_mean + margin_pool # Non-pooled version (Welch-like) df = (xs**2 / n1 + ys**2 / n2)**2 / ( (xs**2 / n1)**2 / (n1 - 1) + (ys**2 / n2)**2 / (n2 - 1) ) tscore_nonpool = t.ppf(1 - alpha / (2 * p), df=df) margin_nonpool = tscore_nonpool * np.sqrt(xs**2 / n1 + ys**2 / n2) lvec2 = diff_mean - margin_nonpool uvec2 = diff_mean + margin_nonpool return { &#39;pooled&#39;: np.column_stack((lvec1, uvec1)), &#39;non_pooled&#39;: np.column_stack((lvec2, uvec2)) } X = adf0[[&#39;gre&#39;, &#39;gpa&#39;]].to_numpy() Y = adf1[[&#39;gre&#39;, &#39;gpa&#39;]].to_numpy() simul_ci = simultaneous_intervals_two_sample(X, Y, alpha=0.01) bonf_ci = bonferroni_intervals_two_sample(X, Y, alpha=0.01) print(&quot;Simultaneous CI (pooled):\\n&quot;, simul_ci) ## Simultaneous CI (pooled): ## [[-8.25460980e+01 -7.43808761e+00] ## [-2.65597537e-01 -1.84978178e-02]] print(&quot;Bonferroni CI (pooled):\\n&quot;, bonf_ci[&#39;pooled&#39;]) ## Bonferroni CI (pooled): ## [[-7.96752949e+01 -1.03088906e+01] ## [-2.56152811e-01 -2.79425444e-02]] print(&quot;Bonferroni CI (non-pooled):\\n&quot;, bonf_ci[&#39;non_pooled&#39;]) ## Bonferroni CI (non-pooled): ## [[-7.90205677e+01 -1.09636179e+01] ## [-2.56015917e-01 -2.80794381e-02]] 4.2.5 Univariate Case Based on a Paired Sample Two samples are considered \\(\\textit{paired}\\) if each observation in the first sample is related to an observation in the second sample. In univariate case, a paired \\(t\\) test is exactly a one-sample \\(t\\) test on the \\(\\textbf{paired differences}\\) \\(d_i\\). The hypotheses are \\(H_0: \\mu_d=\\delta_0\\) versus \\(H_a: \\mu_d\\ne \\delta_0\\). $: $ simple random paired sample; the paired difference has a normal distribution or large number of pairs. The test statistic is \\[ t_o=\\frac{\\bar d-\\delta_0}{\\frac{s_d}{\\sqrt{n}}}, \\quad \\bar d=\\frac{\\sum d_i}{n}, \\quad s_d=\\sqrt{\\frac{\\sum(d_i-\\bar d)^2}{n-1}}=\\sqrt{\\frac{\\sum d_i^2-\\frac{(\\sum d_i)^2}{n}}{n-1}}, \\quad df=n-1. \\] Reject \\(H_0: \\mu_d=\\delta_0\\) if \\(t_o\\ge t_{n-1}(\\alpha/2)\\). 4.2.6 Multivariate Case Based on a Paired Sample Generalize the univariate case to multivariate case, let the matrix of pairwise differences \\(\\mathbf{D}=\\mathbf{X}_1-\\mathbf{X}_2\\), the element-wise difference between the two observation matrices \\(\\mathbf{X}_1=[x_{1ik}]_{k=1}^{n}, i=1, 2, \\cdots, p\\) and \\(\\mathbf{X}_2=[x_{2ik}]_{k=1}^{n}, i=1, 2, \\cdots, p\\). And \\(\\bar {\\mathbf{d}}\\) is the sample mean vector of difference matrix \\(\\mathbf{D}\\), and \\(\\mathbf{S}_d\\) is the sample covariance matrix of \\(\\mathbf{D}\\). The hypotheses of the paired Hotelling’s \\(T^2\\) test is \\(H_0: \\boldsymbol{\\mu}_d=\\boldsymbol{\\delta}_0\\) versus \\(H_a: \\boldsymbol{\\mu}_d\\ne \\boldsymbol{\\delta}_0\\). The test statistic is \\[ T^2=n(\\mathbf{\\bar d}-\\boldsymbol{\\delta}_0)^T\\mathbf{S}_d^{-1}(\\mathbf{\\bar d}-\\boldsymbol{\\delta}_0) \\Longrightarrow \\frac{(n-p)}{(n-1)p}T^2\\sim F_{p, n-p}. \\] We reject \\(H_0: \\boldsymbol{\\mu}_d=\\boldsymbol{\\delta}_0\\) at significance level \\(\\alpha\\) if \\[ F_o=\\frac{(n-p)}{p(n-1)}T^2\\ge F_{p, n-p}(\\alpha) \\] where \\(F_{p, n-p}(\\alpha)\\) is the upper \\(\\alpha\\times 100\\)th percentile of the \\(F_{p, n-p}\\) distribution. $$ A sample of husband and wife pairs are asked to respond to each of the following questions: What is the level of passionate love you feel for your partner? What is the level of passionate love your partner feels for you? What is the level of companionate love you feel for your partner? What is the level of companionate love your partner feels for you? A total of 30 married couples were questioned. Responses were recorded on the five-point scale. Responses included the following values: None at all Very little Some A great deal Tremendous amount The data are summarized in the following table: ## (np.float64(-0.5), np.float64(630.5), np.float64(299.5), np.float64(-0.5)) We can conduct the test in \\(\\textsf{R}.\\) spouse = pd.read_csv(&quot;data/spouse_paired.txt&quot;, header=None, delim_whitespace=True) hmat = spouse.iloc[:, 0:4].to_numpy() # husband wmat = spouse.iloc[:, 4:8].to_numpy() # wife dmat = hmat - wmat n, p = dmat.shape mean_diff = np.mean(dmat, axis=0).reshape(p, 1) cov_diff = np.cov(dmat, rowvar=False) t2 = n * mean_diff.T @ np.linalg.inv(cov_diff) @ mean_diff f0 = (n - p) / (p * (n - 1)) * t2 f0 = f0.item() f_crit = f.ppf(0.95, dfn=p, dfd=n - p) p_value = 1 - f.cdf(f0, dfn=p, dfd=n - p) print(&quot;Hotelling&#39;s T²:&quot;, t2.item()) ## Hotelling&#39;s T²: 13.127840260936427 print(&quot;F statistic:&quot;, f0) ## F statistic: 2.942446955037475 print(&quot;Critical F-value (alpha = 0.05):&quot;, f_crit) ## Critical F-value (alpha = 0.05): 2.7425941372218587 print(&quot;p-value:&quot;, p_value) ## p-value: 0.03936914422934046 Given that \\[ \\mathbf{\\bar d}=\\left[ \\begin{array}{c} 0.0667\\\\ -0.1333\\\\ -0.3000\\\\ -0.1333\\\\ \\end{array} \\right], \\mathbf{S}_d=\\left[ \\begin{array}{rrrr} 0.8230 &amp; 0.0782&amp; -0.0138&amp; -0.0598\\\\ 0.0782&amp; 0.8092&amp; -0.2138 &amp;-0.1563\\\\ -0.0138 &amp;-0.2138&amp; 0.5621&amp; 0.5103\\\\ -0.0598&amp; -0.1563 &amp;0.5103 &amp; 0.6023\\\\ \\end{array} \\right], \\] test at the 5% significance level whether husbands respond to the questions in the same way as their wives. We can use \\[ \\begin{aligned} T^2&amp;=n(\\mathbf{\\bar d}-\\boldsymbol{\\delta}_0)^T\\mathbf{S}_d^{-1}(\\mathbf{\\bar d}-\\boldsymbol{\\delta}_0)\\\\ &amp;=30\\times [0.0667, -0.1333, -0.3000, -0.1333]\\left[ \\begin{array}{rrrrr} 1.2558 &amp; -0.1502&amp; -0.4510 &amp; 0.4678\\\\ -0.1502&amp; 1.4115&amp; 0.9279&amp; -0.4348\\\\ -0.4510&amp; 0.9279&amp; 8.4174&amp; -6.9356\\\\ 0.4678&amp; -0.4348&amp; -6.9356&amp; 7.4702\\\\ \\end{array} \\right] \\left[ \\begin{array}{c} 0.0667\\\\ -0.1333\\\\ -0.3000\\\\ -0.1333\\\\ \\end{array} \\right]\\\\ &amp;=13.123 \\end{aligned} \\] complete the test. 4.3 Hypothesis Test for Several Mean Vectors We first review how to compare several population means in the univariate case. 4.3.1 Univariate Case: One-Way ANOVA F Test We use one-way ANOVA comparing several population means in the univariate case. The hypotheses of one-way ANOVA are formulated as \\(H_0\\): all means are equal, \\(\\mu_1=\\mu_2=\\cdots=\\mu_k\\). \\(H_a\\): not all the means are equal. The test statistic is \\[ F=\\frac{\\frac{SSTR}{k-1}}{\\frac{SSE}{n-k}}\\sim F_{k-1, n-k} \\] which follows an F distribution with degrees of freedom \\(k-1\\) and \\(n-k\\) where \\(k\\) is the number of means and \\(n=n_1+n_2+\\cdots+n_k\\) is the total number of observations from all \\(k\\) populations. The variation \\(SSTR\\) and variation are calculated as \\[ SSTR=\\sum_{i=1}^k n_i(\\bar x_i-\\bar x)^2; \\quad SSE=\\sum_{i=1}^k (n_i-1)s_i^2 \\] where \\(n_i, \\bar x_i, s_i^2, i=1, 2, \\cdots, k\\) are the sample size, the sample mean and sample variance of the \\(k\\) samples from their own populations, \\(\\bar x\\) is the mean of all observations. The total variation in the response \\(SST\\) is given by the identity: \\(SST=SSTR+SSE\\). Reject \\(H_0: \\mu_1=\\mu_2=\\cdots=\\mu_k\\) at significance level \\(\\alpha\\) if the observed F score is larger than \\(F_{k-1, n-k}(\\alpha)\\). 4.3.2 Multivariate Case: One-Way MANOVA The idea for one-way ANOVA can be generalized to the multivariate case by replacing the sample mean \\(\\bar x_i\\) with mean vector \\(\\mathbf{\\bar x_i}\\) and the sample variance \\(s_i^2\\) with the covariance matrix \\(\\mathbf{S_i}\\). The between-treatment variation is given by \\[ \\mathbf{B}=\\sum_{i=1}^kn_i (\\mathbf{\\bar x_i}-\\mathbf{\\bar x})(\\mathbf{\\bar x_i}-\\mathbf{\\bar x})^{T} \\] and the within-treatment variation is \\[ \\mathbf{W}=\\sum_{i=1}^k (n_i-1) \\mathbf{S_i} \\] Reject \\(H_0: \\boldsymbol{\\mu_1}=\\boldsymbol{\\mu_2}=\\cdots=\\boldsymbol{\\mu_k}\\) if \\[ \\Lambda^{\\ast}=\\frac{|\\mathbf{W}|}{|\\mathbf{B}+\\mathbf{W}|} \\] is too . The quantity \\(\\Lambda^{\\ast}\\) was originally proposed by Wilks and has distribution as follows: \\[ \\begin{array}{ccc} \\hline \\text{No. of variables}&amp;\\text{No. of groups}&amp;\\text{Sampling distribution of $\\Lambda^{\\ast}$}\\\\ \\hline p=1&amp;k\\ge2&amp;\\left(\\frac{n-k}{k-1}\\right)\\left(\\frac{1-\\Lambda^{\\ast}}{\\Lambda^{\\ast}}\\right)\\sim F_{k-1, n-k}\\\\ p=2&amp;k\\ge2&amp;\\left(\\frac{n-k-1}{k-1}\\right)\\left(\\frac{1-\\sqrt{\\Lambda^{\\ast}}}{\\sqrt{\\Lambda^{\\ast}}}\\right)\\sim F_{2(k-1), 2(n-k-1)}\\\\ p\\ge1&amp;k=2&amp;\\left(\\frac{n-p-1}{p}\\right)\\left(\\frac{1-\\Lambda^{\\ast}}{\\Lambda^{\\ast}}\\right)\\sim F_{p, n-p-1}\\\\ p\\ge 1&amp;$k=3&amp;\\left(\\frac{n-p-2}{p}\\right)\\left(\\frac{1-\\sqrt{\\Lambda^{\\ast}}}{\\sqrt{\\Lambda^{\\ast}}}\\right)\\sim F_{2p, 2(n-p-2)}\\\\ \\hline \\end{array} \\] If \\(n\\) is large, Barlett has shown that if \\(H_0\\) is true, the quantity \\[ -\\left(n-1-\\frac{p+k}{2}\\right)\\ln{\\Lambda^{\\ast}} \\] has approximately a chi-square distribution with \\(df=p(k-1)\\). Therefore, we can reject \\(H_0\\) at significance level \\(\\alpha\\) if \\[ -\\left(n-1-\\frac{p+k}{2}\\right)\\ln\\left(\\frac{|\\mathbf{W}|}{|\\mathbf{B}+\\mathbf{W}|}\\right)\\ge \\chi^2_{p(k-1)}(\\alpha) \\] This is called the one-way MANOVA (multivariate analysis of variance). Suppose there are three groups, \\(n_1=3, n_2=2, n_3=3\\) and the data matrices are \\[ \\mathbf{X_1}=\\left[ \\begin{array}{cc} 9 &amp; 3\\\\ 6 &amp; 2\\\\ 9 &amp; 7 \\end{array} \\right]; \\quad \\mathbf{X_2}=\\left[ \\begin{array}{cc} 0 &amp; 4\\\\ 2 &amp; 0\\\\ \\end{array} \\right]; \\quad \\mathbf{X_3}=\\left[ \\begin{array}{cc} 3 &amp; 8\\\\ 1 &amp; 9\\\\ 2 &amp; 7 \\end{array} \\right] \\mbox{ with } \\mathbf{\\bar x}=\\left[ \\begin{array}{c} 4\\\\ 5 \\end{array} \\right]; \\quad \\mathbf{\\bar x_1}=\\left[ \\begin{array}{c} 8\\\\ 4 \\end{array} \\right]; \\quad \\mathbf{\\bar x_2}=\\left[ \\begin{array}{c} 1\\\\ 2 \\end{array} \\right]; \\quad \\mathbf{\\bar x_3}=\\left[ \\begin{array}{c} 2\\\\ 8 \\end{array} \\right] \\] The between-treatment variation is \\[ \\mathbf{B}=\\sum_{i=1}^kn_i (\\mathbf{\\bar x_i}-\\mathbf{\\bar x})(\\mathbf{\\bar x_i}-\\mathbf{\\bar x})^{T} =3\\left[ \\begin{array}{c} 4\\\\ -1 \\end{array} \\right][4, -1]+2\\left[ \\begin{array}{c} -3\\\\ -3 \\end{array} \\right][-3, -3]+3\\left[ \\begin{array}{c} -2\\\\ 3 \\end{array} \\right][-2, 3] =\\left[ \\begin{array}{cc} 78 &amp; -12\\\\ -12 &amp; 48\\\\ \\end{array} \\right] \\] And then the within-treatment variation is \\[ \\mathbf{W}=\\sum_{i=1}^k (n_i-1) \\mathbf{S_i}=(3-1)\\left[ \\begin{array}{cc} 3 &amp; 3\\\\ 3 &amp; 7\\\\ \\end{array} \\right]+(2-1)\\left[ \\begin{array}{cc} 2 &amp; -4\\\\ -4 &amp; 8\\\\ \\end{array} \\right]+(3-1)\\left[ \\begin{array}{cc} 1 &amp; -0.5\\\\ -0.5 &amp; 1\\\\ \\end{array} \\right]=\\left[ \\begin{array}{cc} 10 &amp; 1\\\\ 1 &amp; 24\\\\ \\end{array} \\right] \\] Therefore, \\[ \\Lambda^{\\ast}=\\frac{|\\mathbf{W}|}{|\\mathbf{B}+\\mathbf{W}|}=\\frac{\\left| \\begin{array}{cc} 10 &amp; 1\\\\ 1 &amp; 24\\\\ \\end{array} \\right|}{\\left| \\begin{array}{cc} 88 &amp; -11\\\\ -11 &amp; 72\\\\ \\end{array} \\right|}=\\frac{239}{6215}=0.0385 \\] Since \\(p=2, k=3\\) and \\(n=8\\) which is small, the distribution of \\(\\Lambda^{\\ast}\\) is \\[ \\left(\\frac{n-k-1}{k-1}\\right)\\left(\\frac{1-\\sqrt{\\Lambda^{\\ast}}}{\\sqrt{\\Lambda^{\\ast}}}\\right)=\\left(\\frac{8-3-1}{3-1}\\right)\\left(\\frac{1-\\sqrt{0.0385}}{\\sqrt{0.0385}}\\right)=8.199 \\] Compared with \\(F_{2(k-1), 2(n-k-1)}(0.05)=F_{2(3-1), 2(8-3-1)}(0.05)=F_{4, 8}(0.05)=3.838\\). Since \\(8.199&gt;F_{4, 8}(0.05)=3.838\\), we can reject \\(H_0: \\mu_1=\\mu_2=\\mu_3\\) at the 5% significance level. Double check with . from statsmodels.multivariate.manova import MANOVA x1 = np.array([[9, 3], [6, 2], [9, 7]]) x2 = np.array([[0, 4], [2, 0]]) x3 = np.array([[3, 8], [1, 9], [2, 7]]) x = np.vstack([x1, x2, x3]) groups = np.array([1]*3 + [2]*2 + [3]*3) df = pd.DataFrame(x, columns=[&#39;V1&#39;, &#39;V2&#39;]) df[&#39;group&#39;] = groups.astype(str) # categorical variable as string manova = MANOVA.from_formula(&#39;V1 + V2 ~ group&#39;, data=df) print(manova.mv_test()) # gives wilks test ## /Users/ruskin/r-python-env/lib/python3.13/site-packages/statsmodels/multivariate/multivariate_ols.py:216: RuntimeWarning: divide by zero encountered in scalar divide ## b = (p + 2*n) * (q + 2*n) / 2 / (2*n + 1) / (n - 1) ## /Users/ruskin/r-python-env/lib/python3.13/site-packages/statsmodels/multivariate/multivariate_ols.py:216: RuntimeWarning: divide by zero encountered in scalar divide ## b = (p + 2*n) * (q + 2*n) / 2 / (2*n + 1) / (n - 1) ## Multivariate linear model ## ============================================================ ## ## ------------------------------------------------------------ ## Intercept Value Num DF Den DF F Value Pr &gt; F ## ------------------------------------------------------------ ## Wilks&#39; lambda 0.0465 2.0000 4.0000 40.9707 0.0022 ## Pillai&#39;s trace 0.9535 2.0000 4.0000 40.9707 0.0022 ## Hotelling-Lawley trace 20.4854 2.0000 4.0000 40.9707 0.0022 ## Roy&#39;s greatest root 20.4854 2.0000 4.0000 40.9707 0.0022 ## ------------------------------------------------------------ ## ## ------------------------------------------------------------ ## group Value Num DF Den DF F Value Pr &gt; F ## ------------------------------------------------------------ ## Wilks&#39; lambda 0.0385 4.0000 8.0000 8.1989 0.0062 ## Pillai&#39;s trace 1.5408 4.0000 10.0000 8.3882 0.0031 ## Hotelling-Lawley trace 9.9414 4.0000 4.0000 9.9414 0.0235 ## Roy&#39;s greatest root 8.0764 2.0000 5.0000 20.1910 0.0040 ## ============================================================ We can also apply Wilks’ lambda test on the Iris flowers data to see whether the three species have different mean values on the measurements. I will use the two variables and to make \\(p=2\\) in the notes. The sample mean vectors for the three species and the overall mean vector are: \\[ \\mathbf{\\bar x_1}=\\left[ \\begin{array}{c} 1.462\\\\ 0.246 \\end{array} \\right]; \\quad \\mathbf{\\bar x_2}=\\left[ \\begin{array}{c} 4.260\\\\ 1.326 \\end{array} \\right]; \\quad \\mathbf{\\bar x_3}=\\left[ \\begin{array}{c} 5.552\\\\ 2.026 \\end{array} \\right]; \\quad \\mathbf{\\bar x}=\\left[ \\begin{array}{c} 3.758\\\\ 1.199 \\end{array} \\right] \\] The between and within treatment variations are \\[ \\mathbf{B}=\\left[ \\begin{array}{cc} 437.103 &amp; 186.774\\\\ 186.774 &amp; 80.413\\\\ \\end{array} \\right]; \\quad \\mathbf{W}=\\left[ \\begin{array}{cc} 27.223 &amp; 6.272\\\\ 6.272 &amp; 6.157\\\\ \\end{array} \\right] \\] which gives \\(\\Lambda^{\\ast}=0.0438\\). The distribution of \\(\\Lambda^{\\ast}\\) is \\[ \\left(\\frac{n-k-1}{k-1}\\right)\\left(\\frac{1-\\sqrt{\\Lambda^{\\ast}}}{\\sqrt{\\Lambda^{\\ast}}}\\right)=\\left(\\frac{150-3-1}{3-1}\\right)\\left(\\frac{1-\\sqrt{0.0438}}{\\sqrt{0.0438}}\\right)=275.9 \\] Compared with \\(F_{2(k-1), 2(n-k-1)}(0.05)=F_{2(3-1), 2(150-3-1)}(0.05)=F_{4, 292}(0.05)=2.403\\). We can reject \\(H_0\\). Since \\(n=n_1+n_2+n_3=3\\times 50=150\\) is relatively large, we can use the chi-square approximation, \\[ -\\left(n-1-\\frac{p+k}{2}\\right)\\ln\\left(\\frac{|\\mathbf{W}|}{|\\mathbf{B}+\\mathbf{W}|}\\right)=458.348 \\] compared with \\(\\chi^2_{p(k-1)}(0.05)=\\chi^2_4(0.05)=9.488\\), reject \\(H_0\\). Confirm with . iris = load_iris(as_frame=True) df = iris.frame df[&#39;species&#39;] = iris.target_names[iris.target] df_subset = df[[&#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;, &#39;species&#39;]] df_subset.columns = [&#39;PetalLength&#39;, &#39;PetalWidth&#39;, &#39;Species&#39;] manova = MANOVA.from_formula(&#39;PetalLength + PetalWidth ~ Species&#39;, data=df_subset) print(manova.mv_test()) ## Multivariate linear model ## ================================================================ ## ## ---------------------------------------------------------------- ## Intercept Value Num DF Den DF F Value Pr &gt; F ## ---------------------------------------------------------------- ## Wilks&#39; lambda 0.1995 2.0000 146.0000 292.9791 0.0000 ## Pillai&#39;s trace 0.8005 2.0000 146.0000 292.9791 0.0000 ## Hotelling-Lawley trace 4.0134 2.0000 146.0000 292.9791 0.0000 ## Roy&#39;s greatest root 4.0134 2.0000 146.0000 292.9791 0.0000 ## ---------------------------------------------------------------- ## ## ---------------------------------------------------------------- ## Species Value Num DF Den DF F Value Pr &gt; F ## ---------------------------------------------------------------- ## Wilks&#39; lambda 0.0438 4.0000 292.0000 275.9001 0.0000 ## Pillai&#39;s trace 1.0465 4.0000 294.0000 80.6612 0.0000 ## Hotelling-Lawley trace 19.7821 4.0000 174.1653 720.4267 0.0000 ## Roy&#39;s greatest root 19.6773 2.0000 147.0000 1446.2819 0.0000 ## ================================================================ If we use all four predicted variables, the result becomes: iris = load_iris(as_frame=True) df = iris.frame.copy() df[&#39;Species&#39;] = iris.target_names[iris.target] df.rename(columns={ &#39;sepal length (cm)&#39;: &#39;SepalLength&#39;, &#39;sepal width (cm)&#39;: &#39;SepalWidth&#39;, &#39;petal length (cm)&#39;: &#39;PetalLength&#39;, &#39;petal width (cm)&#39;: &#39;PetalWidth&#39; }, inplace=True) manova = MANOVA.from_formula(&#39;SepalLength + SepalWidth + PetalLength + PetalWidth ~ Species&#39;, data=df) print(manova.mv_test()) ## Multivariate linear model ## ================================================================ ## ## ---------------------------------------------------------------- ## Intercept Value Num DF Den DF F Value Pr &gt; F ## ---------------------------------------------------------------- ## Wilks&#39; lambda 0.0170 4.0000 144.0000 2086.7720 0.0000 ## Pillai&#39;s trace 0.9830 4.0000 144.0000 2086.7720 0.0000 ## Hotelling-Lawley trace 57.9659 4.0000 144.0000 2086.7720 0.0000 ## Roy&#39;s greatest root 57.9659 4.0000 144.0000 2086.7720 0.0000 ## ---------------------------------------------------------------- ## ## ---------------------------------------------------------------- ## Species Value Num DF Den DF F Value Pr &gt; F ## ---------------------------------------------------------------- ## Wilks&#39; lambda 0.0234 8.0000 288.0000 199.1453 0.0000 ## Pillai&#39;s trace 1.1919 8.0000 290.0000 53.4665 0.0000 ## Hotelling-Lawley trace 32.4773 8.0000 203.4024 582.1970 0.0000 ## Roy&#39;s greatest root 32.1919 4.0000 145.0000 1166.9574 0.0000 ## ================================================================ Revisit the Learning Outcomes After finishing this note, students should be able to Explain the main idea of using a chi-square Q-Q plot to assess the multivariate normality assumption. Assess the multivariate normality through a chi-square Q-Q plot and casual procedures using R. Conduct a Hotelling’s \\(T^2\\) test on one mean vector based on one sample or paired sample. Conduct a Hotelling’s \\(T^2\\) test on two mean vectors based on two independent samples. Conduct a one-way MANOVA test on at least two mean vectors based on at least two independent samples. Obtain \\((1-\\alpha)\\times 100\\%\\) Bonferroni confidence intervals associated with a certain test if applicable. "],["principal-component-analysis.html", "5 Principal Component Analysis Learning Outcomes 5.1 Finding the Principal Components 5.2 Scaling in Principal Component Analysis 5.3 Limitations of Principal Component Analysis 5.4 Further Reading Revisit the Learning Outcomes", " 5 Principal Component Analysis A principal component analysis is to capture the variance-covariance structure of the original \\(p\\) variables using \\(k\\) uncorrelated linear combinations of those variables such that \\(k\\le p\\). Each of those \\(k\\) linear combination is called a \\(\\textit{principal component}.\\) The advantages of conducting a principal component analysis are: With fewer variables the principle components, it is easier to interpret and present the data graphically. The model fitted using the uncorrelated principal components might result in a better model. Learning Outcomes After finishing this note, students should be able to Explain the advantages and limitations of principal components analysis. Prove the two important results for principal component analysis: The \\(i\\)th principal component is given by \\[ Y_i=\\mathbf{e}_i^{T}\\mathbf{X}=e_{i1}X_1+e_{i2}X_2+\\cdots+e_{ip}X_p, i=1, 2, \\cdots, p \\] where \\(\\mathbf{e}_i\\) is the unit eigenvector corresponding to the \\(i\\)th largest eigenvalue of \\(\\mathbf{\\Sigma}\\). The total variance \\[ \\sigma_{11}+\\sigma_{22}+\\cdots+\\sigma_{pp}=\\sum_{i=1}^p Var(X_i)=\\lambda_1+\\lambda_2+\\cdots+\\lambda_p=\\sum_{i=1}^p Var(Y_i) \\] Determine the number of principal components, \\(k\\), needed to capture a given percentage of variation of the data. And find the first \\(k\\) corresponding principal components. 5.1 Finding the Principal Components Algebraically, principal components are linear combinations of the \\(p\\) random variables \\(X_1, X_2, \\cdots, X_p\\). Geometrically, these linear combinations represent the selection of a new coordinate system obtained by rotating the original system using \\(X_1, X_2, \\cdots, X_p\\) as the coordinate axes. The new axes represent the directions with maximum variability and provide a simpler description of the covariance structure. Let the random vector \\(\\mathbf{X}=[X_1, X_2, \\cdots, X_p]^{T}\\) have the covariance matrix \\(\\mathbf{\\Sigma}\\) with eigenvalues \\(\\lambda_1\\ge \\lambda_2\\ge \\cdots \\ge \\lambda_p\\ge0\\). Consider the following linear combinations: \\[ \\begin{aligned} Y_1&amp;=\\mathbf{a}_1^{T}\\mathbf{X}=a_{11}X_1+a_{12}X_2+\\cdots+a_{1p}X_p\\\\ Y_2&amp;=\\mathbf{a}_2^{T}\\mathbf{X}=a_{21}X_1+a_{22}X_2+\\cdots+a_{2p}X_p\\\\ \\vdots\\\\ Y_p&amp;=\\mathbf{a}_p^{T}\\mathbf{X}=a_{p1}X_1+a_{p2}X_2+\\cdots+a_{pp}X_p \\end{aligned} \\] We have \\[ \\begin{aligned} Var(Y_i)&amp;=Cov(\\mathbf{a}_i^{T}\\mathbf{X}, \\mathbf{a}_i^{T}\\mathbf{X})=\\mathbf{a}_i^{T}Cov(\\mathbf{X})\\mathbf{a}_i=\\mathbf{a}_i^{T}\\mathbf{\\Sigma}\\mathbf{a}_i, i=1, 2, \\cdots, p\\\\ Cov(Y_i, Y_j)&amp;=Cov(\\mathbf{a}_i^{T}\\mathbf{X}, \\mathbf{a}_j^{T}\\mathbf{X})=\\mathbf{a}_i^{T}\\mathbf{\\Sigma}\\mathbf{a}_j, i\\ne j \\end{aligned} \\] The principal components are those uncorrelated linear combinations \\(Y_1, Y_2, \\cdots, Y_p\\) whose variances are maximized. The first principal component is the linear combination with the largest variance, that is to find the vector \\(\\mathbf{a}_1\\) such that \\(Var(Y_1)=\\mathbf{a}_1^{T}\\mathbf{\\Sigma}\\mathbf{a}_1\\) is maximized. Note that \\(Var(Y_1)\\) will increase if \\(\\mathbf{a}_1\\) is multiplied by some constant. To fix this problem, we constrain \\(\\mathbf{a}_1\\) to have unit length, that is \\(\\mathbf{a}_1^{T}\\mathbf{a}_1=1\\). The second principal component is the linear combination that maximizes \\(Var(Y_2)=\\mathbf{a}_2^{T}\\mathbf{\\Sigma}\\mathbf{a}_2\\) subject to \\(\\mathbf{a}_2^{T}\\mathbf{a}_2=1\\) and \\(Cov(\\mathbf{a}_1^{T}\\mathbf{X}, \\mathbf{a}_2^{T}\\mathbf{X})=0\\). In general, the \\(i\\)th principal component is the linear combination \\(\\mathbf{a}_i^{T}\\mathbf{X}\\) that maximizes \\(Var(Y_i)=\\mathbf{a}_i^{T}\\mathbf{\\Sigma}\\mathbf{a}_i\\) subject to \\(\\mathbf{a}_i^{T}\\mathbf{a}_i=1\\) and \\(Cov(\\mathbf{a}_i^{T}\\mathbf{X}, \\mathbf{a}_j^{T}\\mathbf{X})=0\\) for all \\(j&lt;i\\). Here are three important results for principal component analysis: The \\(i\\)th principal component is given by \\[ Y_i=\\mathbf{e}_i^{T}\\mathbf{X}=e_{i1}X_1+e_{i2}X_2+\\cdots+e_{ip}X_p, i=1, 2, \\cdots, p \\] where \\(\\mathbf{e}_i\\) is the unit eigenvector corresponding to the \\(i\\)th largest eigenvalue of \\(\\mathbf{\\Sigma}\\). The total variance \\[ \\sigma_{11}+\\sigma_{22}+\\cdots+\\sigma_{pp}=\\sum_{i=1}^p Var(X_i)=\\lambda_1+\\lambda_2+\\cdots+\\lambda_p=\\sum_{i=1}^p Var(Y_i) \\] The proportion of total variance explained by the \\(j\\)th principal component is \\[ \\frac{\\lambda_j}{\\lambda_1+\\lambda_2+\\cdots+\\lambda_p}, j=1, 2, \\cdots, p \\] The following facts are useful for the proof of the first result. Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be \\(p\\times 1\\) vectors and consider their inner product \\(c=\\mathbf{u}^{T}\\mathbf{v}\\). When we differentiate \\(c\\) with respect to each \\(u_j\\) in turn, we get \\[ \\frac{\\partial c}{\\partial \\mathbf{u}}=\\left[ \\begin{array}{c} \\frac{\\partial c}{\\partial u_1}\\\\ \\vdots\\\\ \\frac{\\partial c}{\\partial u_p} \\end{array} \\right]=\\left[ \\begin{array}{c} v_1\\\\ \\vdots\\\\ v_p \\end{array} \\right]=\\mathbf{v} \\] Similarly, \\(\\frac{\\partial c}{\\partial \\mathbf{v}}=\\mathbf{u}\\). Let \\(q=\\mathbf{u}^{T}\\mathbf{\\Sigma}\\mathbf{u}\\), then differentiate \\(q\\) with respect to \\(u_j\\) in turn, we have \\[ \\frac{\\partial q}{\\partial \\mathbf{u}}=2\\mathbf{\\Sigma}\\mathbf{u} \\] \\(\\textbf{Proofs (Exercises)}\\) Show that the first principal component is the unit eigenvector corresponding to the largest eigenvalue of the covariance matrix \\(\\mathbf{\\Sigma}\\). We need to solve an optimization problem with constraints. That is to find the maximum of \\(\\mathbf{a}_1^{T}\\mathbf{\\Sigma}\\mathbf{a}_1\\) subject to \\(\\mathbf{a}_1^{T}\\mathbf{a}_1=1\\). A \\(\\gamma\\) is introduced to deal with the constraint. The second principal component is the linear combination of \\(X_i\\) with the second largest variation and orthogonal to the first principal component. Show that the second principal component is the unit eigenvector corresponding to the second largest eigenvalue of the covariance matrix \\(\\mathbf{\\Sigma}\\). Given the fact that \\(tr(\\mathbf{AB})=tr(\\mathbf{BA})\\), show that the total variance \\[ \\sigma_{11}+\\sigma_{22}+\\cdots+\\sigma_{pp}=\\sum_{i=1}^p Var(X_i)=\\lambda_1+\\lambda_2+\\cdots+\\lambda_p=\\sum_{i=1}^p Var(Y_i). \\] 5.2 Scaling in Principal Component Analysis A major problem with principal component analysis is that it is not scale invariant. That means multiplying one variable by a common constant might dramatically change the resulting principal components. The variable measured on a large scale with greater variance will dominate the principal components. Unless the variables have the same units and similar ranges, principal component analysis is usually carried out on the correlation matrix rather than the covariance matrix. Using the correlation matrix is equivalent to rescaling the variables to have mean 0 and standard deviation 1 before computing the covariance matrix. 5.3 Limitations of Principal Component Analysis Some limitations of principle component analysis are as follows: The new variables \\(Y_i\\) are combinations of the original variables and hence can only capture the linear pattern. Principal component analysis is helpful in reducing the dimension of the data if the variables are correlated; otherwise, it does nothing except for ordering the variables according to their variance. Compared to the original variables \\(X_1, X_2, \\cdots, X_p\\), the new variables might be hard to interpret. \\(\\textbf{Example}: \\text{Variation of Principle Components}\\) Suppose \\[ \\mathbf{X}=[X_1, X_2]^{T}\\sim \\mbox{MVN}\\left(\\left[ \\begin{array}{c} 0\\\\ 0 \\end{array} \\right], \\left[ \\begin{array}{cc} 1&amp;0.75\\\\ 0.75&amp;1 \\end{array} \\right]\\right), \\] find the principal components of \\(\\mathbf{X}\\). It can be shown that the eigenvalues and corresponding unit eigenvectors are: \\[ \\lambda_1=1.75, \\mathbf{e}_1=\\left[ \\begin{array}{r} \\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}} \\end{array} \\right]=\\left[ \\begin{array}{r} 0.707\\\\0.707 \\end{array} \\right]; \\quad \\lambda_2=0.25, \\mathbf{e}_2=\\left[ \\begin{array}{r} -\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}} \\end{array} \\right]=\\left[ \\begin{array}{r} -0.707\\\\0.707 \\end{array} \\right] \\] Therefore, the first principal component (PC) is the unit eigenvector corresponding to the largest eigenvalue (\\(\\lambda_1=1.75\\)), we have \\[ Y_1=0.707X_1+0.707X_2 \\] and the second principal component is \\[ Y_2=-0.707X_1+0.707X_2 \\] The first PC accounts for \\(\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}=\\frac{1.75}{2}=87.5\\%\\) of the total variance. \\(\\textbf{Example}: \\text{PCA of Birds Data}\\) Recall the birds data. Let’s first take a look of the correlation matrix of the five measurements. [ ] Figure @ref{fig:corplot} shows the correlation plot of the five measurements of the birds data. It seems that the measurements are moderately correlated. import pandas as pd import seaborn as sns import matplotlib.pyplot as plt bird = pd.read_csv(&quot;data/bumpus.txt&quot;, sep=r&#39;\\s+&#39;) plt.figure(figsize=(8, 6)) sns.heatmap(bird.drop(&#39;ID&#39;, axis=1).corr(), annot=True, cmap=&quot;GnBu&quot;, center=0, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .8}) plt.title(&quot;Correlation Matrix of Birds Data&quot;) plt.tight_layout() plt.show() Figure 5.1: Correlation Plot of Birds Data plt.close() The eigenvalues and unit eigenvectors of the \\(\\textbf{correlation matrix}\\) are given by \\[ \\begin{array}{c|rrrrr} \\hline \\text{Eigenvalue}&amp;3.616&amp; 0.532&amp; 0.386&amp; 0.302&amp; 0.165\\\\ \\hline \\text{ Component}&amp;\\mbox{PC}_1 &amp; \\mbox{PC}_2 &amp; \\mbox{PC}_3 &amp; \\mbox{PC}_4 &amp; \\mbox{PC}_5\\\\ \\hline X_1 &amp; 0.452&amp; -0.051&amp; 0.690&amp; -0.420&amp; 0.374\\\\ X_2 &amp;0.462&amp; 0.300&amp; 0.341&amp; 0.548&amp; -0.530\\\\ X_3&amp; 0.451&amp; 0.325&amp; -0.454&amp; -0.606&amp; -0.343\\\\ X_4&amp; 0.471&amp; 0.185&amp; -0.411&amp; 0.388 &amp; 0.652\\\\ X_5&amp; 0.398&amp; -0.876&amp; -0.178&amp; 0.069&amp; -0.192\\\\ \\hline \\end{array} \\] The sum of the eigenvalues is 5 and the proportion of variation explained by the first principal component is \\(\\frac{3.616}{5}=0.723\\). The first PC is the eigenvector corresponding to the largest eigenvalue \\(\\lambda_1=3.616\\), \\[ \\begin{aligned} Y_1&amp;=0.452X_1+0.462X_2+0.451X_3+0.471X_4+0.398X_5\\\\ &amp;\\approx \\mbox{constant}\\times \\frac{1}{5}(X_1+X_2+X_3+X_4+X_5)\\\\ &amp;=\\mbox{constant}\\times \\mbox{average of the $X$&#39;s} \\end{aligned} \\] The coefficients of the variables are nearly equal and about 72.3% of the variation in the data due to the overall size of the birds. This also explains why we could not find a subspace with one or two variables that distinguishes the survivors and non-survivors. Figure @ref{fig:birdpc} is the index plot of the two groups of birds using the first two PCs as the axes. from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA # dataset bird = pd.read_csv(&quot;data/bumpus.txt&quot;, sep=r&quot;\\s+&quot;) print(bird[:2]) ## ID X1 X2 X3 X4 X5 ## 0 1 156 245 31.6 18.5 20.5 ## 1 2 154 240 30.4 17.9 19.6 bird.iloc[:, 1:] = bird.iloc[:, 1:].apply(pd.to_numeric, errors=&quot;coerce&quot;) # standardizing X = bird.iloc[:, 1:] X_scaled = StandardScaler().fit_transform(X) pca = PCA(n_components=2) pc = pca.fit_transform(X_scaled) plt.figure(figsize=(8, 5)) plt.scatter(pc[:21, 0], pc[:21, 1], marker=&#39;o&#39;) # survivors plt.scatter(pc[21:, 0], pc[21:, 1], marker=&#39;+&#39;) # non-survivors plt.xlabel(&quot;First PC&quot;) plt.ylabel(&quot;Second PC&quot;) plt.title(&quot;PCA Index Scatter Plot of Birds Data&quot;) plt.tight_layout() plt.show() Figure 5.2: Index scatter plot of the birds data on the first two principal components. Black circle: survivors; red plus: non-survivors \\(\\textbf{Example}: \\text{PCA of Iris Data}\\) Iris flowers data. Four measurements, the correlation matrix is given \\[ \\begin{array}{c|rrrr} \\hline &amp; \\text{Sepal.Length}&amp; \\text{Sepal.Width}&amp; \\text{Petal.Length} &amp;\\text{Petal.Width}\\\\ \\hline \\text{Sepal.Length} &amp; 1.000 &amp; -0.118&amp; 0.872 &amp; 0.818\\\\ \\text{Sepal.Width} &amp; -0.118 &amp; 1.000 &amp; -0.428 &amp; -0.366\\\\ \\text{Petal.Length} &amp; 0.872 &amp; -0.428 &amp; 1.000 &amp; 0.963\\\\ \\text{Petal.Width} &amp; 0.818 &amp; -0.366 &amp; 0.963 &amp; 1.000\\\\ \\hline \\end{array} \\] We can tell that Petal.Length and Petal.Width are highly correlated, the correlation between Sepal.Length and Petal.Length is also quite strong; correlation between Septal.Length and Septal.Width is weak, and between Sepal.Length and Petal.Width is moderate. The eigenvalues and unit eigenvectors of the are given by \\[ \\begin{array}{c|rrrrr} \\hline \\text{Eigenvalue}&amp;2.918&amp; 0.914&amp; 0.147&amp; 0.021\\\\ \\hline \\text{Components}&amp; PC1 &amp; PC2&amp; PC3 &amp; PC4\\\\ \\hline \\text{Sepal.Length}&amp; 0.521&amp; -0.377&amp; 0.720&amp; 0.261\\\\ \\text{Sepal.Width} &amp; -0.269 &amp;-0.923 &amp;-0.244 &amp;-0.124\\\\ \\text{Petal.Length}&amp; 0.580 &amp;-0.024 &amp;-0.142 &amp;-0.801\\\\ \\text{Petal.Width} &amp; 0.565 &amp;-0.067 &amp;-0.634 &amp; 0.524\\\\ \\hline \\end{array} \\] The largest eigenvalue is \\(\\lambda_1=2.918\\) and the first PC is \\[ Y_1=0.521Sepal.Length-0.269Sepal.Width+0.580Petal.Length+0.565Petal.Width \\] Compared to other variables, the magnitude of Sepal.Width is relatively small and hence contributes less in the first PC. Moreover, the sign of its coefficient is opposite of the other three variables. The first PC accounts for \\(\\frac{2.918}{2.918+0.914+0.147+0.021}=72.95\\%\\) total variation, the second PC accounts for \\(\\frac{0.914}{2.918+0.914+0.147+0.021}=22.85\\%\\), the third PC explains \\(\\frac{0.147}{2.918+0.914+0.147+0.021}=3.675\\%\\), and the fourth PC explains \\(\\frac{0.021}{2.918+0.914+0.147+0.021}=0.525\\%\\). The first two PCs together explain 95.81% of the total variation. Therefore, we can reduce the dimensionality from 4 to 2. In order to determine the number of PCs required to capture most of the data variation, we can plot the cumulative variance versus the number of PCs (see right panel of Figure \\(\\ref{fig:pcnumber}\\)), and the number of PCs should be the one from which the curve becomes flat. From the right panel of Figure \\(\\ref{fig:pcnumber}\\), we need the first two PCs in order to capture at least 80% of the total variation of the data; however, we only need the first PC in order to capture at least 70% of the total variation. from sklearn.datasets import load_iris iris = load_iris(as_frame=True) X = StandardScaler().fit_transform(iris.data) # PCA pca = PCA() pca.fit(X) #sk-container-id-1 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: #000; --sklearn-color-text-muted: #666; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-1 { color: var(--sklearn-color-text); } #sk-container-id-1 pre { padding: 0; } #sk-container-id-1 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-1 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-1 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-1 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-1 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-1 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-1 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-1 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-1 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-1 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-1 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-1 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-1 label.sk-toggleable__label { cursor: pointer; display: flex; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; align-items: start; justify-content: space-between; gap: 0.5em; } #sk-container-id-1 label.sk-toggleable__label .caption { font-size: 0.6rem; font-weight: lighter; color: var(--sklearn-color-text-muted); } #sk-container-id-1 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"▸\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-1 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-1 div.sk-toggleable__content { display: none; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-1 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ display: block; width: 100%; overflow: visible; } #sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"▾\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-1 div.sk-label label.sk-toggleable__label, #sk-container-id-1 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-1 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-1 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-1 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-1 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-1 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 0.5em; text-align: center; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `` HTML tag */ #sk-container-id-1 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-1 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-1 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-1 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } .estimator-table summary { padding: .5rem; font-family: monospace; cursor: pointer; } .estimator-table details[open] { padding-left: 0.1rem; padding-right: 0.1rem; padding-bottom: 0.3rem; } .estimator-table .parameters-table { margin-left: auto !important; margin-right: auto !important; } .estimator-table .parameters-table tr:nth-child(odd) { background-color: #fff; } .estimator-table .parameters-table tr:nth-child(even) { background-color: #f6f6f6; } .estimator-table .parameters-table tr:hover { background-color: #e0e0e0; } .estimator-table table td { border: 1px solid rgba(106, 105, 104, 0.232); } .user-set td { color:rgb(255, 94, 0); text-align: left; } .user-set td.value pre { color:rgb(255, 94, 0) !important; background-color: transparent !important; } .default td { color: black; text-align: left; } .user-set td i, .default td i { color: black; } .copy-paste-icon { background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=); background-repeat: no-repeat; background-size: 14px 14px; background-position: 0; display: inline-block; width: 14px; height: 14px; cursor: pointer; } PCA()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFitted Parameters n_components&nbsp; None copy&nbsp; True whiten&nbsp; False svd_solver&nbsp; &#x27;auto&#x27; tol&nbsp; 0.0 iterated_power&nbsp; &#x27;auto&#x27; n_oversamples&nbsp; 10 power_iteration_normalizer&nbsp; &#x27;auto&#x27; random_state&nbsp; None explained_var = pca.explained_variance_ cumulative_var = pca.explained_variance_ratio_.cumsum() fig, axes = plt.subplots(1, 2, figsize=(12, 5)) # Left: Scree plot axes[0].plot(range(1, len(explained_var) + 1), explained_var, marker=&#39;o&#39;) axes[0].set_title(&quot;irisPC&quot;) axes[0].set_xlabel(&quot;Principal Component&quot;) axes[0].set_ylabel(&quot;Variance&quot;) # Right: Cumulative variance axes[1].plot(range(1, len(cumulative_var) + 1), cumulative_var, marker=&#39;o&#39;) axes[1].set_title(&quot;Cumulative Variance Explained&quot;) axes[1].set_xlabel(&quot;Number of PCs&quot;) axes[1].set_ylabel(&quot;Cumulative Proportion of Variance&quot;) axes[1].set_ylim(0, 1) ## (0.0, 1.0) plt.tight_layout() plt.subplots_adjust(wspace=0.3) # spacing width of the 2 plots plt.show() Figure 5.3: Left: PC variance versus number of PCs; Right: cumulative PC variance versus number of PCs plt.close() Figure @ref{fig:pciris} is a biplot that plots two sets of information in one plot: a scatter plot of the data and a scatter plot of the first two PCs. We can tell from the biplot that both petal length and petal width are mainly only related to the first PC, sepal length is related to both the first two PCs, sepal width is mainly related to the second PC. import numpy as np from sklearn.datasets import load_iris from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler iris = load_iris(as_frame=True) X = iris.data y = iris.target target_names = iris.target_names features = X.columns X_scaled = StandardScaler().fit_transform(X) pca = PCA(n_components=2) principalComponents = pca.fit_transform(X_scaled) plt.figure(figsize=(10, 7)) # scatterplot colors = [&#39;red&#39;, &#39;green&#39;, &#39;cornflowerblue&#39;] for i, target_name in enumerate(target_names): plt.scatter( principalComponents[y == i, 0], principalComponents[y == i, 1], alpha=0.7, label=target_name, color=colors[i] ) # loadings&#39; arrows loadings = pca.components_.T * np.sqrt(pca.explained_variance_) for i, feature in enumerate(features): plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], color=&#39;blue&#39;, alpha=0.7, head_width=0.01) plt.text(loadings[i, 0] * 1.15, loadings[i, 1] * 1.15, feature, color=&#39;red&#39;, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=9) pc1_var = pca.explained_variance_ratio_[0] * 100 pc2_var = pca.explained_variance_ratio_[1] * 100 plt.xlabel(f&quot;PC1 ({pc1_var:.2f}%)&quot;) plt.ylabel(f&quot;PC2 ({pc2_var:.2f}%)&quot;) plt.legend() plt.grid(True) plt.title(&quot;Biplot of the Iris Dataset&quot;) plt.tight_layout() plt.show() Figure 5.4: Biplot of the Iris flowers data on the first two principal components plt.close() We can use the first two PCs to cluster the flowers into three species and the result is shown in Figure @ref{fig:cluster}. iris = load_iris() X = iris.data y = iris.target species = pd.Series(iris.target_names[y], name=&quot;Species&quot;) X_scaled = StandardScaler().fit_transform(X) pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) df_pca = pd.DataFrame(X_pca, columns=[&quot;PC1&quot;, &quot;PC2&quot;]) df_pca[&quot;Species&quot;] = species # ellipse plt.figure(figsize=(9, 6)) sns.set(style=&quot;whitegrid&quot;) sns.scatterplot(data=df_pca, x=&quot;PC1&quot;, y=&quot;PC2&quot;, hue=&quot;Species&quot;, style=&quot;Species&quot;, edgecolor=&quot;black&quot;, s=60) from matplotlib.patches import Ellipse import matplotlib.transforms as transforms def confidence_ellipse(x, y, ax, n_std=2.0, **kwargs): if x.size &lt;= 1: return cov = np.cov(x, y) mean = [np.mean(x), np.mean(y)] lambda_, v = np.linalg.eigh(cov) lambda_ = np.sqrt(lambda_) ell = Ellipse(xy=mean, width=lambda_[0]*n_std*2, height=lambda_[1]*n_std*2, angle=np.rad2deg(np.arccos(v[0, 0])), **kwargs) ax.add_patch(ell) ax = plt.gca() for label in df_pca[&quot;Species&quot;].unique(): group = df_pca[df_pca[&quot;Species&quot;] == label] confidence_ellipse(group[&quot;PC1&quot;], group[&quot;PC2&quot;], ax, edgecolor=&#39;black&#39;, facecolor=&#39;none&#39;) plt.title(&quot;Clustering Iris Flowers Based on the First Two PCs&quot;) plt.tight_layout() plt.show() Figure 5.5: Clustering Iris flowers based on the first two PCs plt.close() 5.4 Further Reading Independent principal component analysis (IPCA): the resulting principle components are independent. Applications: de-noise, separate the signals into several independent sources. Kernel principal component analysis (KPCA): map the data into a high dimension such that the principal components can be non-linear functions of the original variables through the kernel trick. Revisit the Learning Outcomes After finishing this note, students should be able to Explain the advantages and limitations of principal components analysis. Prove the two important results for principal component analysis: The \\(i\\)th principal component is given by \\[ Y_i=\\mathbf{e}_i^{T}\\mathbf{X}=e_{i1}X_1+e_{i2}X_2+\\cdots+e_{ip}X_p, i=1, 2, \\cdots, p \\] where \\(\\mathbf{e}_i\\) is the unit eigenvector corresponding to the \\(i\\)th largest eigenvalue of \\(\\mathbf{\\Sigma}\\). The total variance \\[ \\sigma_{11}+\\sigma_{22}+\\cdots+\\sigma_{pp}=\\sum_{i=1}^p Var(X_i)=\\lambda_1+\\lambda_2+\\cdots+\\lambda_p=\\sum_{i=1}^p Var(Y_i) \\] Determine the number of principal components, \\(k\\), needed to capture a given percentage of variation of the data. And find the first \\(k\\) corresponding principal components. "],["factor-analysis.html", "6 Factor Analysis Learning Outcomes 6.1 Model of Factor Analysis 6.2 Estimating Factor Loadings \\(l_{ij}\\) and Specific Variance \\(\\psi_i\\) 6.3 Factor Rotation 6.4 Factor Scores Revisit the Learning Outcomes", " 6 Factor Analysis The objective of factor analysis is to search for \\(m\\) common factors (latent variables which are not observed) to summarize the \\(p\\) variables (observed) of the data, which implies that only \\(m\\) features (\\(m&lt;p\\)) are required to summarize the data. For example, a survey questionnaire might contain 50 questions about students’ learning abilities; some questions are about their reading ability, some are about analytical ability and some are about calculation ability, etc. Even though there are 50 measurements for each student we only need several factors to summarize the common features. Learning Outcomes After finishing this note, students should be able to Write down the model for factor analysis and explain each term in the model. Obtain the factor loadings, communality, and specific variance of a certain variable \\(X_i\\), the proportion of variance explained by a common factor based on the computer outputs. Determine the proper number of common factors. Explain the ideas for estimating the factor loadings and the specific variance: the principle component and maximum likelihood methods. Apply the Newton-Raphson method to find the solutions of a simple equation. Explain why factor rotation is needed in factor analysis. Obtain/derive the factor scores using the weighted least squares method. 6.1 Model of Factor Analysis Recall that the objective of principle component analysis is to find \\(k\\) uncorrelated linear combinations of the original variables to summarize the data and the principle components are the associated eigenvectors of the \\(k\\) largest eigenvalues of the covariance (or correlation) matrix of the data. PCA is not based on any model. Factor analysis, however, is based on the following model: \\[\\begin{align*} X_1&amp;=b_{11}F_1+b_{12}F_2+\\cdots+b_{1m}F_m+\\epsilon_1\\\\ X_2&amp;=b_{21}F_1+b_{22}F_2+\\cdots+b_{2m}F_m+\\epsilon_2\\\\ \\vdots\\\\ X_p&amp;=b_{p1}F_1+b_{p2}F_2+\\cdots+b_{pm}F_m+\\epsilon_p \\end{align*}\\] or \\[\\begin{align*} X_1-\\mu_1&amp;=l_{11}F_1+l_{12}F_2+\\cdots+l_{1m}F_m+\\epsilon_1\\\\ X_2-\\mu_2&amp;=l_{21}F_1+l_{22}F_2+\\cdots+l_{2m}F_m+\\epsilon_2\\\\ \\vdots\\\\ X_p-\\mu_p&amp;=l_{p1}F_1+l_{p2}F_2+\\cdots+l_{pm}F_m+\\epsilon_p \\end{align*}\\] We have \\[\\begin{equation} \\label{eq:model} \\mathbf{X}-\\mathbf{\\mu}=\\mathbf{LF}+\\mathbf{\\epsilon} \\end{equation}\\] where \\(\\mathbf{X}\\) is a \\(p\\times 1\\) column vector of random variables and \\(\\mathbf{\\mu}\\) is the vector of mean of the variables; \\(\\mathbf{F}\\) is a \\(m\\times 1\\) column vector of common factors and \\(\\mathbf{L}\\) is a \\(p\\times m\\) matrix gives the loading of the common factors; and \\(\\mathbf{\\epsilon}\\) is a \\(p \\times 1\\) column vector of random variables for the error. In the model, all components except for \\(\\mathbf{X}\\) are unobserved and need to estimate. There are too many unknown parameters in the model to solve the equations; therefore, some assumptions are added to the model: \\[E(\\mathbf{F})=\\mathbf{0}_{m\\times 1}, Cov(\\mathbf{F})=E(\\mathbf{F}\\mathbf{F}^T)=\\mathbf{I}_{m\\times m}\\] \\[E(\\mathbf{\\epsilon})=\\mathbf{0}_{p\\times 1}, Cov(\\mathbf{\\epsilon})=E(\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T)=\\mathbf{\\Psi}_{p\\times p}= \\left[ \\begin{array}{cccc} \\psi_1&amp;0&amp;\\cdots&amp;0\\\\ 0&amp;\\psi_2&amp;\\cdots&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;0\\\\ 0&amp;0&amp;\\cdots&amp;\\psi_p \\end{array} \\right]\\] \\[Cov(\\mathbf{\\epsilon}, \\mathbf{F})=E(\\mathbf{\\epsilon}\\mathbf{F}^T)=\\mathbf{0}_{p\\times m}; Cov(\\mathbf{F}, \\mathbf{\\epsilon})=E(\\mathbf{F}\\mathbf{\\epsilon}^T)=\\mathbf{0}_{m\\times p}\\] Then \\[ (\\mathbf{X}-\\mathbf{\\mu})(\\mathbf{X}-\\mathbf{\\mu})^T=(\\mathbf{LF}+\\mathbf{\\epsilon})(\\mathbf{LF}+\\mathbf{\\epsilon})^T=\\mathbf{LF}\\mathbf{F}^T\\mathbf{L}^T+\\mathbf{\\epsilon}(\\mathbf{LF})^T+\\mathbf{LF}\\mathbf{\\epsilon}^T+\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T=\\mathbf{LF}\\mathbf{F}^T\\mathbf{L}^T+\\mathbf{\\epsilon}\\mathbf{F}^T\\mathbf{L}^T+\\mathbf{L}\\mathbf{F}\\mathbf{\\epsilon}^T+\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T \\] \\[ \\mathbf{\\Sigma}=\\mbox{E}[(\\mathbf{X}-\\mathbf{\\mu})(\\mathbf{X}-\\mathbf{\\mu})^T]=\\mathbf{L}\\mbox{E}(\\mathbf{F}\\mathbf{F}^T)\\mathbf{L}^T+\\mbox{E}(\\mathbf{\\epsilon}\\mathbf{F}^T)\\mathbf{L}^T+\\mathbf{L}\\mbox{E}(\\mathbf{F}\\mathbf{\\epsilon}^T)+\\mbox{E}(\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T)=\\mathbf{L}\\mathbf{L}^T+\\mathbf{\\Psi}. \\] Suppose \\(p=2\\), write out \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}^T+\\mathbf{\\Psi}\\) by terms. There are some terminologies in factor analysis. The variance of \\(X_i\\) can be decomposed into two parts: communality \\(h_i^2=l_{i1}^2+l_{i2}^2+\\cdots+l_{im}^2\\) and specific variance \\(\\psi_i\\). That is \\[ Var(X_i)=\\underbrace{l_{i1}^2+l_{i2}^2+\\cdots+l_{im}^2}\\limits_{\\mbox{{\\small communality}}}+\\underbrace{\\psi_i}\\limits_{\\mbox{{\\small specific variance}}}=h_i^2+\\psi_i. \\] The communality of variable \\(X_i\\) is the sum of squares of the \\(i\\)th row of the loading matrix \\(\\mathbf{L}\\). If we use the correlation matrix \\(\\mathbf{\\rho}\\) to conduct the factor analysis, we have \\(Var(X_i)=h_i^2+\\psi_i=1\\). The number of common factors can be determined by the number of eigenvalues that are at least 1 for the correlation matrix, which is referred to be the Kaiser rule. Or, one can draw the scree plot to determine the number of common factors required to capture a certain account of the total variation of the data. For variable \\(X_i\\), the proportion of variance explained by the common factors is \\(\\frac{h_i^2}{\\sigma_{ii}}\\). The proportion of total variance explained by the \\(m\\) common factors is \\[ \\frac{\\sum_{i=1}^p h_i^2}{\\sum_{i=1}^p \\sigma_{ii}}. \\] And the proportion of total variance explained by a single common factor, say factor \\(j\\) is \\[ \\frac{l_{1j}^2+l_{2j}^2+\\cdots+l_{pj}^2}{\\sigma_{11}+\\sigma_{22}+\\cdots+\\sigma_{pp}}=\\frac{\\mbox{sum of squares of the $j$th column of the loading matrix $\\mathbf{L}$}}{\\mbox{ trace of $\\mathbf{\\Sigma}$}}. \\] 6.2 Estimating Factor Loadings \\(l_{ij}\\) and Specific Variance \\(\\psi_i\\) To find the factor loadings \\(\\mathbf{L}\\) and the specific variances \\(\\mathbf{\\Psi}\\), two most popular estimation methods are the principle component method and the maximum likelihood method. 6.2.1 The Principle Component Method Recall the spectral decomposition; the covariance matrix can be expressed as \\[ \\mathbf{\\Sigma}=\\lambda_1\\mathbf{e_1}\\mathbf{e_1}^{T}+\\lambda_2\\mathbf{e_2}\\mathbf{e_2}^{T}+\\cdots+\\lambda_n\\mathbf{e_n}\\mathbf{e_n}^{T}=\\sum_{i=1}^n \\lambda_i\\mathbf{e_i}\\mathbf{e_i}^{T}=\\mathbf{P}\\mathbf{\\Lambda}\\mathbf{P}^{T} \\] where \\(\\lambda_i\\) is the \\(i\\)th largest eigenvalue of \\(\\mathbf{\\Sigma}\\), \\(\\mathbf{e_i}\\) is its corresponding unit eigenvector, and \\(\\mathbf{P}=[\\mathbf{e_1}, \\mathbf{e_2}, \\cdots, \\mathbf{e_n}]\\) and \\(\\mathbf{\\Lambda}\\) is the diagonal matrix \\[ \\mathbf{\\Lambda}= \\left[ \\begin{array}{ccccc} \\lambda_1 &amp;0&amp;0&amp;\\cdots&amp; 0\\\\ 0&amp;\\lambda_2&amp;0&amp;\\cdots&amp; 0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;\\cdots&amp; \\lambda_n \\end{array} \\right]. \\] Simply let \\(\\mathbf{L}=[\\sqrt{\\lambda_1}\\mathbf{e_1}, \\sqrt{\\lambda_2}\\mathbf{e_2}, \\cdots, \\sqrt{\\lambda_m}\\mathbf{e_m}]\\), i.e., the first \\(m\\) principle components. In practice, the principal component factor analysis of the sample covariance matrix \\(\\mathbf{S}\\) is specified in terms of its eigenvalue-eigenvector pairs \\((\\hat \\lambda_1, \\hat {\\mathbf{e}}_1), (\\hat \\lambda_2, \\hat {\\mathbf{e}}_2), \\cdots, (\\hat \\lambda_p, \\hat {\\mathbf{e}}_p)\\), where \\(\\hat \\lambda_1\\ge \\hat \\lambda_2\\ge \\cdots \\ge \\hat \\lambda_p\\). Then the matrix of estimated factor loadings \\(\\tilde{l}_{ij}\\) is given by \\[ \\mathbf{\\widetilde {L}}=[\\sqrt{\\hat \\lambda_1} \\hat {\\mathbf{e}}_1, \\sqrt{\\hat \\lambda_2} \\hat {\\mathbf{e}}_2, \\cdots, \\sqrt{\\hat \\lambda_m}\\hat {\\mathbf{e}}_m] \\] The estimated specific variances are given by the diagonal elements of the diagonal matrix \\[ \\mathbf{S}-\\widetilde{\\mathbf{L}}\\widetilde{\\mathbf{L}}^{T} \\], that is \\[ \\widetilde{\\mathbf{\\Psi}}= \\left[ \\begin{array}{cccc} \\widetilde{\\psi}_1&amp;0&amp;\\cdots&amp;0\\\\ 0&amp;\\widetilde{\\psi}_2&amp;\\cdots&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;0\\\\ 0&amp;0&amp;\\cdots&amp;\\widetilde{\\psi}_p \\end{array} \\right] \\quad \\mbox{ with $\\widetilde{\\psi}_i=s_{ii}-\\sum_{j=1}^m \\tilde{l}^2_{ij}$} \\] Communalities are estimated by \\(\\tilde {h}_i^2=\\tilde{l}_{i1}^2+\\tilde{l}_{i2}^2+\\cdots+\\tilde{l}_{im}^2\\). 6.2.2 The Maximum Likelihood Method If we further assume both the common factors \\(\\mathbf{F}\\) and the error \\(\\mathbf{\\epsilon}\\) to be normally distributed, then \\((\\mathbf{X}-\\mathbf{\\mu})\\sim \\mbox{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})\\). For a single observation \\(\\mathbf{x}\\), its density function is \\[ f(\\mathbf{x})=\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}}\\exp \\left\\{-\\frac{(\\mathbf{x}-\\mathbf{\\mu})^{T}\\mathbf{\\Sigma}^{-1} (\\mathbf{x}-\\mathbf{\\mu})}{2}\\right\\}. \\] Consider \\(n\\) independent observations, the likelihood function is the product of the \\(n\\) independent multivariate normal density functions \\[ Q=\\prod_{i=1}^n f(\\mathbf{x_i})=\\prod_{i=1}^n \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}}\\exp \\left\\{-\\frac{(\\mathbf{x_i}-\\mathbf{\\mu})^{T}\\mathbf{\\Sigma}^{-1} (\\mathbf{x_i}-\\mathbf{\\mu})}{2}\\right\\}. \\] Since \\(\\mathbf{\\Sigma}=\\mathbf{L}\\mathbf{L}^T+\\mathbf{\\Psi}\\), the about likelihood function becomes \\[ Q=\\prod_{i=1}^n f(\\mathbf{x_i})=\\prod_{i=1}^n \\frac{1}{(2\\pi)^{p/2}|\\mathbf{L}\\mathbf{L}^T+\\mathbf{\\Psi}|^{1/2}}|\\exp \\left\\{-\\frac{(\\mathbf{x_i}-\\mathbf{\\mu})^{T}(\\mathbf{L}\\mathbf{L}^T+\\mathbf{\\Psi})^{-1} (\\mathbf{x_i}-\\mathbf{\\mu})}{2}\\right\\}. \\] To find the maximum likelihood (ML) estimates, the standard way is to find the values of \\(\\mathbf{L}, \\mathbf{\\Psi}\\), and \\(\\mathbf{\\mu}\\) such that the likelihood function \\(Q\\) by solving for the following equations: \\[ \\frac{\\partial Q}{\\partial \\mathbf{L}}=0;\\quad \\frac{\\partial Q}{\\partial \\mathbf{\\Psi}}=0; \\quad \\frac{\\partial Q}{\\partial \\mathbf{\\mu}}=0. \\] We denote the maximize likelihood estimates as \\(\\hat {\\mathbf{L}}\\), \\(\\hat {\\mathbf{\\Psi}}\\), and \\(\\hat {\\mathbf{\\mu}}\\). However, there is no closed-form solution to this problem, and the ML estimator must be obtained by numerical method. One widely used numerical method is the Newton-Raphson method, which solves the form \\(f(x) = 0\\) equations. The main idea of the Newton-Raphson method is as follows. We set an initial value \\(x_0\\), and the sequence \\(x_0, x_1, x_2, x_3, \\cdots\\) generated in the manner described below should converge to the exact root. The equation of the tangent line to the graph \\(y = f(x)\\) at the point \\((x_0, f(x_0))\\) is \\[ y-f(x_0) = f^{&#39;}(x_0)(x-x_0). \\] The tangent line intersects the \\(x\\)-axis when \\(y = 0\\) and \\(x = x_1\\), so \\[ 0-f(x_0) = f^{&#39;}(x_0)(x_1-x_0) \\quad \\Longrightarrow x_1=x_0-\\frac{f(x_0)}{f^{&#39;}(x_0)}. \\] More generally, we have \\[ x_{n+1}=x_n-\\frac{f(x_n)}{f^{&#39;}(x_n)}. \\] Therefore, the steps of Newton-Raphson method to solve \\(f(x) = 0\\) are: Set an initial value \\(x_0\\). Calculate \\(x_{n+1}=x_n-\\frac{f(x_n)}{f^{&#39;}(x_n)}\\). Stop if \\(|x_{n+1}-x_n|&lt;\\mbox{threshold}\\); otherwise, let \\(x_n=x_{n+1}\\) and \\(x_{n+1}=x_n-\\frac{f(x_n)}{f^{&#39;}(x_n)}\\). Repeat step (3). To find the ML estimate of \\(f(x)\\) is to solve for \\(f^{&#39;}(x)=0\\). Therefore, we just need to iterate \\(x_{n+1}=x_n-\\frac{f^{&#39;}(x_n)}{f^{&#39;&#39;}(x_n)}\\) until converge. \\(\\textbf{Example}: \\text{Newton-Raphson Univariate Case}\\) Find the roots of \\(f(x)=x^2+6x+8\\) using Newton-Raphson method. Note that \\(f&#39;(x)=2x+6\\) and the update is \\[ x_{n+1}=x_{n}-\\frac{f(x_n)}{f&#39;(x_n)}=x_{n}-\\frac{x_n^2+6x_n+8}{2x_n+6} \\] Set the initial value \\(x_0=-5\\) and the stopping criterion \\(|x_{n+1}-x_n|&lt;0.00001\\). \\(x_1=x_0-\\frac{x_0^2+6x_0+8}{2x_0+6}=-4.25\\), since \\(|x_1-x_0|=|-4.25-(-5)|&gt;0.00001\\), move on. \\(x_2=x_1-\\frac{x_1^2+6x_1+8}{2x_1+6}=-4.025\\), since \\(|x_2-x_1|=|-4.025-(-4.25)|&gt;0.00001\\), move on. \\(x_3=x_2-\\frac{x_2^2+6x_2+8}{2x_2+6}=-4.000305\\), since \\(|x_3-x_2|=|-4.000305-(-4.025)|=0.0247&gt;0.00001\\), move on. \\(x_4=x_3-\\frac{x_3^2+6x_3+8}{2x_3+6}=-4\\), since \\(|x_4-x_3|=|-4-(-4.000305)|=0.000305&gt;0.00001\\), move on. \\(x_5=x_4-\\frac{x_4^2+6x_4+8}{2x_4+6}=-4\\), since \\(|x_5-x_4|=|-4-(-4)|=0&lt;0.00001\\), stop. The solution is \\(x=-4\\). Set the initial value \\(x_0=10\\), repeat the procedure and we get \\(x=-2\\). def f(x): return x**2 + 6*x + 8 def fp(x): return 2*x + 6 x0 = -5 error = 0.00001 x1 = x0 - f(x0) / fp(x0) while abs(x1 - x0) &gt; error: x0 = x1 x1 = x0 - f(x0) / fp(x0) print(&quot;x =&quot;, x1) ## x = -4.025 ## x = -4.000304878048781 ## x = -4.000000046461148 ## x = -4.0 x0 = 10 x1 = x0 - f(x0) / fp(x0) while abs(x1 - x0) &gt; error: x0 = x1 x1 = x0 - f(x0) / fp(x0) print(&quot;x =&quot;, x1) ## x = 0.34570135746606345 ## x = -1.1777038249185776 ## x = -1.8144728039273512 ## x = -1.9854831080229098 ## x = -1.999896137681718 ## x = -1.9999999946068696 ## x = -2.0 \\(\\textbf{Example}: \\text{MLE Using Newton-Raphson Univariate Case}\\) Suppose \\(X_1, \\cdots, X_n\\sim \\text{Bernoulli}(p)\\), find the MLE of the probability success \\(\\hat p\\). The likelihood function is \\[ L(p)=\\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\\sum y_i}(1-p)^{n-\\sum y_i}. \\] The log-likelihood is \\[ l(p)=(\\sum y_i)\\log p+(n-\\sum y_i)\\log(1-p). \\] The first and the second derivatives of the log-likelihood are \\[ \\begin{aligned} l&#39;(p)&amp;=\\frac{\\partial l(p)}{\\partial p}=\\frac{\\sum y_i}{p}-\\frac{n-\\sum y_i}{1-p}\\\\ l&#39;&#39;(p)&amp;=\\frac{\\partial^2 l(p)}{\\partial p^2}=-\\frac{\\sum y_i}{p^2}-\\frac{n-\\sum y_i}{(1-p)^2}\\\\ \\end{aligned} \\] To find the MLE \\(\\hat p\\), we need to find the solution of the equation \\(l&#39;(p)=0\\) which can be solved by the Newton-Raphson method with the iterative update: \\[ p_{n+1}=p_n-\\frac{l&#39;(p_n)}{l&quot;(p_n)}. \\] code for obtaining MLE by the Newton-Raphson method. import numpy as np def lp1(yvec, p): n = len(yvec) y = np.sum(yvec) return y/p - (n - y)/(1 - p) def lp2(yvec, p): n = len(yvec) y = np.sum(yvec) return -y/(p**2) - (n - y)/((1 - p)**2) def newton_p(yvec, p0, err=1e-6, maxit=100): for i in range(1, maxit + 1): p1 = p0 - lp1(yvec, p0)/lp2(yvec, p0) if abs(p1 - p0) &lt; err: break p0 = p1 return {&#39;phat&#39;: p0, &#39;iterations&#39;: i} np.random.seed(4061) n = 100 true_p = 0.6 yvec = np.random.binomial(1, true_p, n) p0 = 0.2 result = newton_p(yvec, p0) print(f&quot;phat = {result[&#39;phat&#39;]}, iterations = {result[&#39;iterations&#39;]}&quot;) ## phat = 0.6700000019071672, iterations = 6 The Newton-Raphson method can be extended to multivariate cases. Take logistic regression for example, the coefficient parameters \\(\\mathbf{\\beta}\\) can be updated by: \\[ \\mathbf{\\beta}_{n+1}=\\mathbf{\\beta}_n-H^{-1}(\\mathbf{\\beta}_n)g( \\mathbf{\\beta}_n) \\] where \\(g(\\mathbf{\\beta})\\) is the gradient of the log likelihood \\(\\log L(\\mathbf{\\beta})\\), it is vector of partial derivatives \\[g(\\mathbf{\\beta})=\\left[\\frac{\\partial \\log L(\\mathbf{\\beta})}{\\partial \\beta_1}, \\frac{\\partial \\log L(\\mathbf{\\beta})}{\\partial \\beta_2}, \\cdots, \\frac{\\partial \\log L(\\mathbf{\\beta})}{\\partial \\beta_p}\\right]\\] and \\(H(\\mathbf{\\beta})\\) is the Hessian matrix of the log likelihood function \\(\\log L(\\mathbf{\\beta})\\), it is a matrix of second partial derivatives. That is the elements of the Hessian matrix are \\[ H_{ij}=\\frac{\\partial^2 \\log L(\\mathbf{\\beta})}{\\partial \\beta_i \\partial \\beta_j}. \\] And the stopping criterion is \\(|\\mathbf{\\beta}_{n+1}-\\mathbf{\\beta}_n|&lt;\\mbox{threshold}\\). \\(\\textbf{Example}: \\text{MLE Using Newton-Raphson Multivariate Case}\\) Suppose \\(X_1, \\cdots, X_n\\sim N(\\mu, \\sigma)\\), find the MLE of mean \\(\\mu\\) and standard deviation \\(\\sigma\\) by Newton-Raphson method. The likelihood function is \\[ L(\\mu, \\sigma)=\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}=(2\\pi)^{-n/2}(\\sigma)^{-n}\\exp\\left\\{-\\frac{\\sum (x_i-\\mu)^2}{2\\sigma^2}\\right\\}. \\] The log-likelihood is \\[ l(\\mu, \\sigma)=(-\\frac{n}{2})\\log(2\\pi)+(-n)\\log(\\sigma)-\\frac{\\sum (x_i-\\mu)^2}{2\\sigma^2}. \\] The gradient vector is \\[ \\left[\\frac{\\partial l(\\mu, \\sigma)}{\\partial \\mu}, \\frac{\\partial l(\\mu, \\sigma)}{\\partial \\sigma}\\right]=\\left[\\frac{\\sum (x_i-\\mu)}{\\sigma^2}, -\\frac{n}{\\sigma}+\\frac{\\sum (x_i-\\mu)^2}{\\sigma^3}\\right]. \\] The Hessian matrix is given by \\[ \\mathbf{H}=\\left[ \\begin{array}{cc} \\frac{\\partial^2 l(\\mu, \\sigma)}{\\partial \\mu^2} &amp; \\frac{\\partial^2 l(\\mu, \\sigma)}{\\partial \\mu \\partial \\sigma}\\\\ \\frac{\\partial^2 l(\\mu, \\sigma)}{ \\partial \\sigma \\partial \\mu} &amp; \\frac{\\partial^2 l(\\mu, \\sigma)}{\\partial \\sigma^2}\\\\ \\end{array} \\right]=\\left[ \\begin{array}{cc} -\\frac{n}{\\sigma^2} &amp; -\\frac{2\\sum (x_i-\\mu)}{\\sigma^3}\\\\ -\\frac{2\\sum (x_i-\\mu)}{\\sigma^3} &amp; \\frac{n}{\\sigma^2}-\\frac{3\\sum (x_i-\\mu)^2}{\\sigma^4}\\\\ \\end{array} \\right]; \\] code for obtaining the MLE of \\((\\mu, \\sigma)\\) for normal distribution using Newton-Raphson method. def gradient_norm(yvec, m, sigma): n = len(yvec) dm1 = np.sum(yvec - m) / (sigma ** 2) dsigma1 = -n / sigma + np.sum((yvec - m) ** 2) / (sigma ** 3) return np.array([dm1, dsigma1]) def hessian_norm(yvec, m, sigma): n = len(yvec) hmat = np.zeros((2, 2)) hmat[0, 0] = -n / (sigma ** 2) hmat[1, 1] = n / (sigma ** 2) - 3 * np.sum((yvec - m) ** 2) / (sigma ** 4) hmat[0, 1] = hmat[1, 0] = -2 * np.sum(yvec - m) / (sigma ** 3) return hmat def newton_norm(yvec, m0, sigma0, err=1e-6, maxit=100): bvec0 = np.array([m0, sigma0]) for i in range(1, maxit + 1): hmat = hessian_norm(yvec, bvec0[0], bvec0[1]) grad = gradient_norm(yvec, bvec0[0], bvec0[1]) try: delta = np.linalg.solve(hmat, grad) except np.linalg.LinAlgError: raise RuntimeError(&quot;Hessian is singular and cannot be inverted.&quot;) bvec1 = bvec0 - delta if np.linalg.norm(bvec1 - bvec0) &lt; err: break bvec0 = bvec1 return {&quot;mean&quot;: bvec1[0], &quot;std&quot;: bvec1[1], &quot;iterations&quot;: i} np.random.seed(4061) n = 1000 true_mean = 10 true_std = 2 m0 = 8 sigma0 = 1 x = np.random.normal(loc=true_mean, scale=true_std, size=n) result = newton_norm(x, m0, sigma0) print(f&quot;mean = {result[&#39;mean&#39;]}, std = {result[&#39;std&#39;]}, iterations = {result[&#39;iterations&#39;]}&quot;) ## mean = 10.021352240103477, std = 2.0308476397702138, iterations = 9 6.3 Factor Rotation After the \\(m\\) common factors are found, the next step is to label the common factors to unveil the underlying features. However, the initial loadings yielded by either the principle component method or the maximum likelihood method might be difficult to interpret; we can rotate them by an orthogonal transformation until a simple and interpretable structure is achieved. The goal is to find an orthogonal transformation such that each variable has a large loading on a single factor and relatively small loadings on the other factors. As a result, we are able to group the variables and relate them to the common factors, i.e., we can label the common factors. Note that for any orthogonal transformation \\(\\mathbf{T}\\), \\(\\mathbf{T}\\mathbf{T}^T=\\mathbf{T}^T\\mathbf{T}=\\mathbf{I}\\). The rotated loading is given by \\(\\mathbf{L}_1=\\mathbf{L}\\mathbf{T}\\), then \\[ \\mathbf{L}_1\\mathbf{L}_1^T=\\mathbf{L}\\mathbf{T}\\mathbf{T}^T\\mathbf{L}^T=\\mathbf{L}\\mathbf{L}^T \\] which means factor rotation won’t change the communalities and specific variances. One of the most popular methods of factor rotation is the \\(\\textit{varimax}\\) criterion, which solves for the rotated loadings \\(\\widetilde {l}_{ij}\\) such that the following quantity is maximized \\[ V=\\frac{1}{p}\\sum_{j=1}^m\\left[\\sum_{i=1}^p\\widetilde {l}_{ij}^4-\\frac{1}{p}(\\sum_{i=1}^p \\widetilde {l}_{ij}^2)^2\\right], \\quad \\widetilde {l}_{ij}=\\frac{l_{ij}}{h_i^2}=\\frac{l_{ij}}{\\sqrt{\\sum_{i=1}^m l_{ij}^2}}. \\] The varimax procedure selects the orthogonal transformation that makes the sample variance of the standardized loadings for each factor (summed over the \\(m\\) common factors) as large as possible. 6.4 Factor Scores In factor analysis, the major interests are factor loadings and factor labeling. In some applications, the estimated values of the common factors are useful for prediction and model fitting. One method to obtain the factor scores is the weighted least squares method. Recall that the factor model is given by \\(\\mathbf{X}-\\mathbf{\\mu}=\\mathbf{LF}+\\mathbf{\\epsilon}\\). The main idea of weighted least squares is to obtain the value of \\(\\hat {\\mathbf{f}}\\) such that the weighted square of the error \\[ \\sum_{i=1}^p \\frac{\\epsilon_i^2}{\\psi_i}=\\mathbf{\\epsilon}^T\\mathbf{\\Psi}^{-1}\\mathbf{\\epsilon}=(\\mathbf{x}-\\mathbf{\\mu}-\\mathbf{L}\\mathbf{f})^T\\mathbf{\\Psi}^{-1} (\\mathbf{x}-\\mathbf{\\mu}-\\mathbf{L}\\mathbf{f}) \\] is minimized. And the solution is \\[ \\hat {\\mathbf{f}}=(\\mathbf{L}^T\\mathbf{\\Psi}^{-1}\\mathbf{L})^{-1}\\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu}). \\] \\(\\textbf{Proof}\\): Let \\[ Q=(\\mathbf{x}-\\mathbf{\\mu}-\\mathbf{L}\\mathbf{f})^T\\mathbf{\\Psi}^{-1} (\\mathbf{x}-\\mathbf{\\mu}-\\mathbf{L}\\mathbf{f})= (\\mathbf{x}-\\mathbf{\\mu})^T\\mathbf{\\Psi}^{-1} (\\mathbf{x}-\\mathbf{\\mu})-\\mathbf{f}^T\\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu})-(\\mathbf{x}-\\mathbf{\\mu})^T\\mathbf{\\Psi}^{-1}\\mathbf{L}\\mathbf{f}+\\mathbf{f}^T\\mathbf{L}^T\\mathbf{\\Psi}^{-1}\\mathbf{L}\\mathbf{f} \\] \\[ \\frac{\\partial Q}{\\partial \\mathbf{f}}=-\\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu})+2\\mathbf{L}^T\\mathbf{\\Psi}^{-1}\\mathbf{L}\\mathbf{f}-\\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu})=0\\Longrightarrow \\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu})=\\mathbf{L}^T\\mathbf{\\Psi}^{-1}\\mathbf{L}\\mathbf{f} \\] which gives the solution is \\[ \\hat {\\mathbf{f}}=(\\mathbf{L}^T\\mathbf{\\Psi}^{-1}\\mathbf{L})^{-1}\\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu}). \\] Plug the maximum likelihood estimates of \\(\\mathbf{L}\\), \\(\\mathbf{\\Psi}\\), and \\(\\mathbf{\\mu}\\) into the equation, the estimated factor scores are \\[ \\hat {\\mathbf{f}}=(\\hat {\\mathbf{L}}^T\\hat {\\mathbf{\\Psi}}^{-1}\\hat {\\mathbf{L}})^{-1} \\hat {\\mathbf{L}}^T\\hat {\\mathbf{\\Psi}}^{-1}(\\mathbf{x}-\\bar {\\mathbf{x}}). \\] \\(\\textbf{Example}\\): Factor Analysis This data set gives 33 men’s decathlon performances at the Olympic Games (1988). 10 columns events of the decathlon: 100 meters (100), long jump (long), shotput (poid), high jump (haut), 400 meters (400), 110-meter hurdles (110), discus throw (disq), pole vault (perc), javelin (jave) and 1500 meters (1500). # ade4 package and the olympic dataset is not built into any standard Python library # need to download it (from the data folder as olympic.csv) import pandas as pd dmat = pd.read_csv(&quot;data/olympic.csv&quot;, index_col=0) print(dmat.head()) ## 100 long poid haut 400 110 disq perc jave 1500 ## 1 11.25 7.43 15.48 2.27 48.90 15.13 49.28 4.7 61.32 268.95 ## 2 10.87 7.45 14.97 1.97 47.71 14.46 44.36 5.1 61.76 273.02 ## 3 11.18 7.44 14.20 1.97 48.29 14.81 43.66 5.2 64.16 263.20 ## 4 10.62 7.38 15.02 2.03 49.06 14.72 44.80 4.9 64.04 285.11 ## 5 11.02 7.43 12.92 1.97 47.44 14.40 41.20 5.2 57.46 256.64 Is it reasonable to conduct a factor analysis? We can check with correlation matrix and correlation plot. The correlation plot illustrates that some events have strong association. Therefore it is reasonable to conduct a factor analysis. import seaborn as sns import matplotlib.pyplot as plt cormat = dmat.corr().round(2) print(&quot;Correlation matrix of 10 events:&quot;) ## Correlation matrix of 10 events: print(cormat) ## 100 long poid haut 400 110 disq perc jave 1500 ## 100 1.00 -0.54 -0.21 -0.15 0.61 0.64 -0.05 -0.39 -0.06 0.26 ## long -0.54 1.00 0.14 0.27 -0.52 -0.48 0.04 0.35 0.18 -0.40 ## poid -0.21 0.14 1.00 0.12 0.09 -0.30 0.81 0.48 0.60 0.27 ## haut -0.15 0.27 0.12 1.00 -0.09 -0.31 0.15 0.21 0.12 -0.11 ## 400 0.61 -0.52 0.09 -0.09 1.00 0.55 0.14 -0.32 0.12 0.59 ## 110 0.64 -0.48 -0.30 -0.31 0.55 1.00 -0.11 -0.52 -0.06 0.14 ## disq -0.05 0.04 0.81 0.15 0.14 -0.11 1.00 0.34 0.44 0.40 ## perc -0.39 0.35 0.48 0.21 -0.32 -0.52 0.34 1.00 0.27 -0.03 ## jave -0.06 0.18 0.60 0.12 0.12 -0.06 0.44 0.27 1.00 0.10 ## 1500 0.26 -0.40 0.27 -0.11 0.59 0.14 0.40 -0.03 0.10 1.00 plt.figure(figsize=(8, 6)) sns.heatmap(cormat, annot=False, cmap=&quot;RdBu_r&quot;, center=0, square=True, linewidths=0.5, cbar_kws={&quot;shrink&quot;: 0.8}) plt.title(&quot;Correlation Plot of 10 Events&quot;) plt.tight_layout() plt.show() Figure 6.1: Correlation Plot of 10 Events plt.close() Determine the number common factors. We can use the Kaiser rule to determine the number of common factors, i.e., number of eigenvalues at least 1 of the correlation matrix. eigenvalues, _ = np.linalg.eig(cormat) print(np.round(eigenvalues, 3)) ## [3.424 2.608 0.942 0.881 0.095 0.262 0.307 0.555 0.429 0.496] Since there are two eigenvalues greater than 1, we can use two common factors. The third largest value is 0.942 which is very close to 1; it is fine to use three common factors as well. We can also use the scree plot or the cumulative variation plot to determine the number of common factors. For the scree plot, we look for the turning point that the elbow becomes flat; for the cumulative variation plot, we look for the cut-point for a specified cumulative proportion of variation. eigenvalues = np.sort(eigenvalues)[::-1] cumulative = np.cumsum(eigenvalues) / len(eigenvalues) print(cumulative) ## [0.34243219 0.60322622 0.69738887 0.78550513 0.84101802 0.89058061 ## 0.93351651 0.964219 0.99045166 1. ] fig, ax = plt.subplots(1, 2, figsize=(10, 4)) # Scree plot ax[0].plot(range(1, len(eigenvalues) + 1), eigenvalues, marker=&#39;o&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x29c533250&gt;] ax[0].set_title(&quot;Scree Plot&quot;) ## Text(0.5, 1.0, &#39;Scree Plot&#39;) ax[0].set_xlabel(&quot;Number of Common Factors&quot;) ## Text(0.5, 0, &#39;Number of Common Factors&#39;) ax[0].set_ylabel(&quot;Eigenvalues&quot;) ## Text(0, 0.5, &#39;Eigenvalues&#39;) # Cumulative proportion plot ax[1].plot(range(1, len(cumulative) + 1), cumulative, marker=&#39;o&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x29c533390&gt;] ax[1].axhline(y=0.8, color=&#39;red&#39;, linestyle=&#39;--&#39;, linewidth=2) ## &lt;matplotlib.lines.Line2D object at 0x29c5334d0&gt; ax[1].axhline(y=0.9, color=&#39;blue&#39;, linestyle=&#39;--&#39;, linewidth=2) ## &lt;matplotlib.lines.Line2D object at 0x29c533610&gt; ax[1].set_title(&quot;Cumulative Variance Explained&quot;) ## Text(0.5, 1.0, &#39;Cumulative Variance Explained&#39;) ax[1].set_xlabel(&quot;Number of Common Factors&quot;) ## Text(0.5, 0, &#39;Number of Common Factors&#39;) ax[1].set_ylabel(&quot;Cumulative Proportion&quot;) ## Text(0, 0.5, &#39;Cumulative Proportion&#39;) plt.tight_layout() plt.show() Figure 6.2: Determine the Number of Common Factors plt.close() The scree plot shows the elbow becomes flat after three common factors. The cumulative proportion variation plot implies that we need three common factors to capture about 70% of the variation of the data, four common factors for 80% and six common factors for 90%. Compare the factor loadings with and without rotation with two and three common factors; using PCA method and likelihood method. from factor_analyzer import FactorAnalyzer from sklearn.preprocessing import StandardScaler dmat_scaled = StandardScaler().fit_transform(dmat) # Without rotation fa_pa = FactorAnalyzer(n_factors=2, method=&#39;principal&#39;, rotation=None) fa_pa.fit(dmat_scaled) ## FactorAnalyzer(method=&#39;principal&#39;, n_factors=2, rotation=None, ## rotation_kwargs={}) loadings_pa = pd.DataFrame(fa_pa.loadings_, index=dmat.columns) fa_ml = FactorAnalyzer(n_factors=2, method=&#39;ml&#39;, rotation=None) fa_ml.fit(dmat_scaled) ## FactorAnalyzer(method=&#39;ml&#39;, n_factors=2, rotation=None, rotation_kwargs={}) loadings_ml = pd.DataFrame(fa_ml.loadings_, index=dmat.columns) print(&quot;Factor Loadings without Rotation (PA and ML):&quot;) ## Factor Loadings without Rotation (PA and ML): print(round(pd.concat([loadings_pa, loadings_ml], axis=1), 3)) ## 0 1 0 1 ## 100 -0.769 0.240 0.745 -0.220 ## long 0.729 -0.246 -0.655 0.155 ## poid 0.498 0.781 0.014 0.989 ## haut 0.392 0.045 -0.221 0.135 ## 400 -0.658 0.569 0.840 0.083 ## 110 -0.801 0.112 0.697 -0.309 ## disq 0.325 0.813 0.148 0.813 ## perc 0.710 0.241 -0.427 0.493 ## jave 0.333 0.600 0.054 0.599 ## 1500 -0.315 0.680 0.543 0.273 # With rotation fa_pa_rot = FactorAnalyzer(n_factors=2, method=&#39;principal&#39;, rotation=&#39;varimax&#39;) fa_pa_rot.fit(dmat_scaled) ## FactorAnalyzer(method=&#39;principal&#39;, n_factors=2, rotation=&#39;varimax&#39;, ## rotation_kwargs={}) loadings_pa_rot = pd.DataFrame(fa_pa_rot.loadings_, index=dmat.columns) fa_ml_rot = FactorAnalyzer(n_factors=2, method=&#39;ml&#39;, rotation=&#39;varimax&#39;) fa_ml_rot.fit(dmat_scaled) ## FactorAnalyzer(method=&#39;ml&#39;, n_factors=2, rotation=&#39;varimax&#39;, rotation_kwargs={}) loadings_ml_rot = pd.DataFrame(fa_ml_rot.loadings_, index=dmat.columns) print(&quot;\\nFactor Loadings with Rotation (PA and ML):&quot;) ## ## Factor Loadings with Rotation (PA and ML): print(round(pd.concat([loadings_pa_rot, loadings_ml_rot], axis=1), 3)) ## 0 1 0 1 ## 100 0.802 -0.072 0.772 -0.089 ## long -0.767 0.052 -0.672 0.041 ## poid -0.161 0.912 -0.155 0.977 ## haut -0.345 0.192 -0.241 0.095 ## 400 0.825 0.274 0.814 0.226 ## 110 0.783 -0.203 0.739 -0.185 ## disq 0.011 0.875 0.007 0.826 ## perc -0.564 0.495 -0.505 0.413 ## jave -0.078 0.682 -0.049 0.600 ## 1500 0.551 0.508 0.488 0.362 The loadings after rotation are more extreme than those without rotation, which makes it easier to label the common factors. The computer output of the model with two common factors after rotation is as follows. Factor Analysis using method = ml Call: fa(r = dmat, nfactors = 2, rotate = &quot;varimax&quot;, fm = &quot;ml&quot;) Standardized loadings (pattern matrix) based upon correlation matrix ML2 ML1 h2 u2 com 100 0.77 -0.09 0.604 0.396 1.0 long -0.67 0.04 0.453 0.547 1.0 poid -0.16 0.98 0.978 0.022 1.1 haut -0.24 0.10 0.067 0.933 1.3 400 0.81 0.23 0.713 0.287 1.2 110 0.74 -0.19 0.580 0.420 1.1 disq 0.01 0.83 0.682 0.318 1.0 perc -0.50 0.41 0.425 0.575 1.9 jave -0.05 0.60 0.362 0.638 1.0 1500 0.49 0.36 0.369 0.631 1.8 ML2 ML1 SS loadings 2.83 2.40 Proportion Var 0.28 0.24 Cumulative Var 0.28 0.52 Proportion Explained 0.54 0.46 Cumulative Proportion 0.54 1.00 Mean item complexity = 1.2 Test of the hypothesis that 2 factors are sufficient. The degrees of freedom for the null model are 45 and the objective function was 4.93 with Chi Square of 137.14 The degrees of freedom for the model are 26 and the objective function was 0.72 The root mean square of the residuals (RMSR) is 0.06 The df corrected root mean square of the residuals is 0.08 The harmonic number of observations is 33 with the empirical chi square 10.47 with prob &lt; 1 The total number of observations was 33 with Likelihood Chi Square = 19.12 with prob &lt; 0.83 Tucker Lewis Index of factoring reliability = 1.139 RMSEA index = 0 and the 90 % confidence intervals are 0 0.085 BIC = -71.79 Fit based upon off diagonal values = 0.97 Measures of factor score adequacy ML2 ML1 Correlation of (regression) scores with factors 0.93 0.99 Multiple R square of scores with factors 0.87 0.98 Minimum correlation of possible factor scores 0.75 0.95 The following table gives the factor scores of the first ten athletes. We can use these two factor scores as predictors to replace the original 10 variables for future model fitting. fa = FactorAnalyzer(n_factors=2, method=&#39;ml&#39;, rotation=&#39;varimax&#39;) fa.fit(dmat_scaled) ## /Users/ruskin/r-python-env/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: &#39;force_all_finite&#39; was renamed to &#39;ensure_all_finite&#39; in 1.6 and will be removed in 1.8. ## warnings.warn( ## FactorAnalyzer(method=&#39;ml&#39;, n_factors=2, rotation=&#39;varimax&#39;, rotation_kwargs={}) factor_scores = fa.transform(dmat_scaled) ## /Users/ruskin/r-python-env/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: &#39;force_all_finite&#39; was renamed to &#39;ensure_all_finite&#39; in 1.6 and will be removed in 1.8. ## warnings.warn( factor_scores_df = pd.DataFrame(factor_scores, columns=[&quot;ML1&quot;, &quot;ML2&quot;]) print(&quot;Factor Scores for ten athletes:&quot;) ## Factor Scores for ten athletes: print(factor_scores_df.round(3).head(10)) ## ML1 ML2 ## 0 -0.329 1.117 ## 1 -1.455 0.538 ## 2 -0.850 0.072 ## 3 -0.940 0.687 ## 4 -1.503 -0.997 ## 5 -1.462 -0.487 ## 6 -0.438 0.109 ## 7 -0.923 0.819 ## 8 -0.390 0.362 ## 9 -0.405 0.957 \\(\\textbf{Example}\\): Principle Component Analysis Versus Factor Analysis Given the covariance matrix \\[ \\text{Cov}(\\mathbf{X})=\\mathbf{\\Sigma}=\\left[ \\begin{array}{rr} 17&amp;4\\\\ 4&amp;2 \\end{array} \\right]. \\] [(a)] Find the first principal component of \\(\\mathbf{X}\\). The basis of the first PC is the unit eigenvector corresponding to the largest eigenvalue. \\[ |\\mathbf{A}-\\lambda \\mathbf{I}|=\\left| \\begin{array}{cc} 17-\\lambda&amp; 4\\\\ 4&amp; 2-\\lambda \\end{array} \\right|=(17-\\lambda)(2-\\lambda)-16=0 \\Longrightarrow \\lambda^2-19\\lambda+18=0 \\] which gives the eigenvalues are \\(\\lambda_1=18\\) and \\(\\lambda_2=1\\). The largest eigenvalue is \\(\\lambda_1=18\\), we have \\[ \\left[ \\begin{array}{cc} 1&amp; 4\\\\ 4&amp; -16 \\end{array} \\right] \\left[ \\begin{array}{c} x_1\\\\x_2 \\end{array} \\right]=\\left[ \\begin{array}{c} 0\\\\0 \\end{array} \\right] \\] which gives \\(x_1=4x_2\\); therefore the eigenvector could be \\[ \\mathbf{x_1}=\\left[ \\begin{array}{c} 4\\\\1 \\end{array} \\right] \\] and the unit eigenvector is \\[ \\mathbf{e_1}=\\left[ \\begin{array}{c} \\frac{4}{\\sqrt{17}}\\\\\\frac{1}{\\sqrt{17}} \\end{array} \\right]. \\] The first PC is \\[ Y_1=\\mathbf{a_1}^T\\mathbf{x}=\\mathbf{e_1}^T\\mathbf{x}=\\left[\\frac{4}{\\sqrt{17}}\\quad \\frac{1}{\\sqrt{17}}\\right] \\left[\\begin{array}{c} x_1\\\\x_2\\\\ \\end{array} \\right]=\\frac{4}{\\sqrt{17}} x_1+\\frac{1}{\\sqrt{17}} x_2 \\] [(b)] Suppose \\(\\mathbf{\\Sigma}\\) can be decomposed as \\[ \\mathbf{\\Sigma}=\\left[ \\begin{array}{rr} 17&amp;4\\\\ 4&amp;2 \\end{array} \\right] =\\left[ \\begin{array}{c} 4\\\\ 1 \\end{array} \\right][4\\quad 1]+\\left[ \\begin{array}{rr} 1&amp;0\\\\ 0&amp;1 \\end{array} \\right] \\] Obtain the factor loading with \\(m=1\\) (only one common factor), communalities, specific variances, and the proportion of variance explained by the common factor. We have \\[\\begin{align*} X_1&amp;=l_{11}F_1+\\epsilon_1=4F_1+\\epsilon_1\\Longrightarrow \\mbox{Var}(X_1)=l_{11}^2+\\psi_1\\\\ X_2&amp;=l_{21}F_1+\\epsilon_2=F_1+\\epsilon_2\\Longrightarrow \\mbox{Var}(X_2)=l_{21}^2+\\psi_2 \\end{align*}\\] The communality (common variance explained by the common factor \\(F_1\\)) for \\(X_1\\) is \\(h_1^2=l_{11}^2=4^2=16\\), and the communality for \\(X_2\\) is \\(h_2^2=l_{21}^2=1^2=1\\), specific variances for \\(X_1\\) and \\(X_2\\) are \\(\\psi_1=1\\) and \\(\\psi_2=1\\) respectively. The proportion of total variance explained by the common factor \\(F_1\\) is \\[ \\frac{l_{11}^2+l_{21}^2}{\\mbox{Var}(X_1)+\\mbox{Var}(X_2)}=\\frac{h_1^2+h_2^2}{17+2}=\\frac{16+1}{19}=0.8947 \\] [(c)] Given an observation \\(\\mathbf{x}=[1 \\quad 2]^T\\) and \\(\\mathbf{\\mu}=\\mathbf{0}\\), calculate its score on the first principal component. The score on the first PC is \\[ Y_1=\\mathbf{a_1}^T\\mathbf{x}=\\mathbf{e_1}^T\\mathbf{x}=\\left[\\frac{4}{\\sqrt{17}}\\quad \\frac{1}{\\sqrt{17}}\\right] \\left[\\begin{array}{c} 1\\\\2\\\\ \\end{array} \\right]=\\frac{4}{\\sqrt{17}}\\times 1+\\frac{1}{\\sqrt{17}}\\times 2=1.455 \\] [(d)] Given an observation \\(\\mathbf{x}=[1 \\quad 2]^T\\) and \\(\\mathbf{\\mu}=\\mathbf{0}\\), calculate its factor score on the first common factor. It can be shown that the weighted least-square estimate of the factor score is given by \\[ \\hat {\\mathbf{f}}=(\\mathbf{L}^T\\mathbf{\\Psi}^{-1}\\mathbf{L})^{-1}\\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu}). \\] Therefore, the factor score on the first common factor is \\[ \\hat {\\mathbf{f}}=(\\mathbf{L}^T\\mathbf{\\Psi}^{-1}\\mathbf{L})^{-1}\\mathbf{L}^T\\mathbf{\\Psi}^{-1}(\\mathbf{x}-\\mathbf{\\mu})=\\left([4 \\quad 1]\\left[ \\begin{array}{rr} 1&amp;0\\\\ 0&amp;1 \\end{array} \\right]^{-1} \\left[\\begin{array}{c} 4\\\\1\\\\ \\end{array} \\right]\\right)^{-1} [4 \\quad 1] \\left[ \\begin{array}{rr} 1&amp;0\\\\ 0&amp;1 \\end{array} \\right]^{-1} \\left[\\begin{array}{c} 1-0\\\\2-0\\\\ \\end{array} \\right]=\\frac{6}{17}= 0.353 \\] Revisit the Learning Outcomes After finishing this note, students should be able to Write down the model for factor analysis and explain each term in the model. Obtain the factor loadings, communality, and specific variance of a certain variable \\(X_i\\), the proportion of variance explained by a common factor based on the computer outputs. Determine the proper number of common factors. Explain the ideas for estimating the factor loadings and the specific variance: the principle component and maximum likelihood methods. Apply the Newton-Raphson method to find the solutions of a simple equation. Explain why factor rotation is needed in factor analysis. Obtain/derive the factor scores using the weighted least squares method. "],["discriminant-analysis-and-classification.html", "7 Discriminant Analysis and Classification Learning Outcomes 7.1 Introduction 7.2 Performance Measure 7.3 Overfitting and Cross Validation 7.4 Classification Models 7.5 Logistic Regression for Binary Response 7.6 Logistic Regression for Multi-class Nominal Data 7.7 Cumulative Logit Model for Multi-class Ordinal Data 7.8 Model Selection for Logistic Regression 7.9 Model Checking 7.10 Classification Tree (Recursive Partitioning) 7.11 Regression Tree 7.12 Random Forest 7.13 Support Vector Machines 7.14 Neural Networks 7.15 Classical Methods 7.16 Summary Revisit Learning Outcomes", " 7 Discriminant Analysis and Classification Learning Outcomes After finishing this chapter, students should be able to Use proper performance metrics to compare classification methods. Use cross-validation method to fit classification models. Explain the main idea of each classification method covered: K-nearest neighbor, logistic regression, classification tree, random forests, neural network, support vector machines, and Fisher’s LDA and QDA. Use R to fit the models listed above and interpret the computer outputs. 7.1 Introduction Discriminant analysis and classification is one of most important applications in multivariate analysis. In a discriminant analysis and classification, each observation consists of a vector of explanatory variables \\(\\mathbf{x}=\\{x_1, x_2, \\cdots, x_p\\}\\) and one categorical response variable \\(y\\in \\{1, 2, \\cdots, k\\}\\) indicating which class this observation belongs to. The main objectives of a classification problem is to build a discriminant function to separate the observations into \\(k\\) categories and classify new observations to one of the \\(k\\) classes. Here are two examples: Classify the Iris flowers. There are three species, each flower has four measurements: sepal length and sepal width, petal length and petal width. Classify emails and spams. The data set has 4601 messages and each message has 57 explanatory variables: frequencies of 48 words such as remove, you edu; frequencies of 6 characters such as $, ! and other 3 variable telling the average length of capital letters and total number of capital letters in the email. The question of interest is whether we could build a model to predict a message’s probability of being a spam, i.e., a spam filter. The main idea of building the discriminant function is to model \\(P(Y=j|\\mathbf{x}), j=1, 2, \\cdots, k\\) and assign the observation to the class yielding the largest probability. 7.2 Performance Measure There are many classification methods, when it comes to compare the performance of different methods, we use the misclassification rate which is defined as the proportion of objects that are classified to a wrong group. The misclassification table for a binary case can be presented as a 2\\(\\times\\) 2 table: \\[\\begin{array}{|c|c|c|} \\hline \\text{True}&amp;\\text{Classification result} &amp; \\text{Classification result}\\\\ \\text{label}&amp;1&amp;0\\\\ \\hline 1&amp;TP \\text{(true positive)} &amp; FN \\text{(false negative)}\\\\ \\hline 0&amp;FP \\text{(false positive)}&amp;TN \\text{(true negative)}\\\\ \\hline \\end{array}\\] And the misclassification rate is defined as \\[ \\mbox{Misclassification Rate}=\\frac{\\mbox{FN}+\\mbox{FP}}{\\mbox{TP}+\\mbox{FN}+\\mbox{FP}+\\mbox{TN}}. %=1-\\frac{\\mbox{sum of diagonal elements}}{n}, \\] The smaller the misclassification rate the better the classifier. The definition of the misclassification rate can be generalized to more than two classes. \\[ \\mbox{Misclassification Rate}=1-\\mbox{accuracy}=1-\\frac{\\mbox{sum of diagonal elements}}{n}, \\] where \\(n=\\mbox{FN}+\\mbox{FP}+\\mbox{TP}+\\mbox{TN}\\) is the sample size. Misclassification rate, however, is not appropriate performance measure when the classes are extremely unbalanced. Take the following confusion table for example, \\[\\begin{array}{|c|c|c|c|} \\hline \\text{True}&amp;\\text{Classification Result}&amp;\\text{Classification Result}\\\\ \\text{label}&amp;1&amp;0&amp;\\text{Total}\\\\ \\hline 1&amp;30&amp;20&amp;50\\\\ \\hline 0&amp;40&amp;1960&amp;2000\\\\ \\hline \\text{Total}&amp;70&amp;1980&amp;2050\\\\ \\hline \\end{array}\\] the misclassification rate for this classifier is \\[ \\mbox{MR}=\\frac{\\mbox{FN}+\\mbox{FP}}{\\mbox{TP}+\\mbox{FN}+\\mbox{FP}+\\mbox{TN}}=\\frac{40+20}{2050}=\\frac{60}{2050} \\] which is even larger than the misclassification rate \\(\\frac{50}{2050}\\) if we simply classify all observations as class-0. The classifier, however, is useful in that it correctly classifies 30 out of 50 class-1 observations. Note that number of class-0 observations (\\(n_0\\)) is much larger than the number of class-1 observations (\\(n_1\\)) in this example; therefore, misclassification rate is not proper for this example. Another popular performance metric for classification problem is the receiver operating characteristic (ROC) curve or more precisely, the area under the ROC curve (AUC). The ROC curve plots the true positive versus false positive. The following figure illustrates several typical ROC curves. Left panel: the model has no discriminant power, it cannot separate the two classes. The area under the ROC curve is close to 0.5. The model has the same effect as random ranking. Middle panel: the model has a good discriminant power, it separates the two classes quite well. The area under the ROC curve is 0.8726. Right panel: the model has a perfect discriminant power, it separates the two classes perfectly. The area under the ROC curve is close to 1. The area under the ROC curve equals the probability of correctly ranking a random class-1 and class-0 pair. The larger the area the better. If a classifier assigns score to the subjects randomly or by guessing, the AUC is 0.5, any classifier with an AUC below 0.5 is useless. The perfect classifier has AUC 1. 7.3 Overfitting and Cross Validation There are some tuning parameters in a discriminant function needed to estimate using the data. We need to watch out for overfitting when fitting the models. It is easier to explain the idea of overfitting using regression models for a continuous variable. For example in Figure ??, I want to model the relationship between \\(x\\) and \\(y\\) based on the black dots. Which model is the best and how well does the model in predicting new observations? The first model is just a simple linear regression (a straight line), the second one is a quadratic regression with the \\(x^2\\) term; the last one goes through all data points. The last model fits the black dots the best since all the data points are on the lines, however, it might not good for predicting new observations. As for a prediction, the quadratic regression does a better job, therefore, we say the first model underfits the data and the last one overfits the data. Figure 7.1: Overfitting/Underfitting Problem Figure 7.2: Overfitting/Underfitting Problem Figure 7.3: Overfitting/Underfitting Problem In order to avoid overfitting, we use the idea of cross-validation when we choose the optimal values of the parameters in the model. The key idea is to train or build the model using part of the data and evaluate using the hidden data. Choose the value of the parameter which gives the smallest cross-validation error rate. We will see one example later. If each time you remove only one observation, this is called leave-one-out cross-validation. If the data set is huge, leave-one-out is too time-consuming, you can use \\(k\\)-fold cross-validation, and divide the data into \\(k\\) folds with the same number of observations, the same distribution of the classes. Fit the model using the \\(k-1\\) folds of the data and use the remaining fold for evaluation. Pick the one that gives the smallest error rate. 7.4 Classification Models The classification models covered in this note are \\(K\\) nearest neighbors (KNN), logistic regression, recursive partitioning (classification tree), random forests, neural network (NNet), and support vector machine (SVM). 7.4.1 \\(K\\) Nearest Neighbors Assign the unlabeled point to the majority class among its \\(k\\)-nearest neighbors. Euclidean distance is one measure to find the nearest neighbors. Figure 7.4 shows the basic idea of the KNN method. Suppose there are two classes: red triangle and blue square, want to assign the black circle to one of the two classes. When \\(k=5\\), among the five nearest neighbors, four of them are red triangles; therefore, \\(P_{\\mbox{red}}=\\frac{4}{5}\\) and \\(P_{\\mbox{black}}=\\frac{1}{5}\\). As a result, we should classify the black circle as a red triangle. Figure 7.4: Basic Idea of KNN Two classes: red triangle and blue square. Need to assign the black circle to one of the two classes. When \\(k=5\\), \\(P_{\\mathbf{red}}=\\frac{4}{5}\\). When \\(k=8\\), \\(P_{\\mathbf{red}}=\\frac{7}{8}\\). A cluster of blue square on upper-left corner. Cross validation can be used to choose the optimal value of \\(k\\): the number of nearest of neighbors. The algorithm (with leave-one-out cross validation) is as follows: Split the data into training and testing sets using stratified sampling. Loop over a grid of values of \\(k\\), the number of neighbors. For example, \\(k=1, 3, 5, 7, 9, \\cdots, n\\) (only consider odd values to avoid ties){ Loop over \\(i=1, 2, \\cdots, n\\) (all observations){ Predict \\(y_i\\) with \\(x_i\\) omitted using KNN} Given the value of \\(k\\), find the \\(k\\) nearest neighbors for each observation in the training set, and assign it to the majority class of its neighbors. Calculate the misclassification rate for the current value of \\(k\\) } Plot error rate (or accuracy) versus \\(k\\), choose the value of \\(k\\) that gives the smallest error rate. Take the Iris data for example, there are three species, 50 flowers for each species. Divide the data into training (80% i.e., 120 flowers, 40 from each species) and test sets (20%, 10 from each species) using stratified sampling. Fit the model on the training set, the plot shows that when \\(k=6, 9, 11\\) or \\(k=16\\) the KNN model gives the smallest error (or largest accuracy). R outputs \\(k=16\\) as the optimal value of \\(k\\). In practice, we can try all the four optimal \\(k\\) values and pick the one gives the largest accuracy. Apply the KNN method on the test data with \\(k=16\\), we obtain the confusion table and the resulting accuracy is \\(\\frac{10+9+10}{30}=0.97\\). library(caret) # Load the caret package data(iris) # Load the iris data set y &lt;- iris$Species # Set the response variable x &lt;- iris[,-5] # Set the predictor variables # Split the data into training and testing sets set.seed(6194) splitIndex &lt;- createDataPartition(y, p = .8, list = FALSE) training &lt;- x[splitIndex,] # training set testing &lt;- x[-splitIndex,] # test set training_labels &lt;- y[splitIndex] # response variable in the training set testing_labels &lt;- y[-splitIndex] # response variable in the test set table(training_labels) #stratified sampling for training and testing sets ## training_labels ## setosa versicolor virginica ## 40 40 40 # Train the model using a grid search for best k ctrl &lt;- trainControl(method=&quot;cv&quot;, number=10) # 10-fold cross-validation grid &lt;- expand.grid(k = 1:20) #grid of k model &lt;- train(training, training_labels, method = &quot;knn&quot;, tuneGrid = grid, trControl = ctrl) plot(model) print(model$bestTune$k) # Print the best k value ## [1] 16 #accuracy on the testing set knnfit &lt;- knn3Train(training,testing, training_labels, k=model$bestTune$k) (cm &lt;- table(Predict=knnfit, True=testing_labels)) # confusion matrix ## True ## Predict setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 9 0 ## virginica 0 1 10 accuracy &lt;- sum(diag(cm))/sum(cm) print(accuracy) # Print the accuracy ## [1] 0.9666667 7.5 Logistic Regression for Binary Response In multiple regression, We model the conditional mean of \\(Y\\) given \\(x_1, x_2, \\cdots, x_k\\), \\(\\mu_{Y|x_1, x_2, \\cdots, x_k}\\), and its predictors \\(x_1, x_2, \\cdots, x_k\\) using a hyperplane, that is \\[ \\mu_{Y|x_1, x_2, \\cdots, x_k}=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_kx_k \\] The meaning of \\(\\beta_0, \\beta_1, \\cdots, \\beta_k\\): \\(\\beta_0\\): the when \\(x_1=0, x_2=0, \\cdots, x_k=0\\). \\(\\beta_i\\): the change in the when \\(x_i\\) while keeping other predictor variables the same. Similar to the multiple regression, we would like to model \\(P(Y=j|\\mathbf{x})\\) using the explanatory variables \\(\\mathbf{x}=\\{x_1, \\cdots, x_p\\}\\). However, the domain of \\(P(Y=j|\\mathbf{x})\\) is \\([0, 1]\\) which is bounded. The logistic regression models the log odd ratio and the explanatory variables using a regression model. We start with the binary case to introduce the idea. The logistic regression model for a binary response variable is given by \\[ \\ln \\frac{P(Y=\\text{setosa})}{P(Y=0)}=\\ln \\frac{P(Y=1)}{1-P(Y=1)}=\\beta_0+\\beta_1x_1+\\cdots+\\beta_px_p \\Longrightarrow P(Y=1)=\\frac{e^{\\beta_0+\\beta_1x_1+\\cdots+\\beta_px_p}}{1+e^{\\beta_0+\\beta_1x_1+\\cdots+\\beta_px_p}}, \\] the observation is classified as \\(y=1\\) if \\(P(Y=1)\\ge 0.5\\). Interpretation of \\(\\beta_i\\) The term \\(\\frac{P(Y=1)}{P(Y=0)}\\) is called the odds which is the expected number of successes per failure. Logistic regression models the log odds using linear regression terms \\(\\beta_0+\\beta_1x_1+\\cdots+\\beta_px_p\\). Consider a continuous variable \\(x_j\\), if its value increases by 1 unit from \\(a\\) to \\(a+1\\) while keeping all the other variables the same, we have \\[ \\ln \\frac{P(Y=1|x_j=a+1)}{P(Y=0|x_j=a+1)} -\\ln \\frac{P(Y=1|x_j=a)}{P(Y=0|x_j=a)}=\\beta_j\\Longrightarrow \\mbox{odds ratio}=e^{\\beta_j}. \\] The sign of \\(\\beta_j\\) indicates whether the odds ratio increases or decreases. If \\(\\beta_j&gt;0\\), the odds ratio increases as \\(x_j\\) increases; if \\(\\beta_j&lt;0\\), the odds ratio decreases as \\(x_j\\) increases. The magnitude of the coefficient \\(|\\beta_j|\\) represents the size of the effect of the predictor variable on the odds ratio. Larger magnitudes indicate a stronger influence. Interpretation of \\(\\beta_j\\): If \\(\\beta_j&gt;0\\), one unit increase in \\(x_j\\) increases the odds ratio by \\((e^{\\beta_j}-1)\\times 100\\%\\) as all other variables remain the same. If \\(\\beta_j&lt;0\\), one unit increase in \\(x_j\\) reduces the odds ratio by \\((1-e^{\\beta_j})\\times 100\\%\\) as all other variables remain the same. Estimation of \\(\\beta_i\\) The estimation of the regression coefficients \\(\\beta_0, \\beta_1, \\cdots, \\beta_p\\) is based on the likelihood methods and maximum likelihood estimates are obtained using the Newton-Raphson method. Let \\(p_i=P(Y_i=1)\\), then \\(Y_i\\sim \\mbox{Bernoulli}(p_i)\\) and the likelihood function for \\(\\beta\\)s is \\[ L(\\beta_0, \\beta_1, \\cdots, \\beta_p; y_1, y_2, \\cdots, y_n)=\\prod_{i=1}^n p_i^{y_i}(1-y_i)^{1-y_i} \\] and the log-likelihood is \\[ l(\\beta_0, \\beta_1, \\cdots, \\beta_p)=\\sum_{i=1}^n y_i\\ln(p_i)+\\sum_{i=1}^n (1-y_i)\\ln(1-p_i) \\mbox{ with } p_i=\\frac{e^{\\beta_0+\\beta_1x_1+\\cdots+\\beta_px_p}}{1+e^{\\beta_0+\\beta_1x_1+\\cdots+\\beta_px_p}}. \\] To obtain the maximum likelihood estimate of \\(\\beta_j\\) we need to take the partial derivative of \\(l(\\beta_0, \\beta_1, \\cdots, \\beta_p)\\) with respect to \\(\\beta_j\\), and find the solutions for the score equations: \\[ \\frac{\\partial l}{\\partial \\beta_0}=0, \\quad \\frac{\\partial l}{\\partial \\beta_1}=0, \\quad \\cdots, \\quad \\frac{\\partial l}{\\partial \\beta_p}=0. \\] In general, there is no closed form solutions and Newton-Raphson method will be used to obtain the estimates of \\(\\beta\\)s. Apply logistic regression on the Iris data with the last two species and the computer outputs are as follows. iris2 &lt;- iris[iris$Species %in% c(&quot;virginica&quot;, &quot;versicolor&quot;), ] #use the last two species iris2$Species &lt;- droplevels(iris2$Species) #drop the level with no observations set.seed(6194) # Split the data into training and testing sets splitIndex &lt;- createDataPartition(iris2$Species, p = .8, list = FALSE) training &lt;- iris2[splitIndex,] # training set testing &lt;- iris2[-splitIndex,] # test set table(training$Species) #stratified sampling ## ## versicolor virginica ## 40 40 m0 &lt;- glm(Species~.,data &lt;- training,family=&quot;binomial&quot;) #generalized linear ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred There are two warning messages glm.fit: algorithm did not converge glm.fit: fitted probabilities numerically 0 or 1 occurred The “did not converge” issue can be resolved by increasing the number of maximum iterations. mm0 &lt;- glm(Species~.,data &lt;- training,family=&quot;binomial&quot;,maxit=1000) The second warning is generated when the generalized linear model (GLM) that you are fitting to your data has predicted probabilities that are equal to 0 or 1, which can cause problems in the optimization process. This issue can arise when you have a highly imbalanced response variable, or when there are predictor variables with high levels of multicollinearity, or when there are variable separating the two classes perfectly. From the scatter plot matrix, the variables \\(\\texttt{Petal Length}\\) and \\(\\texttt{Petal Width}\\) can separate the two species very well. \\(\\texttt{Sepal Length}\\) and \\(\\texttt{Petal Length}\\) are highly correlated, so are \\(\\texttt{Petal Length}\\) and \\(\\texttt{Petal Width}\\). library(GGally) ggpairs(training[,-5],mapping = aes(color = training[,5])) Remove the variable \\(\\texttt{Petal Length}\\) and refit the logistic model, we have. logitfit &lt;- glm(Species~.,data=training[,-3],family=&quot;binomial&quot;) summary(logitfit) ## ## Call: ## glm(formula = Species ~ ., family = &quot;binomial&quot;, data = training[, ## -3]) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -51.572 27.804 -1.855 0.0636 . ## Sepal.Length 2.901 2.200 1.318 0.1874 ## Sepal.Width -9.146 6.411 -1.427 0.1537 ## Petal.Width 35.481 19.049 1.863 0.0625 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 110.904 on 79 degrees of freedom ## Residual deviance: 10.387 on 76 degrees of freedom ## AIC: 18.387 ## ## Number of Fisher Scoring iterations: 10 logitpred &lt;- predict(logitfit,newdata=testing[,-3], type=&quot;response&quot;) (ltab &lt;- table(Predit=ifelse(logitpred&lt;0.5,&quot;versicolor&quot;,&quot;virginica&quot;),True=testing$Species)) ## True ## Predit versicolor virginica ## versicolor 10 3 ## virginica 0 7 (laccuracy &lt;- sum(diag(ltab))/sum(ltab)) ## [1] 0.85 \\(\\textbf{Example}\\): Binary Logistic Regression for Admission Data data0 &lt;- read.csv(&quot;data/admission.csv&quot;) # head(data0) #show the first six rows of the data set ## admit gre gpa prestige ## 1 0 380 3.61 3 ## 2 1 660 3.67 3 ## 3 1 800 4.00 1 ## 4 1 640 3.19 4 ## 5 0 520 2.93 4 ## 6 1 760 3.00 2 str(data0) #admit is numerical, we need to change it to categorical ## &#39;data.frame&#39;: 400 obs. of 4 variables: ## $ admit : int 0 1 1 1 0 1 1 0 1 0 ... ## $ gre : int 380 660 800 640 520 760 560 400 540 700 ... ## $ gpa : num 3.61 3.67 4 3.19 2.93 3 2.98 3.08 3.39 3.92 ... ## $ prestige: int 3 3 1 4 4 2 1 2 3 2 ... data &lt;- data0 data$admit &lt;- as.factor(data$admit) #convert admit to a factor #fit a logistic regression, treat &quot;prestige&quot; as numerical mlogit &lt;- glm(admit~.,data=data,family = &quot;binomial&quot;) summary(mlogit) ## ## Call: ## glm(formula = admit ~ ., family = &quot;binomial&quot;, data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.340670 1.137622 -2.937 0.00332 ** ## gre 0.002247 0.001092 2.059 0.03953 * ## gpa 0.752857 0.328375 2.293 0.02187 * ## prestige -0.559515 0.127056 -4.404 1.06e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 496.15 on 396 degrees of freedom ## Residual deviance: 456.56 on 393 degrees of freedom ## (3 observations deleted due to missingness) ## AIC: 464.56 ## ## Number of Fisher Scoring iterations: 4 The fitted logistic regression is \\[\\begin{align*} \\ln \\frac{P(admitted)}{P(not)}&amp;=\\beta_0+\\beta_1\\times gre+\\beta_2 \\times gpa+\\beta_3\\times prestige\\\\ &amp;=-3.34067+0.002247\\times gre+0.752857 \\times gpa-0.559515\\times prestige\\\\ &amp;\\Longrightarrow \\frac{P(admitted)}{P(not)}=e^{-3.34067+0.002247\\times gre+0.752857 \\times gpa-0.559515\\tim es prestige}. \\end{align*}\\] All the three predictor variables are statistical significant at the 5% significance level. Interpretation of the coefficients of the fitted logistic regression: the odds ratio \\[ \\frac{\\frac{P(admitted|gre=a+1)}{P(not)}}{\\frac{P(admitted|gre=a)}{P(not)}} \\] equals to \\(e^{0.002247}=1.00225\\) when the gre score increases by 1 given gpa and prestige remaining the same. This means that the odds of being admitted increase by a factor of 1.00225 or increase by \\((1.00225-1)\\times 100\\%=0.225\\%\\) when the gre score increases by 1 given gpa and prestige remaining the same. Similarly, the odds of being admitted increase by a factor of \\(e^{0.752857}=2.123057\\) or increase by 112.3% when the gpa increases by 1 given gre score and prestige remaining the same; the odds of being admitted decreases by a factor of \\(e^{0.559515}=1.749824\\) when the prestige increases by 1 (downgrade from the first rank to the second rank) given gre score and gpa remaining the same. The overall misclassificaiton rate is 0.2922. Let’s predict the chance of being admitted for the first student who has a gre score 380, gpa 3.61 and graduated for 3rd class undergraduate institution. pred1 &lt;- predict(mlogit,data[1,],type &lt;- &quot;response&quot;) #predict P(admit) for 1st student Since \\[ \\frac{P(admitted)}{1-P(admitted)}=e^{-3.34067+0.002247\\times gre+0.752857 \\times gpa-0.559515\\times prestige} \\Longrightarrow \\] \\[P(admitted)=\\frac{e^{-3.34067+0.002247\\times gre+0.752857 \\times gpa-0.559515\\times prestige}}{1+e^{-3.34067+0.002247\\times gre+0.752857 \\times gpa-0.559515\\times prestige}}\\] \\[=\\frac{e^{-3.34067+0.002247\\times 380+0.752857 \\times 3.61-0.559515\\times 3}}{1+e^{-3.34067+0.002247\\times 380+0.752857 \\times 3.61-0.559515\\times 3}}=0.1904.\\] Since \\(P(admitted)=0.1904&lt;0.5\\), we predict this student as “not admitted” which is a correct decision since the first student was not admitted according to the data. We can also confirm the probability of admission for the first student as follows. bvec &lt;- mlogit$coefficients #the coefficients from the logistic regression xvec &lt;- c(1,as.numeric(data[1,-1])) a &lt;- sum(xvec*bvec) (p &lt;- exp(a)/(1+exp(a))) #the same as pred1 ## [1] 0.1903973 pvec &lt;- predict(mlogit,data,type &lt;- &quot;response&quot;) #predicted prob for all tresult &lt;- ifelse(pvec&lt;0.5,0,1) #convert prob to 1 (at least 0.5) or 0 (&lt;0.5) (ltab &lt;- table(True=data[,1],Predict=tresult)) #confusion table ## Predict ## True 0 1 ## 0 253 18 ## 1 98 28 (lmr &lt;- 1-sum(diag(ltab))/sum(ltab)) #misclassification rate ## [1] 0.2921914 Note that 0.5 is not always the proper cut-off. If the data is imbalanced, i.e., one class dominates, the relative frequency of class-1 observations is a better choice for the cut-off. 7.6 Logistic Regression for Multi-class Nominal Data Suppose the categorical response variable has \\(c\\) levels, and arbitrarily let category 1 be the reference level. Logistic regression can be generalized to multi-class as follows. Fit \\(c-1\\) binary logistic regression \\[ \\ln \\frac{P(Y=j)}{P(Y=1)}=\\beta_0^{(j)}+\\beta_1^{(j)}x_1+\\cdots+\\beta_p^{(j)}x_p=\\mathbf{x}\\mathbf{\\beta}^{(j)} \\Longrightarrow P(Y=j)=P(Y=1) e^{\\mathbf{x} \\mathbf{\\beta}^{(j)}}, j=2, 3, \\cdots, c. \\] That is \\[ \\begin{aligned} P(Y=1)&amp;=P(Y=1)\\\\ P(Y=2)&amp;=P(Y=1)\\times e^{\\mathbf{x} \\mathbf{\\beta}^{(2)}}\\\\ P(Y=3)&amp;=P(Y=1)\\times e^{\\mathbf{x} \\mathbf{\\beta}^{(3)}}\\\\ \\vdots&amp;=\\vdots\\\\ P(Y=c)&amp;=P(Y=1)\\times e^{\\mathbf{x} \\mathbf{\\beta}^{(c)}}\\\\ \\end{aligned} \\] Given the fact that \\(\\sum_{j=1}^c P(Y=j)=1\\), we have \\[ P(Y=1)(1+e^{\\mathbf{x} \\mathbf{\\beta}^{(2)}}+e^{\\mathbf{x} \\mathbf{\\beta}^{(3)}}+\\cdots+e^{\\mathbf{x} \\mathbf{\\beta}^{(c)}})=1\\Longrightarrow P(Y=1)=\\frac{1}{1+e^{\\mathbf{x} \\mathbf{\\beta}^{(2)}+}e^{\\mathbf{x} \\mathbf{\\beta}^{(3)}}+\\cdots+e^{\\mathbf{x} \\mathbf{\\beta}^{(c)}}}. \\] Therefore, we can solve for \\[ P(Y=j)=\\frac{e^{\\mathbf{x} \\mathbf{\\beta}^{(j)}}}{1+e^{\\mathbf{x} \\mathbf{\\beta}^{(2)}+}e^{\\mathbf{x} \\mathbf{\\beta}^{(3)}}+\\cdots+e^{\\mathbf{x} \\mathbf{\\beta}^{(c)}}}, j=2, \\cdots, c. \\] Classify the observation to the class with the largest probability. library(nnet) set.seed(6194) splitIndex &lt;- createDataPartition(iris$Species, p = .8, list = FALSE) training &lt;- iris[ splitIndex,] # training set testing &lt;- iris[-splitIndex,] # test set table(training$Species) ## ## setosa versicolor virginica ## 40 40 40 mlogit &lt;- multinom(Species ~ ., data = training, trace=F) summary(mlogit) ## Call: ## multinom(formula = Species ~ ., data = training, trace = F) ## ## Coefficients: ## (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width ## versicolor 25.18619 -2.19761 -15.64359 12.00310 -1.846649 ## virginica -47.58622 -55.41464 -58.71412 98.52301 57.943819 ## ## Std. Errors: ## (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width ## versicolor 625.4250 234.2197 320.0780 30.32423 207.0105 ## virginica 247.3652 171.1546 368.0706 223.88557 180.1804 ## ## Residual Deviance: 1.154569 ## AIC: 21.15457 training[1,] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa Based on the coefficients of \\(\\texttt{mlogit}\\), we have \\[\\ln \\frac{P(Y=\\text{versicolor})}{P(Y=\\text{setosa})}=25.18619-2.19761x_1-15.64359x_2+12.00310x_3-1.846649x_4 \\quad \\Longrightarrow\\] \\[P(Y=\\text{versicolor})=P(Y=\\text{setosa})e^{25.18619-2.19761x_1-15.64359x_2+12.00310x_3-1.846649x_4}=P(Y=\\text{setosa})e^{a_{21}},\\] where \\(x_1=\\text{Sepal Length}, x_2=\\text{Sepal Width}, x_1=\\text{Petal Length}, x_2=\\text{Petal Width}\\). Similarly, \\[\\ln \\frac{P(Y=\\text{virginica})}{P(Y=\\text{setosa})}=-47.58622-55.41464x_1-58.71412x_2+98.52301x_3+57.943819x_4 \\quad \\Longrightarrow\\] \\[P(Y=\\text{virginica})=P(Y=\\text{setosa})e^{-47.58622-55.41464x_1-58.71412x_2+98.52301x_3+57.943819x_4}=P(Y=\\text{setosa})e^{a_{31}}.\\] Given the fact that \\(P(Y=\\text{setosa})+P(Y=\\text{versicolor})+P(Y=\\text{virginica})=1\\), we have \\[P(Y=\\text{setosa})+P(Y=\\text{versicolor})+P(Y=\\text{virginica})=P(Y=\\text{setosa})+P(Y=\\text{setosa})e^{a_{21}}+P(Y=\\text{setosa})e^{a_{31}}=1 \\Longrightarrow\\] \\[P(Y=\\text{setosa})=\\frac{1}{1+e^{a_{21}}+e^{a_{31}}}, \\quad P(Y=\\text{versicolor})=\\frac{e^{a_{21}}}{1+e^{a_{21}}+e^{a_{31}}}, \\quad P(Y=\\text{virginica})=\\frac{e^{a_{31}}}{1+e^{a_{21}}+e^{a_{31}}}.\\] Assign the observation to the class giving the largest probability. For example, the first flower in the training set has \\(x_1=5.1, x_2=3.5, x_3=1.4, x_4=0.2\\), \\[ \\begin{aligned} a_{21}&amp;=25.18619-2.19761x_1-15.64359x_2+12.00310x_3-1.846649x_4 \\\\ &amp;=25.18619-2.19761(5.1)-15.64359(3.5)+12.00310(1.4)-1.846649(0.2) =-24.33918. \\end{aligned} \\] Similarly, \\[ \\begin{aligned} a_{31}&amp;=-47.58622-55.41464x_1-58.71412x_2+98.52301x_3+57.943819x_4\\\\ &amp;=-47.58622-55.41464(5.1)-58.71412(3.5)+98.52301(1.4)+57.943819(0.2)=-386.1793. \\end{aligned} \\] Therefore, \\[ P(Y=\\text{setosa})=\\frac{1}{1+e^{a_{21}}+e^{a_{31}}}=\\frac{1}{1+e^{-24.33918}+e^{-386.1793}}=1; \\] and \\[ P(Y=\\text{versicolor})=\\frac{e^{a_{21}}}{1+e^{a_{21}}+e^{a_{31}}}=\\frac{e^{-24.33918}}{1+e^{-24.33918}+e^{-386.1793}}=2.689191\\times 10^{-11}; \\] and \\[ P(Y=\\text{virginica})=\\frac{e^{a_{31}}}{1+e^{a_{21}}+e^{a_{31}}}=\\frac{e^{-386.1793}}{1+e^{-24.33918}+e^{-386.1793}}=1.925122\\times 10^{-168}. \\] Since \\(P(Y=\\text{setosa})&gt;P(Y=\\text{versicolor})&gt;P(Y=\\text{virginica})\\), we classify the first flower as setosa, which turns out to be a correct decision. The accuracy of multi-class logistic regression on the testing set is 0.93333 which is slightly worst than that of KNN. mlogitp &lt;- predict(mlogit,testing) (mltab &lt;- table(Predict=mlogitp,True=testing$Species)) ## True ## Predict setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 9 1 ## virginica 0 1 9 (accuracy=sum(diag(mltab))/sum(mltab)) ## [1] 0.9333333 7.7 Cumulative Logit Model for Multi-class Ordinal Data When the response variable is ordinal with \\(c\\) levels, we have \\[ P(Y\\le j)=P(Y=1)+P(Y=2)+\\cdots+P(Y=j)=p_1+p_2+\\cdots+p_j, j=1, 2, \\cdots, c. \\] The cumulative probabilities reflect the ordering: \\[ P(Y\\le 1)\\le P(Y\\le 2)\\le \\cdots \\le P(Y\\le c)=1. \\] The cumulative logits are given by \\[ \\text{logit}[P(Y\\le j)]=\\ln\\frac{P(Y\\le j)}{1-P(Y\\le j)}=\\ln \\frac{p_1+p_2+\\cdots+p_j}{p_{j+1}+\\cdots+p_c}, j=1, 2, \\cdots, c-1. \\] Take \\(c=3\\) for example, the two cumulative logits are \\[ \\text{logit}[P(Y \\le 1)]=\\ln \\frac{p_1}{p_2+p_3}; \\quad \\text{logit}[P(Y\\le 2)]=\\ln \\frac{p_1+p_2}{p_3}. \\] For cumulative logit model, we don’t need to model \\(P(Y\\le c)\\) since \\(P(Y\\le c)=1\\). 7.7.1 Cumulative Logit Models with Proportional Odds A cumulative logit model for level \\(j\\) can be treated as a binary logistic regression model in which categories 1 to \\(j\\) combine to form one category and categories \\(j+1\\) to \\(c\\) form the other. The cumulative logit model with only one explanatory variable \\(x\\) is given by \\[ \\text{logit}[P(Y \\le j)]=\\alpha_j+\\beta x, j=1, 2, \\cdots, c-1. \\] The parameter \\(\\beta\\) quantifies the effect of \\(x\\) on the log odds of resulting in category \\(j\\) or below; and the effect is the same for all \\(j=1, 2, \\cdots, c-1\\). The interpretation of the coefficients of the fitted proportional odds regression model is as follows: Interpretation of \\(\\alpha_j\\). For a fixed value of \\(x\\), \\[ \\text{logit} P(Y\\le j|x=a)]-\\text{logit} P(Y \\le i|x=a)=(\\alpha_j+\\beta a)-(\\alpha_i+\\beta a)=\\alpha_j-\\alpha_i, j&gt;i \\] which implies \\[ \\frac{P(Y\\le j)/P(Y&gt;j)}{P(Y\\le i)/P(Y&gt;i)}=e^{a_j-a_i}. \\] That is the odds of resulting in category \\(j\\) or below is \\(e^{\\alpha_j-\\alpha_i}\\) times of the odds of resulting in category \\(i\\) or below given that \\(x\\) is the same. Interpretation of \\(\\beta\\). With one unit increase in \\(x\\), \\[ \\logit P(Y\\le j|x=a+1)-\\logit P(Y\\le j|x=a)=[\\alpha_j+\\beta (a+1)]-[\\alpha_j+\\beta a]=\\beta \\] which implies \\[ \\frac{P(Y\\le j|x=a+1)/P(Y&gt;j|x=a+1)}{P(Y\\le j|x=a)/P(Y&gt;j|x=a)}=e^{\\beta}. \\] This means the odds of resulting in category \\(j\\) or below increases by \\((e^{\\beta}-1)\\times 100\\%\\) when \\(x\\) increases by 1 unit. The change is the same for each category \\(j\\); this property is called proportional odds. If \\(\\beta&gt;0\\) then the probability to fall into a lower category increases with increasing \\(x\\). This is against the usual interpretation, positive slope is associated with a positive correlation. For this reason, some statistical software models \\(\\text{logit}(P(Y\\le j))=\\alpha_j-\\beta x\\) and reports the negative of the slope. Make sure that you read the documentation of the built-in functions. For example in R, \\(\\texttt{polr}\\) function in package \\(\\texttt{MASS}\\) reports the negative of the slope. However, the \\(\\texttt{vglm}\\) function in package \\(\\texttt{VGAM}\\) reports the positive of the slope. In general, logistic regression for ordinal response is given by \\[ \\logit[P(Y \\le j)]=\\alpha_j+\\beta_1 x_1+\\beta_2x_2+\\cdots+\\beta_kx_k, j=1, 2, \\cdots, c-1, \\] the intercept \\(\\alpha\\) varies but the slopes \\(\\beta_i\\)s are the same for all categories \\(j=1, 2, \\cdots c-1\\). This model has \\((c-1)+k\\) parameters, much smaller than the number of parameters of the baseline logistic regression for nominal response which uses different intercept and slopes \\(\\beta_i\\)s for each categories \\(j=2, \\cdots c\\) given that \\(c=1\\) is the reference level. The number of parameters in a baseline logistic regression model is \\((c-1)(k+1)\\). 7.7.2 Model Probability of Each Category Given that \\[ \\text{logit}[P(Y \\le j)]=\\alpha_j+\\beta_1 x_1+\\beta_2x_2+\\cdots+\\beta_kx_k, j=1, 2, \\cdots, c-1, \\] we have \\[ P(Y \\le j)=\\frac{e^{\\alpha_j+\\beta_1 x_1+\\beta_2x_2+\\cdots+\\beta_kx_k}}{1+e^{\\alpha_j+\\beta_1 x_1+\\beta_2x_2+\\cdots+\\beta_kx_k}}, j=1, 2, \\cdots, c-1. \\] Therefore, the probability of each category can be calculated as \\[ P(Y=j)=P(Y\\le j)-P(Y\\le j-1), j=2, 3, \\cdots, c. \\] Note that \\(P(Y=1)=P(Y\\le 1)\\). We assign the observation to the category with the largest probability, i.e., \\[ \\hat y=\\mbox{argmax}_j P(Y=j). \\] \\(\\textbf{Example}\\): Proportional Odds Model for Ordinal Response Consider the heart disease data with variables age : Age of the patient sex : Sex of the patient (1 = male; 0 = female) cp : Chest Pain type Value 0: asymptomatic Value 1: non-anginal pain Value 2: atypical angina Value 3: typical angina trtbps : resting blood pressure (in mm Hg) chol : cholesterol in mg/dl fetched via BMI sensor fbs : (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false) restecg : resting electrocardiographic results Value 0: normal Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV) Value 2: showing probable or definite left ventricular hypertrophy by Estes’ criteria thalach : maximum heart rate achieved exang: exercise induced angina (1 = yes; 0 = no) oldpeak: ST depression induced by exercise relative to rest. ST relates to positions on the ECG plot. slp: the slope of the peak exercise ST segment Value 1: upsloping Value 2: flat Value 3: downsloping ca: number of major vessels (0-3) thal: A blood disorder called thalassemia Value 3: normal blood flow Value 6: fixed defect (no blood flow in some part of the heart) Value 7: reversible defect (a blood flow is observed but it is not normal) output : heart disease (0-4, 0=no presence) Fit a proportional odds model on the response variable \\(\\texttt{output}\\) which has five levels (0-4) using \\(\\texttt{sex}\\) and number of major vessels with blockage \\(\\texttt{ca}\\) as the predictor variables. We can use the \\(\\texttt{polr}\\) function in R package \\(\\texttt{MASS}\\) to fit a proportional odds logistic regression. Note that the \\(\\texttt{polr}\\) function reports negative of the slope. hdf &lt;- read.csv(&quot;data/heart_disease.csv&quot;) hdf$sex &lt;- as.factor(hdf$sex) #convert integer to factor hdf$exang &lt;- as.factor(hdf$exang) hdf$cp &lt;- as.factor(hdf$cp) hdf$restecg &lt;- as.factor(hdf$restecg) hdf$fbs &lt;- as.factor(hdf$fbs) hdf$slp &lt;- as.factor(hdf$slp) hdf$thal &lt;- as.factor(hdf$thal) #hdf$output &lt;- as.factor(hdf$output) head(hdf) #show the first 6 observations ## age sex cp trtbps chol fbs restecg thalachh exang oldpeak slp ca thal output ## 1 63 1 3 145 233 1 2 150 0 2.3 3 0 6 0 ## 2 67 1 0 160 286 0 2 108 1 1.5 2 3 3 2 ## 3 67 1 0 120 229 0 2 129 1 2.6 2 2 7 1 ## 4 37 1 1 130 250 0 0 187 0 3.5 3 0 3 0 ## 5 41 0 2 130 204 0 2 172 0 1.4 1 0 3 0 ## 6 56 1 2 120 236 0 0 178 0 0.8 1 0 3 0 library(MASS) pofit &lt;- polr(as.factor(output)~sex+ca,data=hdf,Hess=T) #return the Hessian matrix summary(pofit) ## Call: ## polr(formula = as.factor(output) ~ sex + ca, data = hdf, Hess = T) ## ## Coefficients: ## Value Std. Error t value ## sex1 1.079 0.2782 3.877 ## ca 1.151 0.1359 8.466 ## ## Intercepts: ## Value Std. Error t value ## 0|1 1.6434 0.2614 6.2858 ## 1|2 2.7185 0.2943 9.2386 ## 2|3 3.6417 0.3267 11.1475 ## 3|4 5.3086 0.4260 12.4608 ## ## Residual Deviance: 665.1239 ## AIC: 677.1239 ## (4 observations deleted due to missingness) The fitted model is \\[ \\begin{aligned} \\text{logit} P(Y\\le 0)&amp;=1.6434-1.079 x_1-1.151 x_2\\\\ \\text{logit} P(Y\\le 1)&amp;=2.7185-1.079 x_1-1.151 x_2\\\\ \\text{logit} P(Y\\le 2)&amp;=3.6417-1.079 x_1-1.151 x_2\\\\ \\text{logit} P(Y\\le 3)&amp;=5.3086-1.079 x_1-1.151 x_2\\\\ \\end{aligned} \\] with \\[ x_1=\\left\\{ \\begin{array}{ll} 1&amp;\\mbox{Male},\\\\ 0&amp;\\mbox{Female}. \\end{array} \\right. \\quad x_2=\\text{Ca (number of major vessels with blockage)}. \\] The label \\(\\texttt{0|1}\\) shows the fact that the odds corresponding to \\(\\frac{P(Y\\le 0)}{P(Y&gt;0)}\\) which is equivalent to \\(\\frac{P(Y\\le 0)}{P(Y\\ge 1)}\\). Similarly, the label \\(\\texttt{1|2}\\) corresponds to \\(\\frac{P(Y\\le 1)}{P(Y&gt;1)}=\\frac{P(Y\\le 1)}{P(Y\\ge 2)}\\). The same rule applies to all remaining labels. We can also get confidence intervals for the parameter estimates. These can be obtained either by profiling the likelihood function or by using the standard errors and assuming a normal distribution. Note that profiled CIs are not symmetric (although they are usually close to symmetric). If the 95% CI does not cross 0, the parameter estimate is statistically significant. ci &lt;- confint(pofit) exp(cbind(OR = coef(pofit), ci)) ## OR 2.5 % 97.5 % ## sex1 2.940876 1.725483 5.152109 ## ca 3.159960 2.432623 4.147272 Interpretation of the coefficients: \\(\\hat \\beta_1=1.079\\): the odds of heart disease (i.e., 1-4 versus 0) for males is 2.94 times of the odds for females while holding \\(\\texttt{Ca}\\) constant. \\(\\hat \\beta_2=1.151\\): For every one unit increase in Ca (number of major vessels with blockage) the odds of heart disease (1-4 versus 0) is multiplied 3.16 times (i.e., increases 216%) while holding constant \\(\\texttt{sex}\\) variable. Probability of Each Category We choose several subjects to predict their categories of heart disease status (0-4). (testdf &lt;- hdf[c(1,2,5,7),c(&quot;sex&quot;,&quot;ca&quot;,&quot;output&quot;)]) ## sex ca output ## 1 1 0 0 ## 2 1 3 2 ## 5 0 0 0 ## 7 0 2 3 predict(pofit,testdf,type=&quot;p&quot;) #probability of each category for each individual ## 0 1 2 3 4 ## 1 0.6375414 0.19996170 0.09094055 0.05721049 0.01434581 ## 2 0.0528015 0.08760571 0.15098063 0.39389758 0.31471458 ## 5 0.8379993 0.10010868 0.03635438 0.02061294 0.00492470 ## 7 0.3412560 0.26159419 0.18974070 0.16031809 0.04709106 predict(pofit,testdf) #return the category label ## [1] 0 3 0 0 ## Levels: 0 1 2 3 4 For the first subject who is a male (\\(x_1=1\\)) and has no major vessel with blockage (\\(x_2=0\\)), his probability of heart disease status is \\[ \\begin{aligned} P(Y\\le 0)&amp;=\\frac{e^{1.6434-1.079 x_1-1.151 x_2}}{1+e^{1.6434-1.079 x_1-1.151 x_2}}=\\frac{e^{1.6434-1.079\\times 1-1.151\\times 0}}{1+e^{1.6434-1.079\\times 1-1.151\\times 0}}=0.6375\\\\ P(Y\\le 1)&amp;=\\frac{e^{2.7185-1.079 x_1-1.151 x_2}}{1+e^{2.7185-1.079 x_1-1.151 x_2}}=\\frac{e^{2.7185-1.079 \\times 1-1.151 \\times 0}}{1+e^{2.7185-1.079 \\times 1-1.151 \\times 0}}=0.8375\\\\ P(Y\\le 2)&amp;=\\frac{e^{3.6417-1.079 x_1-1.151 x_2}}{1+e^{3.6417-1.079 x_1-1.151 x_2}}=\\frac{e^{3.6417-1.079 \\times 1-1.151 \\times 0}}{1+e^{3.6417-1.079 \\times 1-1.151 \\times 0}}=0.9284\\\\ P(Y\\le 3)&amp;=\\frac{e^{5.3086-1.079 x_1-1.151 x_2}}{1+e^{5.3086-1.079 x_1-1.151 x_2}}=\\frac{e^{5.3086-1.079 \\times 1-1.151 \\times 0}}{1+e^{5.3086-1.079 \\times 1-1.151 \\times 0}}=0.9857\\\\ \\end{aligned} \\] which gives \\[ \\begin{aligned} P(Y=0)&amp;=P(Y\\le 0)=0.6375\\\\ P(Y=1)&amp;=P(Y\\le 1)-P(Y\\le 0)=0.8375-0.6375=0.2\\\\ P(Y=2)&amp;=P(Y\\le 2)-P(Y\\le 1)=0.9284-0.8375=0.0909\\\\ P(Y=3)&amp;=P(Y\\le 3)-P(Y\\le 2)=0.9857-0.9284=0.0573\\\\ P(Y=4)&amp;=P(Y\\le 4)-P(Y\\le 3)=1-0.9857=0.0143\\\\ \\end{aligned} \\] Since \\(P(Y=0)\\) has the largest value, we predict the first subject’s disease status as 0, which is a correct prediction. Similarly, for the 4th subject who is a female (\\(x_1=0\\)) with 2 major vessels blocked (\\(x_2=2\\)), her probability of each disease status can be calculated as \\[ \\begin{aligned} P(Y\\le 0)&amp;=\\frac{e^{1.6434-1.079 x_1-1.151 x_2}}{1+e^{1.6434-1.079 x_1-1.151 x_2}}=\\frac{e^{1.6434-1.079\\times 0-1.151\\times 2}}{1+e^{1.6434-1.079\\times 0-1.151\\times 2}}=0.3411\\\\ P(Y\\le 1)&amp;=\\frac{e^{2.7185-1.079 x_1-1.151 x_2}}{1+e^{2.7185-1.079 x_1-1.151 x_2}}=\\frac{e^{2.7185-1.079 \\times 0-1.151 \\times 2}}{1+e^{2.7185-1.079 \\times 0-1.151 \\times 2}}=0.6026\\\\ P(Y\\le 2)&amp;=\\frac{e^{3.6417-1.079 x_1-1.151 x_2}}{1+e^{3.6417-1.079 x_1-1.151 x_2}}=\\frac{e^{3.6417-1.079 \\times 0-1.151 \\times 2}}{1+e^{3.6417-1.079 \\times 0-1.151 \\times 2}}=0.7924\\\\ P(Y\\le 3)&amp;=\\frac{e^{5.3086-1.079 x_1-1.151 x_2}}{1+e^{5.3086-1.079 x_1-1.151 x_2}}=\\frac{e^{5.3086-1.079 \\times 0-1.151 \\times 2}}{1+e^{5.3086-1.079 \\times 0-1.151 \\times 2}}=0.9529\\\\ \\end{aligned} \\] which gives \\[ \\begin{aligned} P(Y=0)&amp;=P(Y\\le 0)=0.3411\\\\ P(Y=1)&amp;=P(Y\\le 1)-P(Y\\le 0)=0.6026-0.3411=0.2615\\\\ P(Y=2)&amp;=P(Y\\le 2)-P(Y\\le 1)=0.7924-0.6026=0.1898\\\\ P(Y=3)&amp;=P(Y\\le 3)-P(Y\\le 2)=0.9529-0.7924=0.1605\\\\ P(Y=4)&amp;=P(Y\\le 4)-P(Y\\le 3)=1-0.9529=0.0471\\\\ \\end{aligned} \\] Since \\(P(Y=0)\\) has the largest value, we predict the 4th subject’s disease status as 0 which turns out to be an error, the true disease status is 3. This might be due to the fact that only \\(\\texttt{sex}\\) and \\(\\texttt{ca}\\) are used to fit the model. 7.8 Model Selection for Logistic Regression Similar multiple linear regression, we can use forward selection and backward elimination methods to choose the “best” subset of predictor variables to include in the logistic regression model. The Akaike information criterion (AIC) and the Bayesian Information Criterion (BIC) can be used to determine the “best” GLM. Consider two nested models, one of which is more complicate than the other. The more complicate model has more terms and might fit the data closely, i.e., it has smaller bias but larger variance. On the other hand, the simpler model has larger bias but smaller variance. Therefore, it is not necessarily better to select the more complicate model. The best is in the sense that we only include those important variables. Ideally we would like to have a logistic regression model that is simple but still able to capture most of the variation in the response variable. Because a simple model with fewer predictor variable is easy to interpret and maintains reasonable accuracy when applied to new data. 7.8.1 AIC and BIC The Akaike information criterion (AIC) and the Bayesian Information Criterion (BIC) balance the trade-off between bias and variance and account for both the likelihood and model complexity. The AIC is defined as \\[ \\mbox{AIC}=-2(\\mbox{log likelihood})+2(\\mbox{number of parameters in model})=-2l+2p. \\] where \\(l\\) is the log likelihood and \\(p\\) is the number of parameters in the model. And the BIC is calculated as \\[ \\mbox{BIC}=-2(\\mbox{log likelihood}) + \\ln(\\mbox{sample size})\\times(\\mbox{number of parameters in model})=-2l+p\\ln (n). \\] In general, \\(\\ln(n)\\ge 2\\); therefore, BIC penalizes more the model complexity. The model with a smaller AIC or BIC is regarded as a better model. Example: AIC and BIC Compare the AIC and BIC of the following two nested models using R: \\[ \\begin{aligned} \\mbox{Model 1}&amp;:\\ln \\frac{p}{1-p}=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4x_1x_2+\\beta_5x_1x_3\\\\ \\mbox{Model 2}&amp;:\\ln \\frac{p}{1-p}=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3\\\\ \\end{aligned} \\] where \\(x_1\\)=resting blood pressure (\\(\\texttt{trtpbs}\\)) and the categorical variable \\(\\texttt{thal}\\) recoded as \\[ x_2=\\left\\{ \\begin{array}{ll} 1&amp;\\mbox{fixed defect},\\\\ 0&amp;\\mbox{Otherwise}. \\end{array} \\right. x_3=\\left\\{ \\begin{array}{ll} 1&amp;\\mbox{reversible defect},\\\\ 0&amp;\\mbox{Otherwise}. \\end{array} \\right. \\] The AIC and BIC for Model 1 are 336.9184 and 359.1611. The AIC and BIC for Model 2 are 333.0621 and 347.8905. Since Model 2 has smaller AIC and BIC, and hence it is regarded as a better model. hdf$output &lt;- as.factor(ifelse(hdf$output&gt;0,1,0)) hm2 &lt;- glm(output~trtbps*thal,data=hdf,family = binomial) #model 1 hm3 &lt;- glm(output~trtbps+thal,data=hdf,family = binomial) #model 2 summary(hm2) ## ## Call: ## glm(formula = output ~ trtbps * thal, family = binomial, data = hdf) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.6184452 1.5219292 -1.720 0.0853 . ## trtbps 0.0105154 0.0115325 0.912 0.3619 ## thal6 1.9025277 4.0923245 0.465 0.6420 ## thal7 1.5684086 2.2373972 0.701 0.4833 ## trtbps:thal6 -0.0001823 0.0300346 -0.006 0.9952 ## trtbps:thal7 0.0061432 0.0169456 0.363 0.7170 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 415.20 on 300 degrees of freedom ## Residual deviance: 324.92 on 295 degrees of freedom ## (2 observations deleted due to missingness) ## AIC: 336.92 ## ## Number of Fisher Scoring iterations: 4 summary(hm3) ## ## Call: ## glm(formula = output ~ trtbps + thal, family = binomial, data = hdf) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.962791 1.067544 -2.775 0.005515 ** ## trtbps 0.013141 0.008003 1.642 0.100598 ## thal6 1.866060 0.537872 3.469 0.000522 *** ## thal7 2.374344 0.287226 8.266 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 415.20 on 300 degrees of freedom ## Residual deviance: 325.06 on 297 degrees of freedom ## (2 observations deleted due to missingness) ## AIC: 333.06 ## ## Number of Fisher Scoring iterations: 4 m1.aic &lt;- -2*logLik(hm2)+2*length(hm2$coefficients) m2.aic &lt;- -2*logLik(hm3)+2*length(hm3$coefficients) c(m1.aic,m2.aic) ## [1] 336.9184 333.0621 m1.bic &lt;- -2*logLik(hm2)+log(length(hm2$y))*length(hm2$coefficients) m2.bic &lt;- -2*logLik(hm3)+log(length(hm3$y))*length(hm3$coefficients) c(m1.bic,m2.bic) ## [1] 359.1611 347.8905 We can also confirm the answer using the built-in functions \\(\\texttt{AIC()}\\) and \\(\\texttt{BIC()}\\). c(AIC(hm2),AIC(hm3)) #AIC ## [1] 336.9184 333.0621 c(BIC(hm2),BIC(hm3)) #BIC ## [1] 359.1611 347.8905 7.8.2 Forward Selection For forward selection, we start with a model contains no predictor variable and include one and only variable into the model in each step. The chosen variable is the one with the largest reduction in AIC. Repeat the steps until the AIC rises. Forward selection can be conducted in R using the built-in function \\(\\texttt{step()}\\). However, the function cannot handle missing values automatically. Remove rows with missing values before using the built-in function. Or we can use the function \\(\\texttt{add1()}\\). Consider the heart disease data, suppose the full model is the one with \\(\\texttt{age}\\), \\(\\texttt{sex}\\), \\(\\texttt{trtpbs}\\) and their interactions. ms0 &lt;- glm(output~1,data=hdf,family=binomial) ms1 &lt;- glm(output~(age+sex+trtbps)^2,data=hdf,family=binomial) forwards &lt;- step(ms0, scope=formula(ms1), direction=&quot;forward&quot;) ## Start: AIC=419.98 ## output ~ 1 ## ## Df Deviance AIC ## + sex 1 393.93 397.93 ## + age 1 402.54 406.54 ## + trtbps 1 411.03 415.03 ## &lt;none&gt; 417.98 419.98 ## ## Step: AIC=397.93 ## output ~ sex ## ## Df Deviance AIC ## + age 1 372.31 378.31 ## + trtbps 1 384.31 390.31 ## &lt;none&gt; 393.93 397.93 ## ## Step: AIC=378.31 ## output ~ sex + age ## ## Df Deviance AIC ## + trtbps 1 368.38 376.38 ## &lt;none&gt; 372.31 378.31 ## + age:sex 1 372.16 380.16 ## ## Step: AIC=376.38 ## output ~ sex + age + trtbps ## ## Df Deviance AIC ## + sex:trtbps 1 360.62 370.62 ## &lt;none&gt; 368.38 376.38 ## + age:sex 1 368.20 378.20 ## + age:trtbps 1 368.36 378.36 ## ## Step: AIC=370.62 ## output ~ sex + age + trtbps + sex:trtbps ## ## Df Deviance AIC ## &lt;none&gt; 360.62 370.62 ## + age:sex 1 359.51 371.51 ## + age:trtbps 1 359.86 371.86 forwards #the final model ## ## Call: glm(formula = output ~ sex + age + trtbps + sex:trtbps, family = binomial, ## data = hdf) ## ## Coefficients: ## (Intercept) sex1 age trtbps sex1:trtbps ## -10.74888 7.68108 0.06065 0.04525 -0.04503 ## ## Degrees of Freedom: 302 Total (i.e. Null); 298 Residual ## Null Deviance: 418 ## Residual Deviance: 360.6 AIC: 370.6 We first fit a logistic regression model with the intercept alone, the resulting AIC is 419.98. Next, we fit every possible model with only one predictor. The model that gives the lowest AIC and also has a statistically significant reduction in AIC compared to the intercept-only model is the one with \\(\\texttt{sex}\\). This model has an AIC of 397.93. Then we consider every possible two-predictor model (\\(\\texttt{sex}\\) plus another predictor variable). The model with \\(\\texttt{age}\\) as the second predictor variable yields the largest reduction in AIC compared to the single-predictor model. The resulting model has an AIC of 378.31. Next, we fit every possible three-predictor model (\\(\\texttt{sex}\\) and \\(\\texttt{age}\\) plus another predictor variable). The model with \\(\\texttt{trtbps}\\) as the third predictor variable has the smallest AIC of 376.38. Next, we consider all possible four-predictor model (\\(\\texttt{sex}\\), \\(\\texttt{age}\\) and \\(\\texttt{trtbps}\\) plus another predictor variable). The model with \\(\\texttt{sex*trtbps}\\) as the 4th predictor variable has the smallest AIC of 370.62. Next, we fit every possible five-predictor model (\\(\\texttt{sex}\\), \\(\\texttt{age}\\), \\(\\texttt{trtbps}\\) and \\(\\texttt{sex*trtbps}\\) plus another variable). It turned out that none of these models reduces the AIC, thus we stop the procedure. The final fitted model is \\[ \\widehat{\\ln \\frac{p}{1-p}}=\\hat \\beta_0+\\hat \\beta_1x_1+\\beta_2x_2+\\hat\\beta_3x_3+\\hat\\beta_4x_1x_3=-10.7489+7.6811x_1+0.0607x_2+0.0453x_3-0.0450x_1x_3, \\] where \\[ x_1=\\left\\{ \\begin{array}{ll} 1&amp;\\mbox{male},\\\\ 0&amp;\\mbox{female}. \\end{array} \\right.,\\quad x_2=\\mbox{age}, \\quad x_3=\\mbox{blood pressure}. \\] We can also summarize the procedure in a table. library(knitr) kable(forwards$anova) #summarize the procedure Step Df Deviance Resid. Df Resid. Dev AIC NA NA 302 417.9821 419.9821 + sex -1 24.049284 301 393.9329 397.9329 + age -1 21.627036 300 372.3058 378.3058 + trtbps -1 3.930785 299 368.3750 376.3750 + sex:trtbps -1 7.753546 298 360.6215 370.6215 We can also use other criteria such as p-value or chi-square scores to select the variable. For example, if we use the likelihood ratio test and add the significant predictor (p-value\\(\\le \\alpha\\)) with the smallest p-value, the procedure is summarized in the following table. flrt &lt;- step(ms0, scope=formula(ms1), direction=&quot;forward&quot;, test=&quot;LRT&quot;) ## Start: AIC=419.98 ## output ~ 1 ## ## Df Deviance AIC LRT Pr(&gt;Chi) ## + sex 1 393.93 397.93 24.0493 9.390e-07 *** ## + age 1 402.54 406.54 15.4466 8.487e-05 *** ## + trtbps 1 411.03 415.03 6.9542 0.008362 ** ## &lt;none&gt; 417.98 419.98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Step: AIC=397.93 ## output ~ sex ## ## Df Deviance AIC LRT Pr(&gt;Chi) ## + age 1 372.31 378.31 21.6270 3.312e-06 *** ## + trtbps 1 384.31 390.31 9.6261 0.001918 ** ## &lt;none&gt; 393.93 397.93 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Step: AIC=378.31 ## output ~ sex + age ## ## Df Deviance AIC LRT Pr(&gt;Chi) ## + trtbps 1 368.38 376.38 3.9308 0.04741 * ## &lt;none&gt; 372.31 378.31 ## + age:sex 1 372.16 380.16 0.1490 0.69954 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Step: AIC=376.38 ## output ~ sex + age + trtbps ## ## Df Deviance AIC LRT Pr(&gt;Chi) ## + sex:trtbps 1 360.62 370.62 7.7535 0.005361 ** ## &lt;none&gt; 368.38 376.38 ## + age:sex 1 368.20 378.20 0.1769 0.674061 ## + age:trtbps 1 368.36 378.36 0.0147 0.903389 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Step: AIC=370.62 ## output ~ sex + age + trtbps + sex:trtbps ## ## Df Deviance AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 360.62 370.62 ## + age:sex 1 359.51 371.51 1.11244 0.2916 ## + age:trtbps 1 359.86 371.86 0.75868 0.3837 kable(flrt$anova) Step Df Deviance Resid. Df Resid. Dev AIC NA NA 302 417.9821 419.9821 + sex -1 24.049284 301 393.9329 397.9329 + age -1 21.627036 300 372.3058 378.3058 + trtbps -1 3.930785 299 368.3750 376.3750 + sex:trtbps -1 7.753546 298 360.6215 370.6215 flrt$coefficients ## (Intercept) sex1 age trtbps sex1:trtbps ## -10.74888498 7.68108021 0.06064530 0.04525432 -0.04502625 7.8.3 Backward Elimination Unlike the forward selection, backward elimination does the other way around. It starts with a model contains all predictor variables. In each step, remove one and only one variable that is least important and has the largest reduction in AIC until AIC starts increasing. P-value, \\(z\\)-score, chi-square score can be used to determine which variable should be removed in each step. Backward elimination can be conducted in R using the built-in function \\(\\texttt{step()}\\). Setting the argument trace=0 tells R not to display the full results of the stepwise selection. We can also use the function \\(\\texttt{drop1()}\\). backward &lt;- step(ms1, direction=&#39;backward&#39;, scope=formula(ms1)) #if trace=0: don&#39;t show the entire procedure ## Start: AIC=372.85 ## output ~ (age + sex + trtbps)^2 ## ## Df Deviance AIC ## - age:trtbps 1 359.51 371.51 ## - age:sex 1 359.86 371.86 ## &lt;none&gt; 358.85 372.85 ## - sex:trtbps 1 368.19 380.19 ## ## Step: AIC=371.51 ## output ~ age + sex + trtbps + age:sex + sex:trtbps ## ## Df Deviance AIC ## - age:sex 1 360.62 370.62 ## &lt;none&gt; 359.51 371.51 ## - sex:trtbps 1 368.20 378.20 ## ## Step: AIC=370.62 ## output ~ age + sex + trtbps + sex:trtbps ## ## Df Deviance AIC ## &lt;none&gt; 360.62 370.62 ## - sex:trtbps 1 368.38 376.38 ## - age 1 376.73 384.73 kable(backward$anova) #summarize the procedure Step Df Deviance Resid. Df Resid. Dev AIC NA NA 296 358.8527 372.8527 - age:trtbps 1 0.6563779 297 359.5090 371.5090 - age:sex 1 1.1124424 298 360.6215 370.6215 backward$coefficients #final model coefficients ## (Intercept) age sex1 trtbps sex1:trtbps ## -10.74888498 0.06064530 7.68108021 0.04525432 -0.04502625 Here is how to interpret the results: First, we fit a model using all three predictors and their interactions. The full model has an AIC of 372.85. Next, remove the interaction term \\(\\texttt{age:trtbps}\\), the resulting model has an AIC of 371.51. Next, remove the interaction term \\(\\texttt{age:age}\\), the resulting model has an AIC of 370.62. Cannot remove any more terms; otherwise AIC rises. The final fitted model by backward elimination is the same as the one given by forward selection. However, this is not always the case. 7.9 Model Checking In multiple linear regression, residuals analysis is used to check the model assumptions. Adjusted \\(R^2\\), mean square error, \\(t\\) test for a single slope, F test for multiple slopes can be applied to test the goodness-of-fit of the model. 7.9.1 Residual Analysis For logistic regression model, the response variable \\(y_i\\) is either 0 or 1 and the fitted value \\(\\hat y_i\\) is a probability between 0 and 1; therefore, the residual \\(e_i=y_i-\\hat y_i\\) is not that well defined as the one in multiple regression where both the observed and fitted values are numerical. Residual analysis for generalized linear model has been implemented in R package \\(\\texttt{DHARMa}\\). The main idea is to create interpretable residuals by simulation for generalized linear models that are standardized to values between 0 and 1. Two plots are generated: Left panel: a QQ-plot to detect overall deviations from the expected distribution. Departure from a linear pattern indicates lack of fit. Right panel: a plot of the residuals against the predicted value (by default). It is highly recommended to plot residuals against a specific other predictors as well. Simulation outliers (data points that are outside the range of simulated values) are highlighted as red stars. More details can be found in this website: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html Let’s try the residual plots on the final logistic model chosen by forward selection method. The residual versus the predictor \\(\\texttt{age}\\) shows some curvature. None of the graphs shows strong evidence against that the model is adequate. library(DHARMa) mf &lt;- glm(output~age+sex+trtbps+sex*trtbps,data &lt;- hdf,family=binomial) res.mf &lt;- simulateResiduals(mf) plot(res.mf) par(mfrow = c(1,3)) plotResiduals(res.mf, hdf$age) plotResiduals(res.mf, hdf$sex) plotResiduals(res.mf, hdf$trtbps) par(mfrow = c(1,1)) 7.9.2 Preditive Power: Accuracy and ROC Curve Accuracy calculated from a confusion table and area under the receiver operator characteristic (ROC) curve can be used to assess a fitted model’s predictive power. For the heart data set, the proportion of heart disease is \\(\\frac{139}{164+139}=0.4587\\). Subjects with a fitted value beyond 0.4587 are classified as diseased. Based on the confusion table, the accuracy of logistic regression with \\(\\texttt{age}\\),\\(\\texttt{sex}\\), \\(\\texttt{trtbps}\\), and interaction between \\(\\texttt{sex}\\) and \\(\\texttt{trtbps}\\) is \\(\\frac{98+103}{98+66+36+103}=0.6634\\). table(hdf$output) #frequency of heart disease (1) or not (0) ## ## 0 1 ## 164 139 (pr &lt;- sum(hdf$output==&quot;1&quot;)/length(hdf$output)) ## [1] 0.4587459 (tab &lt;- table(True &lt;- hdf$output,Predict=as.numeric(mf$fitted.values&gt;pr))) ## Predict ## 0 1 ## 0 98 66 ## 1 36 103 sum(diag(tab))/sum(tab) ## [1] 0.6633663 Several packages in R provide ROC analysis such as \\(\\texttt{pROC}\\), \\(\\texttt{performance}\\) and \\(\\texttt{PRROC}\\). Here is the ROC curve of the logistic regression with \\(\\texttt{age}\\),\\(\\texttt{sex}\\), \\(\\texttt{trtbps}\\), and interaction between \\(\\texttt{sex}\\) and \\(\\texttt{trtbps}\\) with an AUC 0.7369. library(PRROC) mf &lt;- glm(output~age+sex+trtbps+sex*trtbps,data=hdf,family=binomial) haroc &lt;- roc.curve(mf$fitted.values[hdf$output==&quot;1&quot;],mf$fitted.values[hdf$output==&quot;0&quot;],curve=T) plot(haroc,cex.lab=1.5,cex.axis=1.35) library(pROC) rocplot &lt;- roc(output ~ fitted(mf), data=hdf) plot.roc(rocplot, legacy.axes=TRUE) # Specificity on x axis if legacy.axes=F auc(rocplot) ## Area under the curve: 0.7369 7.10 Classification Tree (Recursive Partitioning) The main idea of the classification tree is to recursively partition the explanatory variable space into small rectangles (or cubes) to make the response variable as pure as possible within the rectangles. It picks one variable and one cut value at a time and chooses the cut that maximizes the purity or minimizes the impurity. There are two popular ways to quantify impurity: the Gini index and entropy. Gini index: \\(G=\\displaystyle\\sum_{i=1}^c p_i(1-p_i)\\). Measures total variance across all classes, smaller when \\(p_i\\) is closer to either 0 or 1. Entropy: \\(D=-\\displaystyle\\sum_{i=1}^c p_i \\ln{p_i}\\). \\(D\\approx 0\\) if all \\(p_i\\) are either 1 or 0. \\(\\textbf{Example}\\): Gini Index Suppose there are two partitions: \\(x_1=1.5\\) and \\(x_2=2\\). Calculate the Gini index for these two cuts and explain which cut is better, i.e., gives a smaller Gini index. Keep searching and partitioning until the cut can not further improve the purity or a certain stopping criterion is met, say the number of items in each terminal node is at least 5. The pseudo-code for the algorithm is as follows: Start at the root node. For each explanatory variable \\(X\\), find the set \\(S\\) that minimizes the sum of the node impurities in the two child nodes and choose the split that gives the smallest value. If a stopping criterion is met, exit; otherwise, apply step 2 to each child node in turn. When fitting a classification tree, especially with a large number of explanatory variables, in general we first grow a large tree \\(\\mathbf{T}_0\\) stopping the splitting process only when some stopping criterion is met, say minimum node size is 5. Then the large tree \\(\\mathbf{T}_0\\) is pruned by minimizing \\(C(\\mathbf{T})+\\alpha|\\mathbf{T}|\\), where \\(C(\\mathbf{T})\\) is the error rate of the tree \\(\\mathbf{T}\\) and \\(|\\mathbf{T}|\\) is the size of the tree, \\(\\alpha\\) is the penalizing constant (complexity number). Larger values of \\(\\alpha\\) penalize big trees more and tend to lead to more pruning. Pruning improves the performance of trees. We apply the classification tree on the iris data. Ten-fold cross-validation was used to select the optimal value of the complexity parameter. The graph suggests we should not prune the tree, i.e., \\(\\alpha=0\\). ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10) treem &lt;- train(Species ~ ., data = training, method = &quot;rpart&quot;, trControl = ctrl) plot(treem) To classify new observations, we just follow the paths of the tree. The resulting tree is as follows. It shows \\(\\texttt{Petal.Length}\\) is an important variable to separate the three species. Take the first flower in the training set for example, it has Petal.Length=1.4 which is smaller than 2.5, we classify as setosa. It turns out to be correct. And the accuracy for classification tree is 0.9333. library(rpart) library(rpart.plot) final_tree &lt;- treem$finalModel # Extract the final tree rpart.plot(final_tree) # Plot the tree training[1,] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa # Evaluate the model on the test set predictions &lt;- predict(treem, newdata = testing) ttab &lt;- confusionMatrix(predictions, testing$Species)$table (accuracy=sum(diag(ttab))/sum(ttab)) ## [1] 0.9333333 : Classification Tree on the Spam Data The Spam Email database contains 4601 instances: 2788 emails (\\(y=0\\)) and 1813 spams (\\(y=1\\)). For each instance, 57 explanatory variables: 48 variables indicating the frequencies of 48 words such as edu'',you’’. 6 variables indicating the frequencies of 6 characters such as \\$'',!’’. 3 variables telling the average length of uninterrupted sequences of capital letters, length of longest uninterrupted sequence of capital letters, and total number of capital letters in the e-mail, respectively. We first split the data into 75% for training and 25% for testing. library(caret) spamdf &lt;- read.csv(&quot;data/spam.csv&quot;) #import the data spamdf$y &lt;- as.factor(ifelse(spamdf$y==1,&quot;spam&quot;,&quot;email&quot;)) table(spamdf$y) ## ## email spam ## 2788 1813 set.seed(6194) ind &lt;- createDataPartition(spamdf$y, p=0.75, list=F) #index not in a list train &lt;- spamdf[ind,] #training set test &lt;- spamdf[-ind,] #test set We first fit a classification tree without pruning (cp=0). library(rpart) library(rpart.plot) mt0 &lt;- rpart(y~., data=train, cp=0) #classification tree without pruning rpart.plot(mt0) tab &lt;- mt0$cptable #CP table round(tab, 4) ## CP nsplit rel error xerror xstd ## 1 0.4875 0 1.0000 1.0000 0.0211 ## 2 0.1456 1 0.5125 0.5463 0.0178 ## 3 0.0537 2 0.3669 0.4868 0.0170 ## 4 0.0353 3 0.3132 0.3331 0.0146 ## 5 0.0235 4 0.2779 0.3162 0.0143 ## 6 0.0132 5 0.2544 0.3037 0.0140 ## 7 0.0096 6 0.2412 0.2934 0.0138 ## 8 0.0074 7 0.2316 0.2875 0.0137 ## 9 0.0066 8 0.2243 0.2728 0.0134 ## 10 0.0054 9 0.2176 0.2662 0.0132 ## 11 0.0037 12 0.2015 0.2507 0.0129 ## 12 0.0033 15 0.1904 0.2404 0.0127 ## 13 0.0032 17 0.1838 0.2331 0.0125 ## 14 0.0029 20 0.1743 0.2331 0.0125 ## 15 0.0022 22 0.1684 0.2316 0.0124 ## 16 0.0015 24 0.1640 0.2235 0.0122 ## 17 0.0007 34 0.1478 0.2228 0.0122 ## 18 0.0006 43 0.1412 0.2250 0.0123 ## 19 0.0000 47 0.1390 0.2301 0.0124 The CP (complexity parameter) table gives the following information: The first column \\(\\texttt{CP}\\) is the complexity parameter. It is \\(\\alpha\\) in the objective function \\(C(\\mathbf{T})+\\alpha|\\mathbf{T}|\\). If \\(\\text{CP}=0\\), we don’t penalize the tree size, i.e., we will grow a big tree. The second column \\(\\texttt{nsplit}\\) gives the number of splits. If \\(\\text{nsplit}=4\\), it means the tree has 4 splits, i.e., the resulting tree has 5 terminal nodes. The third column \\(\\texttt{rel error}\\) indicates the impact of adding or removing nodes on the model’s performance. Lower relative error values suggest that the split significantly improves the accuracy, and such splits are favored during the construction of the decision tree. The fourth column \\(\\verb`xerror`\\) is the cross-validated error. It indicates the error rate observed when the model is applied to unseen data. We use the CP value with the smallest cross-validated error to prune the tree. The last column \\(\\verb`xstd`\\) gives the variation of the associated cross-validated error. Smaller xstd values indicate that the cross-validated error rate estimates are more stable and reliable. Based on the CP table, the tree without pruning has (47+1)=48 terminal nodes. The tree with 34 splits (i.e., 35 terminal nodes) gives the smallest cross-validated error (xerror), so we should prune the tree with CP=0.0007. (cp1 &lt;- tab[which.min(tab[,4]),1]) #CP value for the tree with smallest xerror ## [1] 0.0007352941 mt.prune &lt;- prune(mt0,cp=cp1) #prune the tree rpart.plot(mt.prune) Calculate the accuracy of the pruned classification tree. pred_tr &lt;- predict(mt.prune,test) #return prob of each class head(pred_tr) ## email spam ## 1 0.11111111 0.88888889 ## 3 0.02483660 0.97516340 ## 7 0.11538462 0.88461538 ## 14 0.04405286 0.95594714 ## 15 0.00000000 1.00000000 ## 21 0.92307692 0.07692308 (spamtab &lt;- table(True=test$y,Predict=ifelse(pred_tr[,1]&gt;0.5,&quot;email&quot;,&quot;spam&quot;))) ## Predict ## True email spam ## email 656 41 ## spam 53 400 sum(diag(spamtab))/nrow(test) ## [1] 0.9182609 7.11 Regression Tree The recursive partitioning method can be applied to numerical responses as well. The tree model is called . The main difference between a regression tree and a classification tree is the performance measure. For each split, we choose the best cut that gives the smallest sum of squared deviation between the sample mean in each rectangle and the overall sample mean. : Housing Values in Suburbs of Boston The Boston data frame has 506 rows and 14 columns: crim (per capita crime rate by town), zn (proportion of residential land zoned for lots over 25,000 sq.ft.), indus (proportion of non-retail business acres per town), chas (Charles River dummy variable, 1 if tract bounds river; 0 otherwise), nox (nitrogen oxides concentration, parts per 10 million), rm (average number of rooms per dwelling), age (proportion of owner-occupied units built prior to 1940), dis (weighted mean of distances to five Boston employment centres), rad (index of accessibility to radial highways), tax (full-value property-tax rate per $10,000), ptratio (pupil-teacher ratio by town), black (\\(1000(Bk - 0.63)^2\\) where \\(Bk\\) is the proportion of blacks by town), lstat (lower status of the population (percent), medv (median value of owner-occupied homes in $1000s). #library(MASS) bdf &lt;- Boston #the data set is in R kable(head(bdf), caption = &quot;Boston Housing Data&quot;) Table 7.1: Boston Housing Data crim zn indus chas nox rm age dis rad tax ptratio black lstat medv 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 24.0 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 21.6 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 34.7 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 36.2 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 28.7 We want to predict the \\(\\verb`mevd`\\) (median value of the homes) using the explanatory variables such as \\(\\verb`crim`\\), \\(\\verb`rm`\\), \\(\\verb`Istat`\\), and etc. We divide the data into the training and test sets: 2/3 for training and 1/3 for testing. Fit the regression tree model on the training set and check the performance of the fitted model on the test set. set.seed(6194) flds &lt;- createFolds(bdf$medv, k = 3, list = TRUE, returnTrain = FALSE) train &lt;- bdf[-flds[[3]],] test &lt;- bdf[flds[[3]],] mrt &lt;- rpart(medv~.,data=train,cp=0) #fit a big tree without pruning rpart.plot(mrt) #plot the tree without pruning (tab &lt;- mrt$cptable) #CP table ## CP nsplit rel error xerror xstd ## 1 0.4820793362 0 1.0000000 1.0089155 0.10216415 ## 2 0.1349486733 1 0.5179207 0.6088939 0.06594488 ## 3 0.0993411447 2 0.3829720 0.4575943 0.05669686 ## 4 0.0266546315 3 0.2836308 0.3557458 0.05102330 ## 5 0.0241441318 4 0.2569762 0.3160056 0.04840921 ## 6 0.0190860424 5 0.2328321 0.3178338 0.04940566 ## 7 0.0162395766 6 0.2137460 0.3105117 0.04947553 ## 8 0.0085731414 7 0.1975065 0.2923048 0.04654080 ## 9 0.0067351258 8 0.1889333 0.2745256 0.04343057 ## 10 0.0063353872 9 0.1821982 0.2714114 0.04338882 ## 11 0.0060326922 10 0.1758628 0.2690827 0.04605941 ## 12 0.0059133168 11 0.1698301 0.2684482 0.04606931 ## 13 0.0035296589 12 0.1639168 0.2575551 0.04535724 ## 14 0.0027195002 13 0.1603871 0.2578765 0.04539281 ## 15 0.0026739050 14 0.1576676 0.2627563 0.04716961 ## 16 0.0023741669 15 0.1549937 0.2640974 0.04716701 ## 17 0.0019567575 16 0.1526196 0.2631027 0.04717093 ## 18 0.0018613285 17 0.1506628 0.2613908 0.04715654 ## 19 0.0018203937 18 0.1488015 0.2636982 0.04716872 ## 20 0.0016699962 19 0.1469811 0.2633756 0.04720416 ## 21 0.0016019975 20 0.1453111 0.2633518 0.04720498 ## 22 0.0011137696 21 0.1437091 0.2637808 0.04720749 ## 23 0.0006622958 22 0.1425953 0.2640179 0.04729943 ## 24 0.0006258317 23 0.1419330 0.2658967 0.04734873 ## 25 0.0005912388 24 0.1413072 0.2657652 0.04735076 ## 26 0.0005793161 25 0.1407160 0.2657922 0.04735032 ## 27 0.0004705356 26 0.1401366 0.2661397 0.04735135 ## 28 0.0002834765 27 0.1396661 0.2659556 0.04735408 ## 29 0.0000000000 28 0.1393826 0.2659094 0.04735478 (cp1 &lt;- tab[which.min(tab[,4]),1]) #CP value for the tree with smallest xerror ## [1] 0.003529659 mrt.prune &lt;- prune(mrt,cp=cp1) #prune the tree rpart.plot(mrt.prune) #plot the pruned tree #calculate the fitted value by regression tree pvec1 &lt;- predict(mrt.prune,test) kable(test[1,]) crim zn indus chas nox rm age dis rad tax ptratio black lstat medv 4 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 33.4 Based on the pruned regression tree, estimate the median values of homes for the first observation in the test set with the features above. 7.12 Random Forest Random Forest is an ensemble machine learning algorithm that builds multiple decision trees and combines their predictions to improve the accuracy and stability of the model. Random Forest is used for both regression and classification tasks. The basic idea behind Random Forest is to randomly select a subset of the features for each tree in the forest, and to split each node in the tree using the best split among a random subset of the features. This process is repeated many times to create many decision trees, each of which provides a prediction for the target variable. The final prediction is then made by combining the predictions of all the trees, typically by taking the average or the majority vote. Random Forest has several advantages over single decision trees, such as increased accuracy, reduced overfitting, and improved interpretability. Additionally, Random Forest can handle missing data and noisy data better than single decision trees. We apply the random forest on the iris data. Ten-fold cross-validation was used to select the optimal number of variables to build the trees. The graph suggests we should use either two or three out of the four features. The final model uses mtry=2 and the accuracy on the testing set is 0.96667. #library(caret) # cross validation library(randomForest) # Set the control parameters for cross-validation ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10) # Train the Random Forest model using 10-fold cross-validation rfm &lt;- train(Species ~ ., data = training, method = &quot;rf&quot;, trControl = ctrl) # Evaluate the model on the test set predictions &lt;- predict(rfm, newdata = testing) (rftab &lt;- confusionMatrix(predictions, testing$Species)$table) ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 9 0 ## virginica 0 1 10 (accuracy=sum(diag(rftab))/sum(rftab)) ## [1] 0.9666667 One appealing feature of random forest is it provides a measure of the importance of each variable. The outputs below show that \\(\\verb`Petal.Length`\\) is the most important variable to separate the three species. # Extract the variable importance importance &lt;- varImp(rfm$finalModel, scale = FALSE) # Sort the variables by importance and print the result result &lt;- data.frame(Variable = row.names(importance), Importance = importance[, &quot;Overall&quot;]) result &lt;- result[order(-result$Importance), ] print(result) ## Variable Importance ## 3 Petal.Length 41.5612190 ## 4 Petal.Width 35.4531345 ## 1 Sepal.Length 1.3640399 ## 2 Sepal.Width 0.9808732 Like decision tree, random forests can handle both the classification (for categorical response) and regression (for numerical response) problems. \\(\\textbf{Example}\\): Random Forest for Regression on Boston Data We fit a random forest model on the Boston Data and compare the sum of squares of the residuals with regression tree and multiple regression. mrf &lt;- randomForest(medv~.,data=train,importance=T) round(importance(mrf),2) ## %IncMSE IncNodePurity ## crim 11.27 1767.62 ## zn 4.28 228.20 ## indus 9.65 1528.50 ## chas 4.58 229.12 ## nox 14.33 1731.14 ## rm 33.14 8103.81 ## age 12.54 808.21 ## dis 14.86 1440.21 ## rad 6.14 250.90 ## tax 11.69 1004.05 ## ptratio 14.03 1838.41 ## black 8.38 520.24 ## lstat 29.64 9027.53 prf &lt;- predict(mrf,test,type=&quot;response&quot;) #fit a multiple regression mlm &lt;- lm(medv~., data=train) summary(mlm) ## ## Call: ## lm(formula = medv ~ ., data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.8613 -2.7534 -0.5527 1.6387 27.1694 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.217608 6.282701 5.924 8.05e-09 *** ## crim -0.143947 0.043087 -3.341 0.000933 *** ## zn 0.037566 0.016802 2.236 0.026045 * ## indus 0.029849 0.078568 0.380 0.704259 ## chas 3.230047 1.086542 2.973 0.003173 ** ## nox -18.786303 4.823356 -3.895 0.000119 *** ## rm 3.913611 0.513768 7.617 2.89e-13 *** ## age -0.009712 0.016889 -0.575 0.565661 ## dis -1.426398 0.246913 -5.777 1.79e-08 *** ## rad 0.351993 0.087932 4.003 7.76e-05 *** ## tax -0.014395 0.005019 -2.868 0.004403 ** ## ptratio -0.977211 0.165481 -5.905 8.92e-09 *** ## black 0.009158 0.003607 2.539 0.011580 * ## lstat -0.462373 0.066304 -6.974 1.75e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.875 on 323 degrees of freedom ## Multiple R-squared: 0.7356, Adjusted R-squared: 0.725 ## F-statistic: 69.13 on 13 and 323 DF, p-value: &lt; 2.2e-16 #fitted value by the multiple regression pvec2 &lt;- predict(mlm, test) #compare the SSE of regression tree and multiple regression plot(test$medv,pvec2, pch = 17,xlab=&quot;True medv&quot;, ylab=&quot;Predicted medv&quot;, col=&quot;black&quot;,ylim=c(0,max(c(pvec1,pvec2)))) abline(0,1) points(test$medv,pvec1,pch=19,col=&quot;red&quot;) points(test$medv,prf,pch=20,col=&quot;blue&quot;) legend(5,45,c(&quot;Random Forest&quot;,&quot;Regression Tree&quot;,&quot;Multiple Regression&quot;), col=c(&quot;blue&quot;,&quot;red&quot;,&quot;black&quot;),pch=c(20,19, 17)) c(sum((prf-test$medv)^2),sum((pvec1-test$medv)^2),sum((pvec2-test$medv)^2)) ## [1] 1776.262 3318.591 3508.860 7.13 Support Vector Machines Support vector machine (SVM) is a novel learning method originally introduced by Cortes and Vapnik (1995). It includes polynomial classifiers, radial basis function (RBF) networks, and single-layer neural networks as special cases. In a binary classification problem, suppose we have data \\(S=\\{(\\mathbf{x}_1,y_1), \\ldots, (\\mathbf{x}_N,y_N)\\}\\) with \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\{1, -1\\}\\). Notice that the two classes here are \\(\\{1, -1\\}\\) rather than \\(\\{0, 1\\}\\) in a typical binary classification problem. The data are said to be linearly separable if there exists a hyperplane \\(f(\\mathbf{x})=0\\) that perfectly separates the two classes; otherwise, the data are linearly non-separable. The real-valued function \\(f(\\mathbf{x}): \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is called the decision function (DF), which can be written as \\[ \\begin{aligned} f(\\mathbf{x})&amp;=\\langle \\mathbf{w}, \\mathbf{x} \\rangle +b\\\\ &amp;=\\mathbf{w}^T \\mathbf{x}+b. \\end{aligned} \\] A new observation \\(\\mathbf{x}\\) is assigned to class 1 if \\(f(\\mathbf{x})\\ge 0\\) and otherwise to class -1. The parameters \\((\\mathbf{w}, b)\\) can be estimated from the data. When the data are linearly separable, we can find some hyperplane that perfectly separates the two classes. Let the margin of a hyperplane \\((\\mathbf{w},b)\\) be \\(2\\gamma\\), where \\(\\gamma\\) is the shortest distance from the hyperplane to a training point. The objective is to find the hyperplane that produces the largest margin. Figure 7.5 shows two hyperplanes and simulated data points which are linearly separable. Note that each hyperplane is halfway between the two dashed lines. The hyperplane with \\(\\mbox{margin}_1\\) is better than the one with \\(\\mbox{margin}_2\\) in that it has a larger margin. Figure 7.5: Two hyperplances of simulated data points. The solid lines are the separating hyperplanes (or decision boundaries). The hyperplane with the larger margin (1) is better It can be shown that finding the maximal margin hyperplane \\((\\mathbf{w}, b)\\) is equivalent to solving the following optimization problem \\[\\begin{equation} \\label{eq:svm-separable} \\begin{array}{rl} \\displaystyle \\min_{\\mathbf{w}, b} &amp; \\frac{1}{2}\\|\\mathbf{w}\\|^2\\\\ \\mbox{ subject to} &amp; y_i(\\mathbf{w}^T \\mathbf{x}_i +b ) \\ge 1, i=1, \\ldots, N. \\tag{7.1} \\end{array} \\end{equation}\\] When the data are linearly non-separable, no solution exists for the (7.1) above. One way to deal with this problem is to still minimize \\(\\frac{1}{2}\\|\\mathbf{w}\\|^2\\) while relaxing all the constraints by introducing some slack variables \\(\\mathbf{\\xi}=(\\xi_1, \\ldots, \\xi_N)\\). These slack variables allow some observations to be on the wrong side of the margin. The optimization problem for linearly non-separable case is \\[\\begin{equation} \\label{eq:svm-nonseparable} \\begin{array}{rl} \\displaystyle \\min_{\\mathbf{w}, b}&amp; \\frac{1}{2}\\|\\mathbf{w}\\|^2+C\\sum_{i=1}^N \\xi_i\\\\ \\mbox{subject to}&amp;y_i(\\mathbf{w}^T \\mathbf{x}_i +b ) \\ge 1-\\xi_i,\\\\ &amp;\\xi_i \\ge 0, i=1, \\ldots, N,\\tag{7.2} \\end{array} \\end{equation}\\] where \\(C\\) (short for Cost) is a regularization parameter controlling the smoothness of the boundary. A large value of \\(C\\) will discourage any positive \\(\\xi_i\\) and result in an overfit wiggly boundary; a small value of \\(C\\) will lead to an over smooth boundary. This trade-off enables our choosing the optimal value of \\(C\\) by cross-validation. By using the Lagrangian method, both the , both (7.1): and (7.2) can be rephrased as quadratic programming problems with linear inequality constraints It can be shown that the solution for \\(\\mathbf{w}\\) has the form \\[ \\hat {\\mathbf{w}}=\\sum_{i=1}^N \\hat \\alpha_i y_i \\mathbf{x}_i, \\] where \\(\\alpha_i \\ge0\\) are Lagrange multipliers. Those observations with strictly positive coefficients \\(\\alpha_i\\) are called support vectors (SV), since the solution hyperplane depends on these vectors alone. Any margin point (those SV with \\(\\alpha_i&gt;0\\) and \\(\\xi_i=0\\)) can be used to solve for \\(b\\). Given \\(\\hat {\\mathbf{w}}\\) and \\(\\hat b\\), the decision function is given by \\[\\begin{eqnarray} \\label{eq:dfinput} \\hat f(\\mathbf{x})&amp;=&amp;\\hat {\\mathbf{w}}^T \\mathbf{x}+\\hat b\\nonumber\\\\ &amp;=&amp;\\sum_{\\mathbf{x}_i \\in \\mbox{sv}} \\hat \\alpha_i y_i \\mathbf{x}_i^T\\mathbf{x}+\\hat b. \\tag{7.3} \\end{eqnarray}\\] SVM can be generalized easily to construct nonlinear boundaries. The common strategy is to map the original data into a high-dimensional space and then construct a linear boundary classifier in the transformed space. The original space of the data is called the input space while the transformed high dimensional space is called the feature space. Figure 7.6 shows an example of a feature mapping from a two-dimensional input space to a two-dimensional feature space. The data can not be separated by a linear function in the input space but can be in the feature space under the mapping \\(\\Phi\\). Although the same dimension is used in Figure (fig:svmmap) for illustration, the feature space is usually of a much higher dimension than the input space. Figure 7.6: An example of mapping where data can be separated by a linear function in the feature space but can not in the input space. With a nonlinear mapping \\(\\Phi\\), SVM is able to produce nonlinear boundaries in the input space by constructing a linear boundary in the feature space. The decision function of SVM now becomes \\[\\begin{equation} \\label{eq:dffeature} \\hat f(\\mathbf{x})=\\sum_{\\mathbf{x}_i \\in \\mbox{sv}} \\hat \\alpha_i y_i \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x})+\\hat b, \\tag{7.4} \\end{equation}\\] which is in terms of inner products in the feature space. Calculating the inner product of \\(\\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x})\\) might be expensive when the feature space is of high dimension. Fortunately, to calculate (\\(\\ref{eq:dffeature}\\)), we do not need to know \\(\\Phi\\) explicitly but only need to know how to evaluate the inner products \\(\\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x})\\). This can be done by using a suitable kernel function. A kernel function \\(\\mathcal{K}\\) is defined as \\[ \\begin{aligned} \\mathcal{K}(\\mathbf{x}_i,\\mathbf{x}_j)&amp;=\\langle \\Phi(\\mathbf{x}_i),\\Phi(\\mathbf{x}_j) \\rangle \\\\ &amp;=\\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j), \\end{aligned} \\] where \\(\\mathbf{x}_i,\\mathbf{x}_j\\) are two points in the input space. Many different forms of \\(\\mathcal{K}\\) are possible, each leading to a different feature space. Mercer’s theorem (Mercer, 1909) provides one way to construct kernels. It says that a symmetric function in the input space, \\(\\mathcal{K}\\), is a kernel function if and only if the Gram matrix \\[ {\\mathbf K}=[\\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_j)]_{i,j=1}^N \\] is positive semi-definite, i.e., has non-negative eigenvalues. Three common kernels are: polynomial: \\(\\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_j)=(\\gamma \\mathbf{x}_i^T \\mathbf{x}_j+ r)^d, \\gamma&gt;0\\). radial basis: \\(\\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_j)=\\mbox{exp}(-\\gamma \\|\\mathbf{x}_i-\\mathbf{x}_j\\|^2), \\gamma&gt;0\\). sigmoid: \\(\\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_j)=\\mbox{tanh}(\\gamma \\mathbf{x}_i^T \\mathbf{x}_j+r)\\). \\end{enumerate} Here, \\(\\gamma, r\\) and \\(d\\) are the kernel parameters that need to be tuned by the training data. Given the kernel function \\(\\mathcal{K}\\), the decision function (\\(\\ref{eq:dffeature}\\)) can be written as \\[ \\begin{aligned} \\hat f(\\mathbf{x})&amp;=\\sum_{\\mathbf{x}_i \\in \\mbox{sv}} \\hat \\alpha_i y_i \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x})+\\hat b\\\\ &amp;=\\sum_{\\mathbf{x}_i \\in \\mbox{sv}} \\hat \\alpha_i y_i \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x})+\\hat b. \\end{aligned} \\] SVM has been implemented in R (Meyer, 2007), which makes use of the C++ implementation of SVM by Chang and Lin (2007). The function svm implemented in R takes the majority class as class 1 and the minority one as class -1 by default; therefore, a data point that is far away from the separating hyperplane on the negative side casts a lot of confidence that this item belongs to the rare class. We apply the SVM on the iris data. Ten-fold cross-validation was used to select the optimal tuning parameter sigma and cost. The final model with sigma=0.5 and cost=1 yields an accuracy of 0.96667 on the testing set. library(e1071) # for SVM # Train the SVM model with cross-validation grid_sigma &lt;- 2^(-15:0) grid_cost &lt;- 2^(-5:0) svmm &lt;- train(Species ~ ., data = training, method = &quot;svmRadial&quot;, preProc = c(&quot;center&quot;, &quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = expand.grid(sigma = grid_sigma, C = grid_cost)) plot(svmm) svmm$finalModel ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.25 ## ## Number of Support Vectors : 47 ## ## Objective Function Value : -3.4903 -2.9427 -21.9289 ## Training error : 0.016667 # Predict the species on the testing data predictions &lt;- predict(svmm, newdata = testing) # Evaluate the accuracy of the model (svmtab &lt;- confusionMatrix(predictions, testing$Species)$table) ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 9 0 ## virginica 0 1 10 (accuracy=sum(diag(svmtab))/sum(svmtab)) ## [1] 0.9666667 7.14 Neural Networks A Neural Network or more precisely an Artificial Neural Network (ANN) is a computational model that is inspired by the way biological neural networks in the human brain process information. The basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (\\(w\\)), which is assigned on the basis of its relative importance to other inputs. The node applies a function \\(f\\) (called the activation function) to the weighted sum of its inputs as shown in the figure below (source: https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/): There are three popular choices of activation functions: Sigmoid \\(f(x)=\\frac{1}{1+e^{-x}}\\). Hyperbolic tangent \\(f(x)=tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\\). Rectified linear function \\(f(x)=\\max(0, x)\\). The corresponding graphs of the activation functions are as follows: Neural networks can also have multiple hidden layers and multiple output units. For example, here is a network with two hidden layers (L2 and L3) and two output units in layer L4: For regression, there is only one output unit; for \\(K\\)-class classification, there are \\(K\\) output units with the \\(k\\)th unit modelling the probability of class \\(k\\). Each node in the hidden layers is a function of linear combinations of the units in the previous layer. For example, \\[ Z_1=f(w_{01}+w_{11}X_1+w_{21}X_2+w_{31}X_3)=f(\\mathbf{w}_1^T\\mathbf{X}) \\] where \\(f(.)\\) can be any of those activation functions, and \\(\\mathbf{X}^T=[1, X_1, X_2, X_3]\\). In general, \\[ Z_i=f(\\mathbf{w}_i^T\\mathbf{X}), i=1, 2, 3; \\quad U_i=f(\\mathbf{a}_i^T\\mathbf{Z}), i=1, 2 \\] where \\(\\mathbf{Z}^T=[1, Z_1, Z_2, Z_3]\\). For the output layer, each node is a function of linear combinations of the last hidden layer \\[ Y_i=g(\\mathbf{b}_i^T\\mathbf{U})=g(T_i), i=1, 2 \\] where the output function \\(g(.)\\) is typically the identify function \\(Y_i=g(T_i)=T_i\\) for regression problem and the softmax function for classification problem. The softmax function is given by \\[ Y_i=g(T_i)=\\frac{e^{T_i}}{\\sum_{j=1}^2 e^{T_j}}, i=1, 2 \\] where \\(\\mathbf{U}^T=[1, U_1, U_2]\\). The neural network model has unknown parameters, the weights, e.g., those \\(w_{ij}, a_{ij}\\) and \\(b_{ij}\\) in the diagram above. The weights can be estimated by minimizing the sum of square errors \\(SSE=\\sum (y_i-\\hat y_i)^2\\) for regression problems or minimizing the cross-entropy (deviance) \\(\\sum y_i\\ln \\hat y_i\\) for classification problems. We apply the nnet on the iris data. A nnet with 3 hidden neurons in the first hidden layer and 2 hidden neurons in the second layer was fit on the training set. The net structure is shown in the graph below. library(neuralnet) # Convert the response variable to a factor training$Species = as.factor(training$Species) # fit a nnet with (3, 2) hidden neurons nnm=neuralnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = training, hidden = c(3, 2), linear.output = FALSE) plot(nnm,rep=&quot;best&quot;) The resulting nnet yields an accuracy of 0.96667 on the testing set. # Predict the response on the testing set nnpred &lt;- compute(nnm, testing[, 1:4])$net.result ind &lt;- apply(nnpred,1,which.max) nnresult &lt;- c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;)[ind] (nntab &lt;- table(Predict=nnresult, True=testing$Species)) ## True ## Predict setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 9 0 ## virginica 0 1 10 (accuracy &lt;- sum(diag(nntab)))/sum(nntab) ## [1] 0.9666667 7.15 Classical Methods The classical methods covered are Mahalanobis distance, Bayesian posterior, Fisher’s LDA and QDA. 7.15.1 Mahalanobis Distance Method The idea is to classify observation to the class with the shortest Mahalanobis distance to the class center. That is \\[ y &lt;- \\text{argmin}_i (\\mathbf{x}-\\mathbf{\\mu_i})^T\\mathbf{\\Sigma}_i^{-1} (\\mathbf{x}-\\mathbf{\\mu_i}) \\] where \\(\\mathbf{\\mu_i}\\) and \\(\\mathbf{\\Sigma}_i\\) are the mean vector and variance-covariance matrix of class \\(i\\). 7.15.2 Bayes Posterior Suppose there are \\(K\\) classes, \\(\\mathbf{x}|Y=i \\sim f_i(\\mathbf{x})\\), and the relative frequency of each class is \\(P(Y=i)=\\pi_i\\) (we call this the prior). By Bayes rule, the posterior probability is \\[ P(Y=i|\\mathbf{x})=\\frac{P(Y=i, \\mathbf{x})}{P(\\mathbf{x})}=\\frac{P(\\mathbf{x}|Y=i)P(Y=i)}{\\displaystyle\\sum_{j=1}^KP(\\mathbf{x}|Y=j)P(Y=j)}=\\frac{f_i(\\mathbf{x})\\pi_i}{\\displaystyle\\sum_{j=1}^K f_j(\\mathbf{x})\\pi_j}\\propto f_i(\\mathbf{x})\\pi_i \\] Assign the observation \\(\\mathbf{x}\\) to the class yielding the largest posterior probability, i.e., \\[ y=\\text{argmax}_i \\frac{f_i(\\mathbf{x})\\pi_i}{\\sum_{j=1}^K f_j(\\mathbf{x})\\pi_j}=\\text{argmax}_i f_i(\\mathbf{x})\\pi_i. \\] If there are only two classes, \\(K=2\\), assign \\(\\mathbf{x}\\) to class 1 if the posterior probability \\(P(Y=1|\\mathbf{x})\\ge P(Y=2|\\mathbf{x})\\), i.e., if \\(P(Y=1|\\mathbf{x})\\ge 0.5\\). If we further assume the distributions \\(f_i\\) are multivariate normal distributions, we can obtain the discriminant function in a close form. If \\(\\mathbf{X}|Y=0\\sim MVN(\\mathbf{\\mu}_0, \\mathbf{\\Sigma}_0), \\mathbf{X}|Y=1\\sim MVN(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\), The Bayesian posterior is given by \\[ P(Y=0|\\mathbf{x})=\\frac{\\pi_0 f_0(\\mathbf{x})}{\\pi_0 f_0(\\mathbf{x})+\\pi_1 f_1(\\mathbf{x})}, \\quad P(Y=1|\\mathbf{x})=\\frac{\\pi_1 f_1(\\mathbf{x})}{\\pi_0 f_0(\\mathbf{x})+\\pi_1 f_1(\\mathbf{x})} \\] Assign the observation \\(\\mathbf{x}\\) to class 0 if \\(P(Y=0|\\mathbf{x})\\ge P(Y=1|\\mathbf{x})\\) or \\[ \\frac{P(Y=0|\\mathbf{x})}{P(Y=1|\\mathbf{x})}\\ge 1 \\Longrightarrow \\log \\frac{P(Y=0|\\mathbf{x})}{P(Y=1|\\mathbf{x})}=\\log \\frac{\\pi_0 f_0(\\mathbf{x})}{\\pi_1 f_1(\\mathbf{x})}\\ge 0 \\] That is \\[\\begin{align*} &amp;\\log \\frac{P(Y=0|\\mathbf{x})}{P(Y=1|\\mathbf{x})}=\\log \\frac{\\pi_0}{\\pi_1}+\\log \\frac{f_0(\\mathbf{x})}{f_1(\\mathbf{x})}=\\log \\frac{\\pi_0}{\\pi_1}+\\log \\left\\{\\frac{\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}_0|^{1/2}}\\exp \\left[-\\frac{(\\mathbf{x}-\\mathbf{\\mu_0})^{T}\\mathbf{\\Sigma_0}^{-1} (\\mathbf{x}-\\mathbf{\\mu_0})}{2}\\right]}{\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}_1|^{1/2}}\\exp \\left[-\\frac{(\\mathbf{x}-\\mathbf{\\mu_1})^{T}\\mathbf{\\Sigma_1}^{-1} (\\mathbf{x}-\\mathbf{\\mu_1})}{2}\\right]}\\right\\}\\\\ &amp;=\\log \\frac{\\pi_0}{\\pi_1}+\\frac{1}{2}\\log\\left(\\frac{|\\mathbf{\\Sigma}_0|}{|\\mathbf{\\Sigma}_1|}\\right)-\\frac{1}{2}\\left[(\\mathbf{x}-\\mathbf{\\mu_0})^{T}\\mathbf{\\Sigma}_0^{-1} (\\mathbf{x}-\\mathbf{\\mu_0})-(\\mathbf{x}-\\mathbf{\\mu_1})^{T}\\mathbf{\\Sigma}_1^{-1} (\\mathbf{x}-\\mathbf{\\mu_1}) \\right]\\\\ &amp;=C-\\frac{1}{2}\\left[\\mathbf{x}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{x}-\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{x}-\\mathbf{x}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{\\mu_0}+\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{\\mu_0}-\\mathbf{x}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{x}+\\mathbf{\\mu_1}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{x}+\\mathbf{x}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{\\mu_1}-\\mathbf{\\mu_1}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{\\mu_1}\\right] \\end{align*}\\] Linear discriminant analysis (LDA) when \\(\\mathbf{\\Sigma}_0=\\mathbf{\\Sigma}_1=\\mathbf{\\Sigma}\\), the quadratic terms cancel out: \\[\\begin{align*} &amp;\\log \\frac{P(Y=0|\\mathbf{x})}{P(Y=1|\\mathbf{x})}=\\log \\frac{\\pi_0}{\\pi_1}+ (\\mathbf{\\mu_0-\\mu_1})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}\\left(\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu_0}-\\mathbf{\\mu_1}^T\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu_1}\\right)\\\\ &amp;=\\log \\frac{\\pi_0}{\\pi_1}+(\\mathbf{\\mu_0-\\mu_1})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}\\left(\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu_0}-\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu_1}+\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu_1}-\\mathbf{\\mu_1}^T\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu_1}\\right)\\\\ &amp;=\\log \\frac{\\pi_0}{\\pi_1}+(\\mathbf{\\mu_0-\\mu_1})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}(\\mathbf{\\mu_0-\\mu_1})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_0+\\mu_1}) \\end{align*}\\] Therefore the discriminant function \\[ f(\\mathbf{w}, \\mathbf{x})=\\log \\frac{\\pi_0}{\\pi_1}+\\mathbf{w}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}\\mathbf{w}^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_0+\\mu_1}) \\quad \\quad \\mbox{ (Let $\\mathbf{w}=\\mathbf{\\mu}_0-\\mathbf{\\mu}_1$)} \\] is a linear combination of the vector \\(\\mathbf{x}\\). We assign \\(\\mathbf{x}\\) to class 0 if the discriminant function \\(f(\\mathbf{w}, \\mathbf{x})\\ge 0\\). Quadratic discriminant analysis (QDA) when \\(\\mathbf{\\Sigma}_0\\ne \\mathbf{\\Sigma}_1\\). The quadratic terms can not cancel out each other; therefore, the discriminant function \\[ f(\\mathbf{w}, \\mathbf{x})=C-\\frac{1}{2}\\left[\\mathbf{x}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{x}-\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{x}-\\mathbf{x}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{\\mu_0}+\\mathbf{\\mu_0}^T\\mathbf{\\Sigma}_0^{-1}\\mathbf{\\mu_0}-\\mathbf{x}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{x}+\\mathbf{\\mu_1}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{x}+\\mathbf{x}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{\\mu_1}-\\mathbf{\\mu_1}^T\\mathbf{\\Sigma}_1^{-1}\\mathbf{\\mu_1}\\right] \\] is a quadratic function in \\(\\mathbf{x}\\). The idea of discriminant function can be generalized to three classes problems. Assign \\(\\mathbf{x}\\) to class 0 if \\[\\begin{align*} f_{01}(\\mathbf{x})&amp;=(\\mathbf{\\mu_0-\\mu_1})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}(\\mathbf{\\mu_0-\\mu_1})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_0+\\mu_1})\\ge 0 \\mbox{ and }\\\\ f_{02}(\\mathbf{x})&amp;=(\\mathbf{\\mu_0-\\mu_2})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}(\\mathbf{\\mu_0-\\mu_2})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_0+\\mu_2})\\ge 0 \\end{align*}\\] Assign \\(\\mathbf{x}\\) to class 1 if \\[\\begin{align*} f_{10}(\\mathbf{x})&amp;=(\\mathbf{\\mu_1-\\mu_0})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}(\\mathbf{\\mu_1-\\mu_0})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_1+\\mu_0})\\ge 0 \\mbox{ and }\\\\ f_{12}(\\mathbf{x})&amp;=(\\mathbf{\\mu_1-\\mu_2})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}(\\mathbf{\\mu_1-\\mu_2})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_1+\\mu_2})\\ge 0 \\end{align*}\\] Assign \\(\\mathbf{x}\\) to class 2 if \\[\\begin{align*} f_{20}(\\mathbf{x})&amp;=(\\mathbf{\\mu_2-\\mu_0})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}(\\mathbf{\\mu_2-\\mu_0})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_2+\\mu_0})\\ge 0 \\mbox{ and }\\\\ f_{21}(\\mathbf{x})&amp;=(\\mathbf{\\mu_2-\\mu_1})^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}(\\mathbf{\\mu_2-\\mu_1})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\mu_2+\\mu_1})\\ge 0 \\end{align*}\\] In general, we assign \\(\\mathbf{x}\\) to the class giving the largest value of \\(\\delta(\\cdot)\\), i.e., \\(y=\\text{argmax}_i \\delta_i(\\mathbf{x})\\) where the \\(\\delta(\\cdot)\\) is defined as \\[\\begin{align*} \\delta_i(\\mathbf{x})&amp;=\\log(\\pi_i)+\\mathbf{\\mu}_i^T\\mathbf{\\Sigma}^{-1}\\mathbf{x}-\\frac{1}{2}\\mathbf{\\mu}_i^T\\mathbf{\\Sigma}^{-1}\\mathbf{\\mu}_i, i=1, 2, \\cdots, k, \\mbox {for LDA}\\\\ \\delta_i(\\mathbf{x})&amp;=\\log(\\pi_i)-\\frac{1}{2}\\log|\\mathbf{\\Sigma}_i|-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu}_i)^T\\mathbf{\\Sigma}_i^{-1}(\\mathbf{x}-\\mathbf{\\mu}_i), i=1, 2, \\cdots, k, \\mbox {for QDA} \\end{align*}\\] 7.15.3 Fisher’s Discriminant Analysis For Fisher’s discriminant analysis, we do not assume the distribution of \\(\\mathbf{X}\\) is multivariate normal. And we want to find a linear combination (projection) of \\(\\mathbf{X}\\), such that the between-group variation in the new coordinate is maximized. That is to find \\[ \\text{argmax}_{\\mathbf{a}} \\frac{\\mathbf{a}^T\\mathbf{S}_B\\mathbf{a}}{\\mathbf{a}^T\\mathbf{S}_W\\mathbf{a}} \\] which is equivalent to finding \\(\\text{argmax}_{\\mathbf{a}} \\mathbf{a}^T\\mathbf{S}_B\\mathbf{a}\\) with the constraint \\(\\mathbf{a}^T\\mathbf{S}_W\\mathbf{a}=1\\). By Lagrange multiplier method, the objective function is \\[ Q= \\mathbf{a}^T\\mathbf{S}_B\\mathbf{a}-\\lambda(\\mathbf{a}^T\\mathbf{S}_W\\mathbf{a}-1) \\] \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\mathbf{a}}&amp;=2\\mathbf{S}_B\\mathbf{a}-2\\lambda \\mathbf{S}_W \\mathbf{a}=0 \\Longrightarrow \\mathbf{S}_B\\mathbf{a}=\\lambda \\mathbf{S}_W\\mathbf{a} \\Longrightarrow \\mathbf{S}_W^{-1}\\mathbf{S}_B\\mathbf{a}=\\lambda\\mathbf{a}\\\\ \\frac{\\partial Q}{\\partial \\lambda}&amp;=\\mathbf{a}^T\\mathbf{S}_W\\mathbf{a}-1=0\\Longrightarrow \\mathbf{a}^T\\mathbf{S}_W\\mathbf{a}=1 \\end{align*}\\] The first equation implies that \\(\\lambda\\) is an eigenvalue of the matrix \\(\\mathbf{S}_W^{-1}\\mathbf{S}_B\\) and \\(\\mathbf{a}\\) is the corresponding eigenvector. The first equation also implies that \\(\\mathbf{a}^T\\mathbf{S}_B\\mathbf{a}=\\lambda\\), that is, to maximize \\(\\mathbf{a}^T\\mathbf{S}_B\\mathbf{a}\\) is to maximize \\(\\lambda\\). As a result, \\(\\mathbf{e}\\) is the eigenvector associated to the largest eigenvalue of the matrix \\(\\mathbf{S}_W^{-1}\\mathbf{S}_B\\), then \\[ \\mathbf{a}=\\frac{\\mathbf{e}}{\\sqrt{c}}, \\quad \\mbox{ where $c=\\mathbf{e}^T\\mathbf{S}_W\\mathbf{e}$}. \\] Assign \\(\\mathbf{x}\\) to the class giving the smallest projected distance to the class center, i.e., \\[ y=\\arg\\min_i [\\mathbf{a}^T(\\mathbf{x}-\\mathbf{\\mu}_i)]^2. \\] \\(\\textbf{Example}\\): Bayes Posterior and Fisher’s Discriminant Analysis Suppose \\(\\mathbf{\\mu}_1=[2.5, 2.5]^T, \\mathbf{\\mu}_2=[7.5, 7.5]^T\\) and \\[ \\mathbf{\\Sigma}_1=\\mathbf{\\Sigma}_2=\\left[ \\begin{array}{rr} 1&amp;0.2\\\\ 0.2&amp;4 \\end{array} \\right]. \\] Classify \\(\\mathbf{x}=[4, 6]^T\\) to either class 1 or class 2. Here is the \\(\\textsf{R}\\) code to do the example. mu1 &lt;- c(2.5, 2.5) mu2 &lt;- c(7.5, 7.5) sigma1 &lt;- sigma2 &lt;- matrix(c(1,0.2,0.2,4),2,2) #given a new data (4, 6), which class shall we assign? x &lt;- c(4,6) e &lt;- solve(sigma1)%*%matrix(mu1-mu2,2,1) c &lt;- t(e)%*%sigma1%*%e a &lt;- e/sqrt(c[1,1]) d1 &lt;- abs(t(a)%*%(matrix(x,2,1)-mu1)) d2 &lt;- abs(t(a)%*%(matrix(x,2,1)-mu2)) c(d1,d2) #closer to mu1, classify to class 1 ## [1] 1.991556 3.397359 We apply LDA and QDA on the iris data focusing on the two species: versicolor and virginica. We first split the data into training and test sets. We did a stratified sampling with 75% training and 25% testing. library(caret) library(e1071) data &lt;- iris[51:150,] levels(data$Species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; data$Species &lt;- droplevels(data$Species) #drop the level with no observations levels(data$Species) ## [1] &quot;versicolor&quot; &quot;virginica&quot; set.seed(4061) flds &lt;- createFolds(data$Species, k = 4, list = TRUE, returnTrain = FALSE) #response must be factor for stratified sampling train &lt;- data[-flds[[1]],] test &lt;- data[flds[[1]],] table(train$Species) ## ## versicolor virginica ## 37 38 table(test$Species) ## ## versicolor virginica ## 13 12 The misclassification rate of Fisher’s LDA method is 0.04, i.e., the accuracy is 96%. library(MASS) library(psych) library(klaR) #pairs.panels(train[,1:4],bg=c(&quot;red&quot;,&quot;blue&quot;)[train$Species],pch=21) objlda &lt;- lda(Species~.,data=train) ldap &lt;- predict(objlda,test) (ldatab=table(True=test$Species,Predict=ldap$class)) ## Predict ## True versicolor virginica ## versicolor 12 1 ## virginica 0 12 (ldar=1-sum(diag(ldatab))/sum(ldatab)) ## [1] 0.04 partimat(Species~.,data=train,method=&quot;lda&quot;) The misclassification rate of Fisher’s QDA method is 0.04, i.e., the accuracy is 96%. objqda &lt;- qda(Species~.,data=train) qdap &lt;- predict(objqda,test) (qdatab &lt;- table(True=test$Species,Predict=qdap$class)) ## Predict ## True versicolor virginica ## versicolor 12 1 ## virginica 0 12 (qdar &lt;- 1-sum(diag(qdatab))/sum(qdatab)) ## [1] 0.04 partimat(Species~.,data=train,method=&quot;qda&quot;) 7.16 Summary Here I summarize briefly the pros and cons of each classifier. \\[ \\begin{array}{|||} \\hline {\\bf Method}&amp;{\\bf Advantages}&amp;{\\bf Disadvantages}\\\\ \\hline \\text{KNN}&amp;\\text{easy to understand and calculate}&amp;\\text{based on distance, not designed for mixed data types}\\\\ &amp;\\text{able to capture local structure}&amp;\\text{need to tune $k$: # of nearest neighbors}\\\\ \\hline \\text{Logistic}&amp;\\text{easy to interpret}&amp;\\text{create linear boundary}\\\\ \\text{regression}&amp;\\text{easy to handle mixed data types}&amp;\\text{unstable if the classes are well separated}\\\\ \\hline \\text{Bayes} &amp;\\text{linear (LDA) and quadratic boundary (QDA)}&amp;\\text{distribution assumption}\\\\ \\text{posterior}&amp;\\text{optimal under normality}&amp;\\text{need estimate means and variances}\\\\ \\hline \\text{Classification}&amp;\\text{easy to interpret (graph)}&amp;\\text{large variation}\\\\ \\text{tree}&amp;\\text{easy to handle mixed data types}&amp;\\text{tend to overfit}\\\\ \\hline \\text{Random} &amp;\\text{good performance}&amp;\\text{hard to interpret}\\\\ \\text{forest}&amp;\\text{importance table}&amp;\\text{need to tune parameters}\\\\ \\hline \\text{Neural}&amp;\\text{good performance}&amp;\\text{hard to interpret}\\\\ \\text{network}&amp;\\text{handle mixed data type}&amp;\\text{need to tune parameters}\\\\ \\hline \\text{SVM}&amp;\\text{good performance, flexible boundary}&amp;\\text{need to tune parameters}\\\\ \\hline \\end{array} \\] Revisit Learning Outcomes After finishing this chapter, students should be able to Use proper performance metrics to compare classification methods. Use cross-validation method to fit classification models. Explain the main idea of each classification method covered: K-nearest neighbor, logistic regression, classification tree, random forests, neural network, support vector machines, and Fisher’s LDA and QDA. Use R to fit the models listed above and interpret the computer outputs. "],["clustering-analysis.html", "8 Clustering Analysis Learning Outcomes 8.1 Introduction 8.2 Clustering Methods 8.3 Determine \\(K\\): Number of Clusters Side-Note on the EM Algorithm Revisit Learning Outcomes", " 8 Clustering Analysis Learning Outcomes After finishing this chapter, students should be able to Explain the differences between classification and cluster problems. Describe briefly the main idea and procedure of hierarchical clustering, \\(K\\)-means, and model-based clustering methods. Conduct a clustering analysis using hierarchical clustering method, \\(K\\)-means, and model-based clustering in R. Interpret the R computer outputs of hierarchical clustering, \\(K\\)-means, and model-based clustering methods. 8.1 Introduction We have already introduced multivariate data analysis and classification. This note introduces another major application in machine learning: clustering. The main difference between a clustering problem and a classification problem is that the class labels are unknown for clustering. The objectives of a clustering problem are Determine the proper number of clusters \\(K\\). Group the observations into \\(K\\) clusters. Allocate new observations to one of those \\(K\\) clusters. 8.2 Clustering Methods Hierarchical clustering method, \\(K\\)-means, and model-based clustering will be covered. 8.2.1 Hierarchical Method Like the forward selection and backward elimination model selection methods in multiple regression, either merging (start with one observation as one group) or division (start with all observation in one cluster) can be used in hierarchical methods. Suppose there are \\(n\\) observations, we can calculate the \\(n\\times n\\) pairwise distance matrix using Euclidean, Manhattan, Mahalanobis, Hamming, and Gower’s distance. Steps to conduct a hierarchical clustering are as follows: One observation one group Merge the two groups with the smallest distance or largest similarity Repeat step 2 until all observations are in one group. There are three way to measure the distance between two groups \\(G_i\\) and \\(G_j\\), \\(d(G_i, G_j)\\). single linkage: \\(d(G_i, G_j)=\\min_{p\\in G_i, q\\in G_j} d(p,q)\\).The distance between the closest pair of objects belonging to two different groups. complete linkage: \\(d(G_i, G_j)=\\max_{p\\in G_i, q\\in G_j} d(p,q)\\).The distance between the furthest pair of objects belonging to two different groups. average linkage:\\(d(G_i, G_j)=\\text{average}_{p\\in G_i, q\\in G_j} d(p,q)\\).The average distance of all between-group pairs. Example: Hierarchical Clustering Let \\(x_1=5, x_2=3, x_3=9, x_4=6, x_5=1\\). Find the distance matrix. # R does it much better import numpy as np from scipy.spatial.distance import pdist x = np.array([5, 3, 9, 6, 1]).reshape(-1, 1) dists = pdist(x, metric=&#39;euclidean&#39;) # formatting like R&#39;s dist() printout k = 0 n = len(x) for i in range(1, n): row = [] for j in range(i): row.append(f&quot;{dists[k]:&gt;4.1f}&quot;) k += 1 print(&quot; &quot;.join(row)) ## 2.0 ## 4.0 1.0 ## 4.0 6.0 3.0 ## 2.0 3.0 8.0 5.0 Conduct a hierarchical clustering with single linkage. We first merge \\(x_1\\) and \\(x_4\\) which pair has the shortest distance 1. Update the distance matrix using the single linkage. \\[ \\begin{aligned} d_{(1,4), 2}&amp;=\\min\\{d_{12}, d_{42}\\}=\\min\\{2, 3\\}=2\\\\ d_{(1,4), 3}&amp;=\\min\\{d_{13}, d_{43}\\}=\\min\\{4, 3\\}=3\\\\ d_{(1,4), 5}&amp;=\\min\\{d_{15}, d_{45}\\}=\\min\\{4, 5\\}=4\\\\ \\end{aligned} \\] We can either merge \\(\\{x_1, x_4\\}\\) with \\(x_2\\) or \\(x_2\\) with \\(x_5\\), both of which have distance 2. I prefer to keep the group size smaller; therefore, I merge \\(x_2\\) and \\(x_5\\). Update the distance matrix. \\[ \\begin{aligned} d_{(1,4), (2,5)}&amp;=\\min\\{d_{(1,4),2}, d_{(1,4),5}\\}=\\min\\{2, 4\\}=2\\\\ d_{(2,5), 3}&amp;=\\min\\{d_{23}, d_{53}\\}=\\min\\{6, 8\\}=6\\\\ \\end{aligned} \\] We merge \\(\\{x_1, x_4\\}\\) and \\(\\{x_2, x_5\\}\\). Lastly, we merger \\(\\{x_1, x_4, x_2, x_5\\}\\) and \\(x_3\\) with distance \\[ d_{(1,4,2,5),3}=\\min\\{d_{(1,4),3}, d_{(2,5),3}\\}=\\min\\{3, 6\\}=3 \\] As a result, the cluster dendrogram is as follows: from scipy.cluster.hierarchy import linkage, dendrogram import matplotlib.pyplot as plt linked = linkage(x, method=&#39;single&#39;, metric=&#39;euclidean&#39;) plt.figure(figsize=(6, 4)) ## &lt;Figure size 600x400 with 0 Axes&gt; dendrogram(linked, labels=[1, 2, 3, 4, 5], orientation=&#39;top&#39;, distance_sort=&#39;ascending&#39;) ## {&#39;icoord&#39;: [[35.0, 35.0, 45.0, 45.0], [25.0, 25.0, 40.0, 40.0], [15.0, 15.0, 32.5, 32.5], [5.0, 5.0, 23.75, 23.75]], &#39;dcoord&#39;: [[0.0, np.float64(1.0), np.float64(1.0), 0.0], [0.0, np.float64(2.0), np.float64(2.0), np.float64(1.0)], [0.0, np.float64(2.0), np.float64(2.0), np.float64(2.0)], [0.0, np.float64(3.0), np.float64(3.0), np.float64(2.0)]], &#39;ivl&#39;: [3, 5, 2, 1, 4], &#39;leaves&#39;: [2, 4, 1, 0, 3], &#39;color_list&#39;: [&#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C0&#39;], &#39;leaves_color_list&#39;: [&#39;C0&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;, &#39;C1&#39;]} plt.title(&quot;Cluster Dendrogram&quot;) ## Text(0.5, 1.0, &#39;Cluster Dendrogram&#39;) plt.xlabel(&quot;Variables&quot;) ## Text(0.5, 0, &#39;Variables&#39;) plt.ylabel(&quot;Distance&quot;) ## Text(0, 0.5, &#39;Distance&#39;) plt.tight_layout() plt.show() plt.close() Conduct a hierarchical clustering with complete linkage. Again, we first merge \\(x_1\\) and \\(x_4\\) and update the distance matrix using the complete linkage. \\[ \\begin{aligned} d_{(1,4), 2}&amp;=\\max\\{d_{12}, d_{42}\\}=\\max\\{2, 3\\}=3\\\\ d_{(1,4), 3}&amp;=\\max\\{d_{13}, d_{43}\\}=\\max\\{4, 3\\}=4\\\\ d_{(1,4), 5}&amp;=\\max\\{d_{15}, d_{45}\\}=\\max\\{4, 5\\}=5\\\\ \\end{aligned} \\] Next, we merge \\(x_2\\) and \\(x_5\\) and update the distance matrix. \\[ \\begin{aligned} d_{(1,4), (2,5)}&amp;=\\max\\{d_{(1,4),2}, d_{(1,4),5}\\}=\\max\\{3, 5\\}=5\\\\ d_{(2,5), 3}&amp;=\\max\\{d_{23}, d_{53}\\}=\\max\\{6, 8\\}=8\\\\ \\end{aligned} \\] We merge \\(\\{x_1, x_4\\}\\) and \\(x_3\\). Lastly, we merger \\(\\{x_1, x_4, x_3\\}\\) and \\(\\{x_2, x-5\\}\\) with distance \\[ d_{(1,4,3),(2,5)}=\\max\\{d_{(1,4),(2,5)}, d_{3,(2,5)}\\}=\\max\\{5, 8\\}=8 \\] As a result, the cluster dendrogram is as follows: linked_complete = linkage(x, method=&#39;complete&#39;, metric=&#39;euclidean&#39;) plt.figure(figsize=(6, 4)) ## &lt;Figure size 600x400 with 0 Axes&gt; dendrogram(linked_complete, labels=[1, 2, 3, 4, 5], # Match R labels orientation=&#39;top&#39;, distance_sort=&#39;ascending&#39;) ## {&#39;icoord&#39;: [[5.0, 5.0, 15.0, 15.0], [35.0, 35.0, 45.0, 45.0], [25.0, 25.0, 40.0, 40.0], [10.0, 10.0, 32.5, 32.5]], &#39;dcoord&#39;: [[0.0, np.float64(2.0), np.float64(2.0), 0.0], [0.0, np.float64(1.0), np.float64(1.0), 0.0], [0.0, np.float64(4.0), np.float64(4.0), np.float64(1.0)], [np.float64(2.0), np.float64(8.0), np.float64(8.0), np.float64(4.0)]], &#39;ivl&#39;: [2, 5, 3, 1, 4], &#39;leaves&#39;: [1, 4, 2, 0, 3], &#39;color_list&#39;: [&#39;C1&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C0&#39;], &#39;leaves_color_list&#39;: [&#39;C1&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C2&#39;, &#39;C2&#39;]} plt.title(&quot;Cluster Dendrogram&quot;) ## Text(0.5, 1.0, &#39;Cluster Dendrogram&#39;) plt.xlabel(&quot;Variables&quot;) ## Text(0.5, 0, &#39;Variables&#39;) plt.ylabel(&quot;Distance&quot;) ## Text(0, 0.5, &#39;Distance&#39;) plt.tight_layout() plt.show() plt.close() Conduct a hierarchical clustering with average linkage. Again, we first merge \\(x_1\\) and \\(x_4\\) and update the distance matrix using the average linkage. [ \\[\\begin{array}{c|ccc} &amp;(1, 4)&amp;2&amp;3\\\\ \\hline 2&amp;2.5&amp;&amp;\\\\ 3&amp;3.5&amp;6&amp;\\\\ 5&amp;4.5&amp;2&amp;8\\\\ \\end{array}\\] ] \\[ \\begin{aligned} d_{(1,4), 2}&amp;=\\frac{d_{12}+d_{42}}{2}=\\frac{2+3}{2}=2.5\\\\ d_{(1,4), 3}&amp;=\\frac{d_{13}+d_{43}}{2}=\\frac{4+3}{2}=3.5\\\\ d_{(1,4), 5}&amp;=\\frac{d_{15}+d_{45}}{2}=\\frac{4+5}{2}=4.5\\\\ \\end{aligned} \\] Next, we merge \\(x_2\\) and \\(x_5\\) and update the distance matrix. [ \\[\\begin{array}{c|cc} &amp;(1, 4)&amp;(2, 5)\\\\ \\hline (2, 5)&amp;3.5&amp;\\\\ 3&amp;3.5&amp;7\\\\ \\end{array}\\] ] \\[ \\begin{aligned} d_{(1,4), (2,5)}&amp;=\\frac{d_{(1,4),2}+d_{(1,4),5}}{2}=\\frac{2.5+4.5}{2}=3.5\\\\ d_{(2,5), 3}&amp;=\\frac{d_{23}+d_{53}}{2}=\\frac{6+8}{2}=7\\\\ \\end{aligned} \\] We merge \\(\\{x_1, x_4\\}\\) and \\(\\{x_2, x_5\\}\\). Lastly, we merger \\(\\{x_1, x_4, x_2, x_5\\}\\) and \\(x_3\\) with distance \\[ d_{(1,4,2,5),3}=\\frac{d_{(1,4),3}+d_{(2,5),3}}{2}=\\frac{3.5+7}{2}=5.25 \\] As a result, the cluster dendrogram is as follows: linked_avg = linkage(x, method=&#39;average&#39;, metric=&#39;euclidean&#39;) plt.figure(figsize=(6, 4)) ## &lt;Figure size 600x400 with 0 Axes&gt; dendrogram(linked_avg, labels=[1, 2, 3, 4, 5], # Match R labels orientation=&#39;top&#39;, distance_sort=&#39;ascending&#39;) ## {&#39;icoord&#39;: [[5.0, 5.0, 15.0, 15.0], [35.0, 35.0, 45.0, 45.0], [25.0, 25.0, 40.0, 40.0], [10.0, 10.0, 32.5, 32.5]], &#39;dcoord&#39;: [[0.0, np.float64(2.0), np.float64(2.0), 0.0], [0.0, np.float64(1.0), np.float64(1.0), 0.0], [0.0, np.float64(3.5), np.float64(3.5), np.float64(1.0)], [np.float64(2.0), np.float64(4.666666666666667), np.float64(4.666666666666667), np.float64(3.5)]], &#39;ivl&#39;: [2, 5, 3, 1, 4], &#39;leaves&#39;: [1, 4, 2, 0, 3], &#39;color_list&#39;: [&#39;C1&#39;, &#39;C2&#39;, &#39;C0&#39;, &#39;C0&#39;], &#39;leaves_color_list&#39;: [&#39;C1&#39;, &#39;C1&#39;, &#39;C0&#39;, &#39;C2&#39;, &#39;C2&#39;]} plt.title(&quot;Cluster Dendrogram&quot;) ## Text(0.5, 1.0, &#39;Cluster Dendrogram&#39;) plt.xlabel(&quot;Variables&quot;) ## Text(0.5, 0, &#39;Variables&#39;) plt.ylabel(&quot;Distance&quot;) ## Text(0, 0.5, &#39;Distance&#39;) plt.tight_layout() plt.show() plt.close() This dendrogram tells us that cutting at distance=5 results in two clusters \\(\\{x_3\\}, \\{x_1,x_4,x_2,x_5\\}\\); cutting at distance=3 results in three clusters \\(\\{x_3\\}, \\{x_1,x_4\\}, \\{x_2,x_5\\}\\). 8.2.2 K-Means Compared to the hierarchical clustering, \\(K\\)-mean is much more efficient in computation. The algorithm of \\(K\\)-mean is as follows: Arbitrarily pick the centers of the \\(K\\) clusters \\(\\mathbf{m}_1^{(\\mbox{old})}, \\mathbf{m}_2^{(\\mbox{old})},\\cdots, \\mathbf{m}_K^{(\\mbox{old})}\\). Assign each object to one and only one of the clusters, choose the one with the shortest distance from the object to the group center. That is \\(Y=\\mbox{argmin}_j d(\\mathbf{x},\\mathbf{m}_j)\\). Update the group (cluster) centers \\[ \\mathbf{m}_i^{(\\mbox{new})}=\\frac{1}{n_i}\\displaystyle\\sum_{\\mathbf{x}\\in G_i} \\mathbf{x}, i=1, 2, \\cdots, K, \\quad \\mbox{$n_i$ is the number of objects in group $i$} \\] Repeat steps (2) and (3) until convergence, i.e., the centers won’t change anymore. Here is an illustration adopted from Dr. Jeffrey L. Andrews’s notes. Twenty-five observations were generated from bivariate normal distributions with mean vectors \\(\\mathbf{\\mu}_1=(0, 0)^T\\) and \\(\\mathbf{\\mu}_2=(5, -5)^T\\), and covariance matrix \\[ \\mathbf{\\Sigma}_1=\\mathbf{\\Sigma}_2=\\left[ \\begin{array}{cc} 1&amp;0\\\\ 0&amp;1 \\end{array} \\right]. \\] The K-means method converges in three iterations, i.e., the labels of observation don’t change after three rounds. ## &lt;IPython.core.display.Image object&gt; Note: The result of \\(K\\)-means will be affected by the initial value, it is important to run the algorithm multiple times from different random initial configurations. Let’s revisit the simple example. Example: K-Means Clustering Let \\(x_1=5, x_2=3, x_3=9, x_4=6, x_5=1\\). Assume \\(K=2\\) clusters. [(a)] If we set the initial centers \\(\\mathbf{m}_1^{(0)}=0, \\mathbf{m}_2^{(0)}=11\\). Allocation. Group 1: \\(\\{x_5, x_2, x_1\\}\\); group 2: \\(\\{x_4, x_3\\}\\). Recalculate the centers. \\[ \\mathbf{m}_1^{(1)}=\\frac{1+3+5}{3}=3, \\mathbf{m}_2^{(1)}=\\frac{6+9}{2}=7.5 \\] Re-allocate. Group 1: \\(\\{x_5, x_2, x_1\\}\\); group 2: \\(\\{x_4, x_3\\}\\). The centers are the same as the previous step. Algorithm converges. The final grouping is group 1: \\(\\{x_5, x_2, x_1\\}\\) and group 2: \\(\\{x_4, x_3\\}\\) with centers 3 and 7.5 respectively. [(b)] If we set the initial centers \\(\\mathbf{m}_1^{(0)}=4, \\mathbf{m}_2^{(0)}=5\\). Allocation. Group 1: \\(\\{x_5, x_2\\}\\); group 2: \\(\\{x_1, x_4, x_3\\}\\). Recalculate the centers. \\[ \\mathbf{m}_1^{(1)}=\\frac{1+3}{2}=2, \\mathbf{m}_2^{(1)}=\\frac{5+6+9}{3}=6.667 \\] Re-allocate. Group 1: \\(\\{x_5, x_2\\}\\); group 2: \\(\\{x_1, x_4, x_3\\}\\). The centers are the same as the previous step. Algorithm converges. The final grouping is group 1: \\(\\{x_5, x_2\\}\\) and group 2: \\(\\{x_1, x_4, x_3\\}\\) with centers 2 and 6.667 respectively. 8.2.3 Model-Based Clustering The model-based clustering method models the joint distribution of the data using a mixture model of \\(K\\) components \\[ f(\\mathbf{x})=\\displaystyle\\sum_{i=1}^K p_if_i(\\mathbf{x}), \\quad \\mbox{ with component proportions $p_i\\ge 0$ and $\\sum p_i=1$.} \\] A popular choice of the components is \\(f_i(\\mathbf{x})\\sim \\mbox{MVN}(\\mathbf{\\mu}_i, \\mathbf{\\Sigma}_i)\\), which results in a multivariate normal mixture model. Suppose there are \\(n\\) independent observations \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_n\\), the likelihood function of the multivariate normal mixture is \\[ L(p_i, \\mathbf{\\mu}_i, \\mathbf{\\Sigma}_i)=\\prod_{j=1}^n f(\\mathbf{x}_j)=\\prod_{j=1}^n \\left(\\sum_{i=1}^K p_i f_i(\\mathbf{x}_j) \\right)=\\prod_{j=1}^n \\left(\\sum_{i=1}^K p_i \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}_i|^{1/2}}\\exp\\{-\\frac{1}{2}(\\mathbf{x}_j-\\mathbf{\\mu}_i)^T\\mathbf{\\Sigma}_i^{-1}(\\mathbf{x}_j-\\mathbf{\\mu}_i)\\} \\right). \\] There are a lot of unknown parameters in the above model: The mixture proportions \\(p_1, p_2, \\cdots, p_K\\) with the constraint \\(\\sum p_i=1 \\Longrightarrow K-1\\) parameters. The mean vectors \\(\\mathbf{\\mu}_i, i=1, \\cdots, K\\), each mean vector is a \\(p\\times 1\\) vector \\(\\Longrightarrow p\\times K\\) parameters. -The covariance matrices \\(\\mathbf{\\Sigma}_i, i=1, \\cdots, K\\). A covariance matrix is symmetric with \\(\\mbox{Var}(X_i), i=1, \\cdots, p\\) as the diagonal elements and covariance \\(\\mbox{Cov}(X_i, X_j), i&lt;j , i,j=1, \\cdots, p\\). Therefore, each covariance matrix consists of \\( \\left( \\begin{array}{@{ }c@{ }} p\\\\2 \\end{array} \\right) +p=\\frac{p(p+1)}{2}\\) parameters. There are \\(K\\) covariance matrices \\(\\Longrightarrow \\frac{p(p+1)}{2}\\times K\\) parameters. As a result, the full model has \\((k-1)+pK+\\frac{p(p+1)}{2}\\times K\\) parameters. Take the Iris data for example, with \\(K=3, p=4\\), the number of parameters of the mixture model ends up to be 44. When \\(p\\) is large, the following reduced models can be considered: [ \\[\\begin{array}{c|c} \\hline \\text{Covariance structure}&amp;\\text{Total # of parameters}\\\\ \\hline \\mathbf{\\Sigma}_i=\\eta \\mathbf{I}&amp;(k-1)+pK+1\\\\ \\mathbf{\\Sigma}_i=\\eta_i \\mathbf{I}&amp;(k-1)+pK+K\\\\ \\mathbf{\\Sigma}_i=\\eta_i \\mbox{diag}(\\lambda_1, \\lambda_2, \\cdots, \\lambda_p)&amp;(k-1)+pK+K+p\\\\ \\hline \\end{array}\\] ] Maximum likelihood method can be used to estimate the values of the parameters. The main idea of maximum likelihood estimate (MLE) is to find the values of the parameters such that the likelihood function \\(L_{\\mbox{max}}\\) is maximized. The Akaike Information criterion (AIC) and the Bayesian Information criterion (BIC) are two popular model selection criterion. We select a model that gives the largest AIC or BIC which are given by \\[ \\mbox{AIC}=2\\ln L_{\\mbox{max}}-2(\\mbox{\\# of free parameters}); \\quad \\mbox{BIC}=2\\ln L_{\\mbox{max}}-\\ln (n) (\\mbox{\\# of free parameters}) \\] where \\(n\\) is the number of observations. BIC is more stringent on the number of parameters than AIC, since \\(\\ln (n)&gt;2\\) for all \\(n\\ge 8\\) EM Algorithm In order to solve the MLE of the mixture model, membership information (each object belongs to which group) is required. The membership information; however, is missing. Therefore, the Expectation-Maximization (EM) algorithm is used to find the MLE. The EM algorithm is a very popular method handling missing values. In order to capture the membership information, we introduce a vector of \\(K\\) binary (unobserved) variables \\(\\mathbf{Z}_i^T=[Z_{i1}, Z_{i2}, \\cdots, Z_{iK}]\\) for each object \\(\\mathbf{x}_i\\), where \\(z_{ij}=1, z_{il}=0, l\\ne j\\) if object \\(i\\) belongs to group \\(j\\). Then the membership weight of \\(\\mathbf{x}_i\\) in group \\(j\\) is \\[ w_{ij}=P(Z_{ij}=1|\\mathbf{x}_i)=\\mbox{E}(Z_{ij}|\\mathbf{x}_i)=P(Z_{ij}=1|\\mathbf{x}_i)=\\frac{f_j(\\mathbf{x}_i)p_j}{\\displaystyle\\sum_{m=1}^K f_m(\\mathbf{x}_i)p_m}, \\quad i=1, 2, \\cdots, n, \\quad j=1, 2,\\cdots, K \\] For each \\(\\mathbf{x}_i\\), \\(\\displaystyle\\sum_{j=1}^K w_{ij}=1\\). As a result, we have a \\(n\\times K\\) membership matrix with each row sum equal to 1. The steps of the EM algorithm are as follows: Set the initial values for the parameters \\(p_1, \\cdots, p_K; \\mathbf{\\mu}_1, \\cdots, \\mathbf{\\mu}_K; \\mathbf{\\Sigma}_1, \\cdots, \\mathbf{\\Sigma}_K\\). E-step. Update the membership weight \\(w_{ij}=\\mbox{E}(Z_{ij})\\): \\[ w_{ij}=\\frac{f_j(\\mathbf{x}_i)p_j}{\\displaystyle\\sum_{m=1}^K f_m(\\mathbf{x}_i)p_m}, \\quad i=1, 2, \\cdots, n, \\quad j=1, 2,\\cdots, K \\] M-step. Update the parameters \\[ p_j^{\\mbox{new}}=\\frac{n_j}{n}, \\quad \\mbox{with } \\quad n_j=\\displaystyle\\sum_{i=1}^n w_{ij} \\quad (\\mbox{sum of $j$ column of the membership weight matrix}) \\] \\[ \\mathbf{\\mu}_j^{\\mbox{new}}=\\frac{1}{n_j}\\displaystyle\\sum_{i=1}^n w_{ij}\\mathbf{x}_i, \\quad \\mathbf{\\Sigma}_j^{\\mbox{new}}=\\frac{1}{n_j}\\displaystyle\\sum_{i=1}^n w_{ij}(\\mathbf{x}_i-\\mathbf{\\mu}_j^{\\mbox{new}})(\\mathbf{x}_i-\\mathbf{\\mu}_j^{\\mbox{new}})^T \\] Repeat steps (2) &amp; (3) until convergence (no change). Note: the \\(K\\)-means algorithm can be regarded as a special of the multivariate normal mixture model by letting \\(\\mathbf{\\Sigma}_i=\\mathbf{I}\\) and the membership weight be 1 for the group with the largest posterior probability and 0 for all the other groups, i.e., \\[ w_{ij}=\\left\\{ \\begin{array}{ll} 1&amp;\\mbox{if group $j$ has the largest posterior probability},\\\\ 0&amp;\\mbox{otherwise}. \\end{array} \\right. \\] 8.2.3.1 Example: Model-based Clustering import pandas as pd import seaborn as sns from sklearn.datasets import load_iris from sklearn.mixture import GaussianMixture from sklearn.decomposition import PCA iris = load_iris() X = iris.data y = iris.target # Not used in modeling # Gaussian Mixture Model (same as Mclust&#39;s default) gmm = GaussianMixture(n_components=3, covariance_type=&#39;full&#39;, random_state=0) gmm.fit(X) ## GaussianMixture(n_components=3, random_state=0) labels = gmm.predict(X) pca = PCA(n_components=2) X_pca = pca.fit_transform(X) plt.figure(figsize=(8, 5)) ## &lt;Figure size 800x500 with 0 Axes&gt; sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette=&quot;Set1&quot;, s=50, edgecolor=&#39;k&#39;) ## &lt;Axes: &gt; plt.title(&quot;Gaussian Mixture Model Classification (EM Algorithm)&quot;) ## Text(0.5, 1.0, &#39;Gaussian Mixture Model Classification (EM Algorithm)&#39;) plt.xlabel(&quot;PC1&quot;) ## Text(0.5, 0, &#39;PC1&#39;) plt.ylabel(&quot;PC2&quot;) ## Text(0, 0.5, &#39;PC2&#39;) plt.legend(title=&quot;Cluster&quot;) ## &lt;matplotlib.legend.Legend object at 0x2b8c61a90&gt; plt.tight_layout() plt.show() print(&quot;Cluster assignments for each observation:\\n&quot;, labels) ## Cluster assignments for each observation: ## [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 2 0 ## 0 0 0 2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## 2 2] If \\(\\verb`VVV`\\) (varying volume, shape, and orientation) model is used, the optimal number of clusters is 2 based on the computer output. As we know that versicolor and virginica are more difficult to separate. The model-based clustering method classifies setosa as one cluster and versicolor and virginica as another cluster. If we use only the \\(\\verb`Petal.Length`\\) and \\(\\verb`Petal.Width`\\) as predictors, the optimal number of clusters is three which is correct. from matplotlib.patches import Ellipse X = pd.DataFrame(iris.data, columns=iris.feature_names) X_petal = X[[&#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]].values gmm.fit(X_petal) ## GaussianMixture(n_components=3, random_state=0) labels = gmm.predict(X_petal) def plot_ellipses(gmm, X, labels): plt.figure(figsize=(8, 5)) colors = [&#39;blue&#39;, &#39;red&#39;, &#39;green&#39;] markers = [&#39;o&#39;, &#39;s&#39;, &#39;^&#39;] for i in range(3): cluster = X[labels == i] plt.scatter(cluster[:, 0], cluster[:, 1], label=f&#39;Cluster {i+1}&#39;, s=40, edgecolor=&#39;k&#39;, marker=markers[i], color=colors[i]) # Ellipse parameters mean = gmm.means_[i] cov = gmm.covariances_[i] vals, vecs = np.linalg.eigh(cov) angle = np.degrees(np.arctan2(*vecs[:, 0][::-1])) width, height = 2 * np.sqrt(vals) ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, edgecolor=&#39;k&#39;, facecolor=&#39;none&#39;, lw=2) plt.gca().add_patch(ellipse) plt.plot(mean[0], mean[1], &#39;k*&#39;, markersize=10) plt.xlabel(&#39;Petal.Length&#39;) plt.ylabel(&#39;Petal.Width&#39;) plt.title(&#39;Model-based Clustering (VVV)&#39;) plt.legend() plt.tight_layout() plt.show() plt.close() plot_ellipses(gmm, X_petal, labels) The \\(\\texttt{modelNames}\\) parameter in the \\(\\texttt{Mclust}\\) function allows us to specify the type of model to fit. It is a character vector that can take different values representing combinations of component and covariance structures. Type \\(\\texttt{?mclustModelNames}\\) in the R console to retrieve the information on the available model names. The following models are available in package \\(\\texttt{mclust}\\). \\(\\textbf{Univariate Mixture}\\) -“E”: equal variance (one-dimensional) -“V”: varying/unequal variance (one-dimensional) 2.\\(\\textbf{Multivariate Mixture}\\) “EII”: Spherical, equal volume (variance). “VII”: Spherical, varying/unequal volume (variance). “EEI”: Diagonal, equal volume and shape. “VEI”: Diagonal, varying volume, equal shape “EVI”: Diagonal, equal volume, varying shape. “VVI”: Diagonal, varying volume and shape. “EEE”: Ellipsoidal, equal volume, shape, and orientation. “VEE”: Ellipsoidal, equal shape and orientation. “EVE”: Ellipsoidal, equal volume and orientation. “VVE”: Ellipsoidal, equal orientation. “EEV”: Ellipsoidal, equal volume and equal shape. “VEV”: Ellipsoidal, equal shape. “EVV”: Ellipsoidal, equal volume. “VVV”: Ellipsoidal, varying volume, shape, and orientation. 3.\\(\\textbf{Single Component}\\) “X”: univariate normal “XII”: spherical multivariate normal “XII”: diagonal multivariate normal “XXX”: ellipsoidal multivariate normal 8.2.4 Pros and Cons of Clustering Methods The following table summarizes some of the pros and cons of hierarchical clustering, \\(K\\)-means and model-based clustering methods. \\[ \\begin{array}{c|c|c} \\hline \\text{Methods}&amp;\\text{Pros}&amp;\\text{Cons}\\\\ \\hline \\text{Hierarchical}&amp;\\text{no need to specify $K$}; &amp;\\text{generally unique solution}\\\\ &amp;\\text{dendrogram easy to interpret}; &amp;\\text{expensive computation}\\\\ \\hline \\text{$K$-means}&amp;\\text{low computation cost}; &amp;\\text{solution depends on initial value}\\\\ &amp;\\text{intuitive}; &amp; \\text{large variation}\\\\ \\hline \\text{Model-Based}&amp;\\text{MLE has good properties}; &amp;\\text{results not valid if data do not} \\\\ &amp;\\text{simultaneously determine $K$ and the centers of the clusters}&amp;\\text{follow multivariate normal distribution}\\\\ \\hline \\end{array} \\] 8.3 Determine \\(K\\): Number of Clusters The optimal number of clusters can be determined by several methods. 8.3.1 The Elbow Plot Method We use the \\(\\verb`elbow plot`\\) (scree plot) to help identify the most proper value of \\(K\\). Recall the decomposition of the total variation of the observations (SST) into between-group variation (SSB) and within-group variation (SSW), i.e., \\(SST=SSB+SSW\\). If all objects are in one group, then \\(SSW=SST\\). The within-group variation drops when number of clusters \\(K\\) increases. The \\(\\verb`elbow plot`\\) plots SSW versus # of groups, we choose the value of \\(K\\) at where the plot starts flat. Recall the resulting dendrogram of clustering the observations \\(x_1=5, x_2=3, x_3=9, x_4=6, x_5=1\\) using hierarchical clustering with average linkage is ## &lt;Figure size 700x500 with 0 Axes&gt; ## {&#39;icoord&#39;: [[5.0, 5.0, 15.0, 15.0], [35.0, 35.0, 45.0, 45.0], [25.0, 25.0, 40.0, 40.0], [10.0, 10.0, 32.5, 32.5]], &#39;dcoord&#39;: [[0.0, np.float64(2.0), np.float64(2.0), 0.0], [0.0, np.float64(1.0), np.float64(1.0), 0.0], [0.0, np.float64(3.5), np.float64(3.5), np.float64(1.0)], [np.float64(2.0), np.float64(4.666666666666667), np.float64(4.666666666666667), np.float64(3.5)]], &#39;ivl&#39;: [2, 5, 3, 1, 4], &#39;leaves&#39;: [1, 4, 2, 0, 3], &#39;color_list&#39;: [&#39;C1&#39;, &#39;C2&#39;, &#39;C0&#39;, &#39;C0&#39;], &#39;leaves_color_list&#39;: [&#39;C1&#39;, &#39;C1&#39;, &#39;C0&#39;, &#39;C2&#39;, &#39;C2&#39;]} ## Text(0.5, 1.0, &#39;Cluster Dendrogram&#39;) ## Text(0.5, 0, &#39;Variables&#39;) ## Text(0, 0.5, &#39;Distance&#39;) To draw the scree plot, we calculate the within-cluster variation SSW for different values of \\(K\\): When \\(K=2\\), the two clusters are \\(\\{x_1, x_4, x_2, x_5\\}\\) and \\(\\{x_3\\}\\). The centers for the two groups are \\(m_1=\\frac{5+6+3+1}{4}=3.75\\) and \\(m_2=9\\). And the \\[ SSW=[(5-3.75)^2+(6-3.75)^2+(3-3.75)^2+(1-3.75)^2]+0=14.75 \\] When \\(K=3\\), the three clusters are \\(\\{x_1, x_4\\}\\),\\(\\{x_2, x_5\\}\\) and \\(\\{x_3\\}\\). The centers for the three groups are \\(m_1=\\frac{5+6}{2}=5.5, m_2=\\frac{3+1}{2}=2\\) and \\(m_3=9\\). And the \\[ SSW=[(5-5.5)^2+(6-5.5)^2]+[(3-2)^2+(1-2)^2]+0=2.5 \\] When \\(K=4\\), the four clusters are \\(\\{x_1, x_4\\}, \\{x_2\\}, \\{x_5\\}\\) and \\(\\{x_3\\}\\). The centers for the four groups are \\(m_1=\\frac{5+6}{2}=5.5, m_2=3, m_3=1\\) and \\(m_4=9\\). And the \\[ SSW=[(5-5.5)^2+(6-5.5)^2]+0+0+0=0.5 \\] The scree plot is given by k_values = [2, 3, 4] SSW_values = [14.75, 2.5, 0.5] plt.figure(figsize=(7, 4.5)) ## &lt;Figure size 700x450 with 0 Axes&gt; plt.plot(k_values, SSW_values, marker=&#39;o&#39;, linestyle=&#39;-&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x2b8dc5450&gt;] plt.xlabel(&quot;K: # of Groups&quot;) ## Text(0.5, 0, &#39;K: # of Groups&#39;) plt.ylabel(&quot;SSW: Within Sum of Squares&quot;) ## Text(0, 0.5, &#39;SSW: Within Sum of Squares&#39;) plt.title(&quot;Scree Plot (Elbow Method)&quot;) ## Text(0.5, 1.0, &#39;Scree Plot (Elbow Method)&#39;) plt.grid(True) plt.xticks(k_values) ## ([&lt;matplotlib.axis.XTick object at 0x2b8d87110&gt;, &lt;matplotlib.axis.XTick object at 0x2b8dc5590&gt;, &lt;matplotlib.axis.XTick object at 0x2b8dc5d10&gt;], [Text(2, 0, &#39;2&#39;), Text(3, 0, &#39;3&#39;), Text(4, 0, &#39;4&#39;)]) plt.tight_layout() plt.show() plt.close() The scree plot shows that curve becomes flat at \\(K=3\\); therefore, \\(K=3\\) is a proper number of clusters. We can calculate the within-group sum of squares using K-means: from sklearn.cluster import KMeans x = np.array([5, 3, 9, 6, 1]).reshape(-1, 1) k_values = [2, 3, 4] wss = [] for k in k_values: kmeans = KMeans(n_clusters=k, random_state=42, n_init=&#39;auto&#39;) kmeans.fit(x) wss.append(kmeans.inertia_) # inertia_ is the total within-cluster sum of squares ## KMeans(n_clusters=2, random_state=42) ## KMeans(n_clusters=3, random_state=42) ## KMeans(n_clusters=4, random_state=42) plt.figure(figsize=(7, 4.5)) ## &lt;Figure size 700x450 with 0 Axes&gt; plt.plot(k_values, wss, marker=&#39;o&#39;, linestyle=&#39;-&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x2b8e3dbd0&gt;] plt.title(&quot;Elbow Method&quot;) ## Text(0.5, 1.0, &#39;Elbow Method&#39;) plt.xlabel(&quot;Number of Clusters&quot;) ## Text(0.5, 0, &#39;Number of Clusters&#39;) plt.ylabel(&quot;Within-cluster Sum of Squares&quot;) ## Text(0, 0.5, &#39;Within-cluster Sum of Squares&#39;) plt.xticks(k_values) ## ([&lt;matplotlib.axis.XTick object at 0x2b8e0b890&gt;, &lt;matplotlib.axis.XTick object at 0x2b8e3dd10&gt;, &lt;matplotlib.axis.XTick object at 0x2b8e3e490&gt;], [Text(2, 0, &#39;2&#39;), Text(3, 0, &#39;3&#39;), Text(4, 0, &#39;4&#39;)]) plt.grid(True) plt.tight_layout() plt.show() plt.close() 8.3.2 The Silhouette Score The silhouette score is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. The formula for the silhouette score for a single data point ii is given by: \\[ S(i)=\\frac{b(i)-a(i)}{\\max\\{a(i), b(i)\\}} \\] where \\(a(i)\\) is the average distance from the th data point to the other data points in the same cluster (cohesion), \\(b(i)\\) is the smallest average distance from the th data point to data points in a different cluster, minimized over clusters (separation). The overall silhouette score for the clustering is the average of the silhouette scores for all data points. If the silhouette score is close to 1, it indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If it’s close to -1, it suggests that the object is poorly matched to its own cluster and well matched to neighboring clusters. from sklearn.metrics import silhouette_score sil_scores = [] for k in k_values: kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) labels = kmeans.fit_predict(x) score = silhouette_score(x, labels) sil_scores.append(score) plt.figure(figsize=(7, 4.5)) ## &lt;Figure size 700x450 with 0 Axes&gt; plt.plot(k_values, sil_scores, marker=&#39;o&#39;, linestyle=&#39;-&#39;) ## [&lt;matplotlib.lines.Line2D object at 0x2b8ea5bd0&gt;] plt.title(&quot;Silhouette Method&quot;) ## Text(0.5, 1.0, &#39;Silhouette Method&#39;) plt.xlabel(&quot;Number of Clusters&quot;) ## Text(0.5, 0, &#39;Number of Clusters&#39;) plt.ylabel(&quot;Average Silhouette Width&quot;) ## Text(0, 0.5, &#39;Average Silhouette Width&#39;) plt.xticks(k_values) ## ([&lt;matplotlib.axis.XTick object at 0x2b8e77890&gt;, &lt;matplotlib.axis.XTick object at 0x2b8e74190&gt;, &lt;matplotlib.axis.XTick object at 0x2b8e74550&gt;], [Text(2, 0, &#39;2&#39;), Text(3, 0, &#39;3&#39;), Text(4, 0, &#39;4&#39;)]) plt.grid(True) plt.tight_layout() plt.show() plt.close() Since \\(K=2\\) gives the largest silhouette score, the optimal number of clusters is two. 8.3.3 Gap Statistics Gap statistics compare the clustering performance of the actual data with that of a reference dataset with no inherent clustering structure. It can be calculate as \\[ \\text{Gap}(k)=\\frac{1}{B}\\sum_{b=1}^B \\log (W_b)-\\log(W_k) \\] where \\(W_k\\) is the total intra-cluster variation for the clustering solution with \\(k\\) clusters, \\(B\\) is the number of bootstrap samples drawn from the reference dataset, \\(W_b\\) is the total intra-cluster variation for the \\(\\texttt{b}\\)th bootstrap sample. The optimal number of clusters is often chosen as the value of \\(k\\) that maximizes the Gap Statistics. The idea is that if the clustering solution for the actual data is better than the clustering solutions for random data, then the Gap Statistics will be positive, indicating a meaningful structure in the data for the chosen number of clusters. Two built-in functions in R can be used to determine the optimal number of clusters: \\(\\verb`fviz.nbclust`\\) function from package \\(\\verb`factoextra`\\): using Elbow, Silhouhette and Gap statistic methods. \\(\\verb`NbClust`\\) function: voting of 30 indices for choosing the best number of clusters. Take the Iris data for example. We standardize the features before conducting clustering analysis. # from gap_stat import OptimalK # # data = load_iris() # X = StandardScaler().fit_transform(data.data) # # k_values = range(2, 11) # wss = [] # silhouette_scores = [] # # for k in k_values: # kmeans = KMeans(n_clusters=k, n_init=10, random_state=42) # kmeans.fit(X) # wss.append(kmeans.inertia_) # silhouette_scores.append(silhouette_score(X, kmeans.labels_)) # # # elbow # plt.figure(figsize=(7,4)) # plt.plot(k_values, wss, &#39;o-&#39;, label=&#39;Within Sum of Squares&#39;) # plt.axvline(x=4, linestyle=&#39;--&#39;, color=&#39;black&#39;) # plt.title(&#39;Elbow Method&#39;) # plt.xlabel(&#39;Number of clusters k&#39;) # plt.ylabel(&#39;Total Within Sum of Squares&#39;) # plt.show() # plt.close() # # # silhouette # plt.figure(figsize=(7,4)) # plt.plot(k_values, silhouette_scores, &#39;o-&#39;, label=&#39;Silhouette Score&#39;) # plt.axvline(x=2, linestyle=&#39;--&#39;, color=&#39;skyblue&#39;) # plt.title(&#39;Silhouette Method&#39;) # plt.xlabel(&#39;Number of clusters k&#39;) # plt.ylabel(&#39;Average silhouette width&#39;) # plt.show() # plt.close() # # #gap # optimalK = OptimalK(parallel_backend=&#39;joblib&#39;) # n_clusters = optimalK(X, cluster_array=np.arange(1, 11)) # # plt.figure(figsize=(7,4)) # plt.plot(optimalK.gap_df.n_clusters, optimalK.gap_df.gap_value, marker=&#39;o&#39;) # plt.axvline(x=n_clusters, linestyle=&#39;--&#39;, color=&#39;skyblue&#39;) # plt.title(&#39;Gap Statistic Method&#39;) # plt.xlabel(&#39;Number of clusters k&#39;) # plt.ylabel(&#39;Gap statistic&#39;) # plt.show() # plt.close() Again, the optimal number of clusters is two when using all four predictors while the optimal number of clusters is either two or three when using only and . Side-Note on the EM Algorithm The main idea underlying the EM algorithm is as follows. Suppose we observe data partially. Let \\(Y\\) presents the data we observed and \\(Z\\) the missing data. Together, we have the complete data \\(X=(Y, Z)\\). Let \\(g(y, z\\mid \\theta)\\) be the density function based on the complete data and \\(f(y\\mid \\theta)\\) be the density function based on the observed data. Since \\(z\\) is unobserved/latent, we cannot evaluate \\(g(y, z\\mid \\theta)\\) using the regular maximum likelihood method. However, the density function based on the observed data can be calculated as \\[ f(y\\mid \\theta)=\\int g(y, z\\mid \\theta)dz, \\] and then it is straightforward to apply the MLE method to \\(f(y\\mid \\theta)\\). Given this setup, the basic steps of the EM algorithm works as follows: E-step: Compute the expected value of the latent variables given the observed data (\\(y\\)) and the current parameter estimates (\\(\\theta_t\\)) be the current estimate of \\(\\theta\\). That is, \\[ Q(\\theta\\mid\\theta_t)=\\mathbb{E}_Z\\left[\\log g(y,z\\mid\\theta)\\mid y, \\theta_t\\right]. \\] The expectation is taken with respect to the missing data density, i.e., \\[ h(z\\mid y,\\theta)=\\frac{g(y,z\\mid\\theta)}{f(y\\mid\\theta)}. \\] M-step: Maximize the expected log-likelihood \\(Q(\\theta\\mid\\theta_t)\\) with respect to \\(\\theta\\) to get the next value of \\(\\theta_{t+1}\\). Repeat the E-step and M-step until the parameters converge or until a predefined number of iterations is reached. One appealing property of the EM algorithm is that the likelihood function based on the observed data \\(\\log f(y|\\theta)\\) always increases with each iteration, i.e., \\[ \\log f(y|\\theta_{t+1})-\\log f(y|\\theta_t)\\ge 0. \\] \\(\\textbf{Proof}:\\) \\[\\begin{align*} \\log f(y\\mid\\theta_{t+1})-\\log f(y\\mid\\theta_t)&amp; = \\log\\int g(y,z\\mid\\theta_{t+1})dz - \\log\\int g(y,z\\mid\\theta_t)dz=\\log\\frac{\\int g(y,z\\mid\\theta_{t+1})dz}{\\int g(y,z\\mid\\theta_t)dz}\\\\ &amp; =\\log\\frac{\\int g(y,z\\mid\\theta_{t})\\frac{g(y,z\\mid\\theta_{t+1})}{g(y,z\\mid\\theta_t)}dz}{\\int g(y,z\\mid\\theta_t)dz}=\\log \\int\\frac{g(y,z\\mid\\theta_{t})}{\\int g(y,z\\mid\\theta_{t})}\\frac{g(y,z\\mid\\theta_{t+1})}{g(y,z\\mid\\theta_t)}dz\\\\ &amp;=\\log \\int \\frac{g(y,z\\mid\\theta_{t+1})}{g(y,z\\mid\\theta_t)}h(z\\mid y, \\theta_t)dz=\\log\\mathbb{E}_{Z}\\left [\\frac{g(y,z\\mid\\theta_{t+1})}{g(y,z\\mid\\theta_t)}\\right]\\\\ &amp;\\ge \\mathbb{E}\\left[\\log \\frac{g(y,z\\mid\\theta_{t+1})}{g(y,z\\mid\\theta_t)} \\right]=\\mathbb{E}_{Z}\\left[\\log g(y,z\\mid\\theta_{t+1}\\right]-\\mathbb{E}_{Z}\\left[\\log g(y,z\\mid\\theta_{t}\\right]\\\\ &amp;=Q(\\theta_{t+1}\\mid \\theta_t)-Q(\\theta_t\\mid \\theta_t)\\ge 0. \\end{align*}\\] Here are several methods and techniques to diagnose convergence of the EM algorithm: Monitor the log-likelihood values across iterations. In the EM algorithm, the log-likelihood should increase for each iteration. If the log-likelihood stops increasing or becomes flat, it might indicate convergence. Set a tolerance threshold for the change in log-likelihood or parameter estimates. If the change falls below this threshold, consider the algorithm converged. Use information criteria such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) to assess model fit. Lower values indicate better fit, and a stabilized or minimal change may indicate convergence. \\[ \\text{AIC}=-2\\log(L)+2k; \\quad \\text{BIC}=-2\\log(L)+k\\log(n) \\] where \\(L\\) is the maximum likelihood of the model, \\(k\\) is the number of estimated parameters in the model, and \\(n\\) is the sample size. If applicable, run the EM algorithm with multiple initial values or chains. Compare the results across different runs to ensure consistency and convergence. \\(\\textbf{Example}\\): EM Algorithm for Gaussian Mixture Models Suppose \\(y_1, \\cdots, y_n\\) are sampled independently from a mixture of two normal distributions with density: \\[ f(y\\mid\\theta)=\\lambda f_1(y\\mid\\mu_1,\\sigma_1^2) + (1-\\lambda)f_2(y\\mid\\mu_2,\\sigma_2^2). \\] Suppose \\(\\lambda=0.4, \\mu_1=0, \\sigma_1=1, \\mu_2=3, \\sigma_2=1\\), generate \\(n=1000\\) observations from the Gaussian mixture model. Create a data frame with \\(x\\) as the values and \\(y\\) as the class label \\(\\{1, 2\\}\\). Suppose we don’t know the class label, we can use the EM algorithm to estimate the parameters, i.e., \\(\\lambda, \\mu_i, \\sigma_1=i, i=1, 2\\). [(2a)] If the class label is unknown, we need to introduce a latent variable \\(Z\\) to indicate the class label: \\(z=1\\) for class 1 and \\(z=0\\) for class 2. The latent variable \\(Z\\) can be modeled by a Bernoulli distribution, \\(Z\\sim \\text{Bernoulli}(\\lambda)\\). Given the data \\(x_1, \\cdots, x_n\\) and class label \\(z_1, \\cdots, z_n\\), write down the likelihood function of the completed data. \\[ g(x,z\\mid\\theta)= [f_1(x\\mid\\mu_1,\\sigma_1^2)]^{z}[f_2(x\\mid\\mu_2,\\sigma^2_2)]^{1-z}\\lambda^z(1-\\lambda)^{1-z}, \\quad \\text{with }z=0, 1. \\] It can be shown that the likelihood function based on the observed data is \\[ f(x\\mid\\theta)=\\sum _{z=0}^1[f_1(x\\mid\\mu_1,\\sigma_1^2)]^{z}[f_2(x\\mid\\mu_2,\\sigma^2_2)]^{1-z}\\lambda^z(1-\\lambda)^{1-z}. \\] And then the log-likelihood function based on the completed data is \\[ \\log g(x, z\\mid\\theta) = \\sum_{i=1}^n [z_i\\log f_1(x_i\\mid\\mu_1,\\sigma^2_1) + (1-z_i)\\log f_2(x_i\\mid\\mu_2,\\sigma^2_2) + z_i\\log\\lambda + (1-z_i)\\log(1-\\lambda)]. \\] [(2b)] E-step of the EM algorithm: calculate \\begin{align*} Q()&amp;=Z[g(x, z)]\\ &amp;=\\ &amp;={i=1}^n \\ &amp;=_{i=1}^n _if_1(x_i_1,_1^2) (1-_i) f_2(x_i_2,_2^2) _i (1-i)(1-)\\ &amp; = {i=1}^n _i (1-_i)+ _i\\ &amp;+ (1-_i)(1-). \\end{align*} Note that the conditional distribution of \\(Z\\) \\[ h(z\\mid x,\\theta) \\propto [\\lambda f_1(x\\mid\\mu_1,\\sigma_1^2)]^z[(1-\\lambda)f_2(x\\mid\\mu_2,\\sigma_2^2)]^{1-z}\\sim \\text{Bernoulli}\\left(\\frac{\\lambda f_1(x\\mid\\mu_1,\\sigma_1^2)}{\\lambda f_1(x\\mid\\mu_1,\\sigma_1^2)+ (1-\\lambda)f_2(x\\mid\\mu_2,\\sigma_2^2)} \\right). \\] Therefore, \\[ \\pi_i=\\mathbb{E}[z_i\\mid x_i, \\theta]=\\frac{\\lambda f_1(x\\mid\\mu_1,\\sigma_1^2)}{\\lambda f_1(x_i\\mid\\mu_1,\\sigma_1^2)+ (1-\\lambda)f_2(x_i\\mid\\mu_2,\\sigma_2^2)} \\] [(2c)] M-step of the EM algorithm: find the values of \\(\\theta\\) such that \\[ Q(\\theta)=\\sum_{i=1}^n\\pi_i\\left[ -\\frac{1}{2}\\log 2\\pi\\sigma_1^2-\\frac{1}{2\\sigma_1^2}(x_i-\\mu_1)^2 \\right]+(1-\\pi_i)\\left[-\\frac{1}{2}\\log 2\\pi\\sigma_2^2-\\frac{1}{2\\sigma_2^2}(x_i-\\mu_2)^2\\right]+\\pi_i\\log\\lambda+(1-\\pi_i)\\log(1-\\lambda) \\] is maximized. Take the derivatives with respect to \\(\\lambda, \\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2\\), we obtain \\[\\begin{align*} \\frac{\\partial Q}{\\partial \\lambda}&amp;=0\\Longrightarrow \\sum\\left[\\pi_i \\frac{1}{\\lambda}+(1-\\pi_i)\\frac{1}{1-\\lambda}(-1)\\right]\\Longrightarrow \\hat \\lambda=\\frac{\\sum \\pi_i}{n}\\\\ \\frac{\\partial Q}{\\partial \\mu_1}&amp;=0\\Longrightarrow \\sum \\left[\\pi_i (-\\frac{1}{2\\sigma_1^2})2(x_i-\\mu_1)(-1)\\right]=0\\Longrightarrow \\hat \\mu_1 =\\frac{\\sum \\pi_i x_i}{\\sum \\pi_i}\\\\ \\frac{\\partial Q}{\\partial \\mu_2}&amp;=0\\Longrightarrow \\sum \\left[(1-\\pi_i) (-\\frac{1}{2\\sigma_2^2})2(x_i-\\mu_2)(-1)\\right]=0\\Longrightarrow \\hat \\mu_2 =\\frac{\\sum (1-\\pi_i) x_i}{\\sum (1-\\pi_i)}\\\\ \\frac{\\partial Q}{\\partial \\sigma_1^2}&amp;=0\\Longrightarrow \\sum \\left[\\pi_i (0-\\frac{1}{\\sigma_1^2})-\\frac{1}{2(\\sigma_1^2)^2}(-1)(x_i-\\mu_1)^2\\right]=0\\Longrightarrow \\hat \\sigma^2_1=\\frac{\\sum\\pi_i(x_i-\\mu_1)^2}{\\sum\\pi_i}\\\\ \\frac{\\partial Q}{\\partial \\sigma_2^2}&amp;=0\\Longrightarrow \\sum \\left[(1-\\pi_i) (0-\\frac{1}{\\sigma_2^2})-\\frac{1}{2(\\sigma_2^2)^2}(-1)(x_i-\\mu_2)^2\\right]=0\\Longrightarrow \\hat \\sigma^2_2=\\frac{\\sum (1-\\pi_i)(x_i-\\mu_2)^2}{\\sum(1-\\pi_i)} \\end{align*}\\] \\end{itemize} \\end{enumerate} We first generate observations from a mixture of two univariate normal distributions. n = 1000 lambda_ = 0.4 mu1, mu2 = 0, 3 sigma1, sigma2 = 1, 1 n1 = int(n * lambda_) n2 = n - n1 np.random.seed(4061) x1 = np.random.normal(mu1, sigma1, n1) x2 = np.random.normal(mu2, sigma2, n2) x = np.concatenate([x1, x2]) y = np.concatenate([np.ones(n1), np.full(n2, 2)]) df = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y}) We can also use the built-in R function. gmm_model = GaussianMixture(n_components=2, random_state=0) gmm_model.fit(df[[&#39;x&#39;]]) ## GaussianMixture(n_components=2, random_state=0) print(&quot;Means:&quot;, gmm_model.means_.flatten()) ## Means: [0.10768213 3.08212892] print(&quot;Variances:&quot;, gmm_model.covariances_.flatten()) ## Variances: [1.08884907 0.91708877] print(&quot;Weights (mixing proportions):&quot;, gmm_model.weights_) ## Weights (mixing proportions): [0.42745858 0.57254142] Here is another built-in R function to fit mixture Gaussian models. model_alt = GaussianMixture(n_components=2, covariance_type=&#39;full&#39;, max_iter=100, random_state=0) model_alt.fit(df[[&#39;x&#39;]]) ## GaussianMixture(n_components=2, random_state=0) print(&quot;Alternative method\\nMeans:&quot;, model_alt.means_.flatten()) ## Alternative method ## Means: [0.10768213 3.08212892] print(&quot;Variances:&quot;, model_alt.covariances_.flatten()) ## Variances: [1.08884907 0.91708877] print(&quot;Weights:&quot;, model_alt.weights_) ## Weights: [0.42745858 0.57254142] Revisit Learning Outcomes After finishing this chapter, students should be able to Explain the differences between classification and cluster problems. Describe briefly the main idea and procedure of hierarchical clustering, \\(K\\)-means, and model-based clustering methods. Conduct a clustering analysis using hierarchical clustering method, \\(K\\)-means, and model-based clustering in R. Interpret the R computer outputs of hierarchical clustering, \\(K\\)-means, and model-based clustering methods. "],["canonical-correlation-analysis.html", "9 Canonical Correlation Analysis Learning Outcomes 9.1 Objective 9.2 Obtain the Canonical Variates Pairs 9.3 Interpretation 9.4 Testing \\(\\mathbf{\\Sigma}_{12}=0\\) Revisit Learning Outcomes", " 9 Canonical Correlation Analysis Learning Outcomes After finishing this chapter, students should be able to Understand the objective of canonical correlation analysis. Prove important results related to canonical correlation analysis. Conduct a canonical correlation analysis in R. Interpret the computer output of a canonical correlation analysis. Conduct a hypothesis test to test whether the canonical relations are significant. Determine the proper number of canonical variates pairs. 9.1 Objective Consider two sets of variables, \\[ \\mathbf{X}= \\left[ \\begin{array}{c} X_1\\\\ X_2\\\\ \\vdots\\\\ X_p \\end{array} \\right], \\quad \\quad \\mathbf{Y}= \\left[ \\begin{array}{c} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_q \\end{array} \\right] \\quad \\quad p\\le q \\] with \\[ \\mbox{Cov}\\left[ \\begin{array}{c} \\mathbf{X}\\\\ \\cdots\\\\\\mathbf{Y} \\end{array} \\right]=\\mbox{Cov}\\left[ \\begin{array}{c} X_1\\\\ X_2\\\\ \\vdots\\\\ X_p\\\\ \\cdots \\\\ Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_q \\end{array} \\right]=\\left[ \\begin{array}{ccc} \\mbox{Var}(\\mathbf{X}) &amp;\\vdots &amp;\\mbox{Cov}(\\mathbf{X},\\mathbf{Y}) \\\\ \\cdots&amp;\\cdots&amp;\\cdots\\\\ \\mbox{Cov}(\\mathbf{Y},\\mathbf{X}) &amp;\\vdots &amp; \\mbox{Var}(\\mathbf{Y})\\\\ \\end{array} \\right]=\\left[ \\begin{array}{ccc} \\mathbf{\\Sigma}_{11} &amp;\\vdots &amp;\\mathbf{\\Sigma}_{12} \\\\ \\cdots&amp;\\cdots&amp;\\cdots\\\\ \\mathbf{\\Sigma}_{21} &amp;\\vdots &amp; \\mathbf{\\Sigma}_{22} \\\\ \\end{array} \\right] \\] We want to use \\(k\\) uncorrelated canonical variates pairs \\((u_i, v_i), i=1, 2 \\cdots, k\\) to represent the data such that the pair \\((u_i, v_i)\\) has the \\(i\\)th largest correlation. That is, \\[\\begin{align*} u_1&amp;=\\mathbf{a}_1^{T}\\mathbf{X}=a_{11}X_1+a_{12}X_2+\\cdots+a_{1p}X_p\\\\ u_2&amp;=\\mathbf{a}_2^{T}\\mathbf{X}=a_{21}X_1+a_{22}X_2+\\cdots+a_{2p}X_p\\\\ \\vdots\\\\ u_p&amp;=\\mathbf{a}_p^{T}\\mathbf{X}=a_{p1}X_1+a_{p2}X_2+\\cdots+a_{pp}X_p\\\\ v_1&amp;=\\mathbf{b}_1^{T}\\mathbf{Y}=b_{11}Y_1+b_{12}Y_2+\\cdots+b_{1q}Y_q\\\\ v_2&amp;=\\mathbf{b}_2^{T}\\mathbf{Y}=b_{21}Y_1+b_{22}Y_2+\\cdots+b_{2q}Y_q\\\\ \\vdots\\\\ v_q&amp;=\\mathbf{b}_q^{T}\\mathbf{Y}=b_{q1}Y_1+b_{q2}Y_2+\\cdots+b_{qq}Y_q\\\\ \\end{align*}\\] with \\[ \\mbox{Cov}(u_1, v_1)\\ge \\mbox{Cov}(u_2, v_2)\\ge \\cdots \\ge \\mbox{Cov}(u_p, v_p). \\] And the canonical variates pairs \\((u_i, v_i)\\) are uncorrelated, that is \\[ \\mbox{Cov}(u_i, u_j)=0, \\mbox{Cov}(u_i, v_j)=0, \\mbox{Cov}(v_i, v_j)=0, \\mbox{Cov}(u_j, v_i)=0, \\mbox{ for $i&lt;j$}. \\] The correlations of the canonical variates pairs \\(\\rho_i=\\text{Corr}(u_i, v_i)\\) are called the canonical correlations. Canonical correlation analysis allows us to summarize the covariance matrix \\(\\mathbf{\\Sigma}_{12}\\) whose dimension is \\(p\\times q\\) using \\(p\\) canonical correlations \\(\\rho_i, i=1, 2, \\cdots, p\\). 9.2 Obtain the Canonical Variates Pairs It can be shown that the first canonical variates pair is associated to the eigenvector corresponding to the largest eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\), the correlation of the first canonical pair is the square root of the largest eigenvalue. In general, the \\(i\\)th canonical variates pair is associated to the eigenvector corresponding to the \\(i\\)th largest eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\). \\(\\textbf{Proof}\\): We want to maximize \\(\\mbox{Cov}(u, v)=\\mbox{Cov}(\\mathbf{a}^T\\mathbf{X},\\mathbf{b}^T\\mathbf{Y})=\\mathbf{a}^T\\mathbf{\\Sigma}_{12}\\mathbf{b}\\) with the constraints \\(\\mathbf{a}^T\\mathbf{\\Sigma}_{11}\\mathbf{a}=1\\) and \\(\\mathbf{b}^T\\mathbf{\\Sigma}_{22}\\mathbf{b}=1\\). The steps to obtain the canonical variates pairs are as follows. Find the eigenvalues of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\), \\(\\lambda_1^2 \\ge \\lambda_2^2 \\ge \\cdots \\ge \\lambda_p^2\\). Obtain the corresponding unit eigenvectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\cdots, \\mathbf{e}_p\\). The \\(i\\)th canonical variates pair \\((u_i, v_i)\\) is given by \\[ u_i=\\mathbf{a}_i^T\\mathbf{X}; \\quad \\mathbf{a}_i=\\frac{\\mathbf{e}_i}{\\sqrt{c_i}}; \\mbox{ with $c_i=\\mathbf{e}_i^T \\mathbf{\\Sigma}_{11} \\mathbf{e}_i$} \\] \\[ v_i=\\mathbf{b}_i^T\\mathbf{Y}; \\quad \\mathbf{b}_i=\\frac{1}{\\lambda_i}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a}_i \\] And the correlation of \\(u_i\\) and \\(v_i\\) is \\[ \\rho_i=\\frac{\\mbox{Cov}(u_i, v_i)}{\\sqrt{\\mbox{Var}(u_i)\\mbox{Var}(v_i)}}=\\frac{\\mathbf{a}_i^T \\mathbf{\\Sigma}_{12}\\mathbf{b}_i}{\\sqrt{(\\mathbf{a}_i^T \\mathbf{\\Sigma}_{11}\\mathbf{a}_i)(\\mathbf{b}_i^T \\mathbf{\\Sigma}_{22}\\mathbf{b}_i)}}=\\lambda_i \\] \\(\\textbf{Note}\\):If we want to find \\(q\\) canonical variates to represent \\(\\mathbf{Y}\\), we can find the eigenvalues of the matrix \\(\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\), \\(\\lambda_1^2 \\ge \\lambda_2^2 \\ge \\cdots \\ge \\lambda_p^2\\ge \\cdots \\ge \\lambda_q^2\\). The first \\(p\\) eigenvalues are the same as those of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\). Obtain the corresponding unit eigenvectors \\(\\mathbf{f}_1, \\mathbf{f}_2, \\cdots, \\mathbf{f}_q\\). Then \\[ \\mathbf{b}_i=\\frac{\\mathbf{f}_i}{\\sqrt{d_i}}; \\mbox{ with $d_i=\\mathbf{f}_i^T \\mathbf{\\Sigma}_{22} \\mathbf{f}_i$}, i=1, 2, \\cdots, q \\] \\(\\textbf{Proof}\\):The first canonical variates pair is the one giving the largest correlation. That is to maximize \\[ \\rho=\\frac{\\mbox{Cov}(u, v)}{\\sqrt{\\mbox{Var}(u)\\mbox{Var}(v)}}=\\frac{\\mathbf{a}^T \\mathbf{\\Sigma}_{12}\\mathbf{b}}{\\sqrt{(\\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a})(\\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b})}}. \\] This is equivalent to maximizing \\(\\mathbf{a}^T \\mathbf{\\Sigma}_{12}\\mathbf{b}\\) with the constraints \\(\\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a}=1\\) and \\(\\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b}=1\\). Using the Lagrange multiplier method, we want to find \\[ \\arg \\max_{\\mathbf{a},\\mathbf{b}, \\lambda, \\gamma} Q=\\mathbf{a}^T \\mathbf{\\Sigma}_{12}\\mathbf{b}-\\frac{\\lambda}{2}(\\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a}-1)-\\frac{\\gamma}{2}(\\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b}-1) \\] \\[\\begin{align} \\label{eq:eq1} \\frac{\\partial Q}{\\partial \\mathbf{a}}&amp;=\\mathbf{\\Sigma}_{12}\\mathbf{b}-\\lambda \\mathbf{\\Sigma}_{11}\\mathbf{a}=0 \\Longrightarrow \\mathbf{a}^T\\mathbf{\\Sigma}_{12}\\mathbf{b}=\\lambda \\mathbf{a}^T\\mathbf{\\Sigma}_{11}\\mathbf{a}=\\lambda \\\\ \\label{eq:eq2} \\frac{\\partial Q}{\\partial \\mathbf{b}}&amp;=\\mathbf{\\Sigma}_{21}\\mathbf{a}-\\gamma \\mathbf{\\Sigma}_{22}\\mathbf{b}=0 \\Longrightarrow \\mathbf{b}^T\\mathbf{\\Sigma}_{21}\\mathbf{a}=\\gamma \\mathbf{b}^T\\mathbf{\\Sigma}_{22}\\mathbf{b}=\\gamma\\\\ \\frac{\\partial Q}{\\partial \\lambda}&amp;=0\\Longrightarrow \\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a}=1\\nonumber\\\\ \\frac{\\partial Q}{\\partial \\gamma}&amp;=0\\Longrightarrow \\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b}=1\\nonumber \\end{align}\\] Equations (\\(\\ref{eq:eq1}\\)) and (\\(\\ref{eq:eq2}\\)) imply \\(\\lambda=\\gamma\\). Equation (\\(\\ref{eq:eq2}\\)) also implies \\(\\mathbf{b}=\\frac{1}{\\gamma}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a}\\). Plug it in back to Equation (\\(\\ref{eq:eq1}\\)), we have \\[ \\mathbf{\\Sigma}_{12}\\frac{1}{\\gamma}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a}=\\lambda \\mathbf{\\Sigma}_{11} \\mathbf{a} \\Longrightarrow \\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a}=\\lambda^2\\mathbf{a}. \\] This implies \\(\\lambda^2\\) is an eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\) and \\(\\mathbf{a}\\) is the corresponding eigenvector. By Equation (\\(\\ref{eq:eq1}\\)), \\(\\mathbf{a}^T\\mathbf{\\Sigma}_{12}\\mathbf{b}=\\lambda\\); therefore, to maximize \\(\\mathbf{a}^T\\mathbf{\\Sigma}_{12}\\mathbf{b}\\) is to maximize \\(\\lambda\\) (or \\(\\lambda^2\\)), i.e., the largest eigenvalue. As a result, \\(\\mathbf{a}\\) is an eigenvector associated with the largest eigenvalue \\(\\lambda^2\\) of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\). We need to scale \\(\\mathbf{a}\\) such that \\(\\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a}=1\\). Suppose \\(\\mathbf{e}\\) is the unit eigenvector associated to the largest eigenvalue \\(\\lambda^2\\), let \\[ \\mathbf{a}=\\frac{\\mathbf{e}}{\\sqrt{c}}, \\quad \\mbox{where $c=\\mathbf{e}^T \\mathbf{\\Sigma}_{11}\\mathbf{e}$}. \\] Let \\[ \\mathbf{b}=\\frac{1}{\\lambda}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a} \\] It can be shown that \\(\\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b}=1\\), \\[ \\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b}=(\\frac{1}{\\lambda}\\mathbf{a}^T\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1})\\mathbf{\\Sigma}_{22}(\\frac{1}{\\lambda}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a})=\\frac{1}{\\lambda^2}\\mathbf{a}^T\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a}=\\frac{1}{\\lambda^2}\\mathbf{a}^T\\mathbf{\\Sigma}_{11}(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{a})=\\frac{1}{\\lambda^2}\\mathbf{a}^T\\mathbf{\\Sigma}_{11}(\\lambda^2 \\mathbf{a})=1 \\] Applying the similar idea, we can prove that the second canonical variates pair is related to the eigenvector of the second largest eigenvalue, and etc. If we work on the correlation matrix, the conclusions still hold, just replace the covariance matrices with correlation matrices. \\(\\textbf{Note}\\): There is another way to calculate the canonical variates pairs, the steps are as follows. Find the eigenvalues of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\) and the corresponding unit eigenvectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\cdots \\mathbf{e}_p\\). It can be shown that \\(\\lambda_1^2 \\ge \\lambda_2^2 \\ge \\cdots \\ge \\lambda_p^2\\) are also the \\(p\\) largest eigenvalues of the matrix \\(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\) with the corresponding unit eigenvectors \\(\\mathbf{f}_1, \\mathbf{f}_2, \\cdots \\mathbf{f}_p\\). The \\(i\\)th canonical variates pair \\((u_i, v_i)\\) is given by \\[ u_i=\\mathbf{a}_i^T\\mathbf{X}; \\quad \\mathbf{a}_i=\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_i; \\quad \\quad v_i=\\mathbf{b}_i^T\\mathbf{Y}; \\quad \\mathbf{b}_i=\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{f}_i \\] And the correlation of \\(u_i\\) and \\(v_i\\) is \\(\\rho_i=\\frac{\\mbox{Cov}(u_i, v_i)}{\\sqrt{\\mbox{Var}(u_i)\\mbox{Var}(v_i)}}=\\lambda_i\\). \\(\\textbf{Proof}\\): Recall that we want to find \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) such that \\[ \\rho=\\frac{\\mbox{Cov}(u, v)}{\\sqrt{\\mbox{Var}(u)\\mbox{Var}(v)}}=\\frac{\\mathbf{a}^T \\mathbf{\\Sigma}_{12}\\mathbf{b}}{\\sqrt{(\\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a})(\\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b})}}. \\] is maximized. Let \\[ \\mathbf{c}=\\mathbf{\\Sigma}_{11}^{1/2}\\mathbf{a}; \\mathbf{d}=\\mathbf{\\Sigma}_{22}^{1/2}\\mathbf{b}, \\] then \\[ \\rho=\\mbox{Corr}(u,v)=\\frac{\\mbox{Cov}(u, v)}{\\sqrt{\\mbox{Var}(u)\\mbox{Var}(v)}}=\\frac{\\mathbf{a}^T \\mathbf{\\Sigma}_{12}\\mathbf{b}}{\\sqrt{(\\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a})(\\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b})}}=\\frac{\\mathbf{c}^T\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{d}}{\\sqrt{(\\mathbf{c}^T\\mathbf{c})(\\mathbf{d}^T\\mathbf{d})}}. \\] By Cauchy Schwarz inequality, the numerator \\[ \\mathbf{c}^T\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{d}\\le (\\mathbf{c}^T\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{c})^{1/2}(\\mathbf{d}^T\\mathbf{d})^{1/2}. \\] Since \\(\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\) is a \\(p\\times p\\) symmetric matrix, we have \\[ \\mathbf{c}^T\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{c}\\le \\lambda_1\\mathbf{c}^T\\mathbf{c} \\] where \\(\\lambda_1\\) is the largest eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\) and maximum is attained when \\(\\mathbf{c}=\\mathbf{e}_1\\), the unit eigenvector associated with \\(\\lambda_1\\). Therefore, \\(\\mathbf{a}=\\mathbf{\\Sigma}_{11}^{-1/2} \\mathbf{c}=\\mathbf{\\Sigma}_{11}^{-1/2} \\mathbf{e}_1\\) and \\(\\mathbf{b} \\propto \\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1\\), the \\(\\max_{\\mathbf{a}, \\mathbf{b}}\\mbox{Corr}(\\mathbf{a}^T\\mathbf{X},\\mathbf{b}^T\\mathbf{Y})=\\sqrt{\\lambda_1}\\). Take \\(\\mathbf{b}=\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{f}_1\\) where \\(\\mathbf{f}_1=\\frac{\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1}{|\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1|}=\\frac{\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1}{\\sqrt{\\lambda_1}}\\), the normalized form of \\(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1\\). It can be shown that \\(\\mathbf{f}_1\\) is the unit engenvector corresponding to the largest eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\). Since \\(\\mathbf{e}_1\\) is the unit eigenvector corresponding to the largest eigenvalue of \\(\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\), we have \\[\\begin{equation} \\label{eq:eq3} \\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1=\\lambda_1 \\mathbf{e}_1 \\end{equation}\\] multiplying equation (\\(\\ref{eq:eq3}\\)) both sides by \\(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\), we have \\[ \\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\underbrace{(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1)}\\limits_{\\mbox{eigenvector}}=\\lambda_1 \\underbrace{(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1)}\\limits_{\\mbox{eigenvector}}, \\] which means \\(\\lambda_1\\) is also the largest eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\) and the corresponding unit eigenvector is \\[ \\mathbf{f}_1=\\frac{\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1}{|\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1|} =\\frac{\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1}{\\sqrt{\\mathbf{e}_1^T \\underbrace{\\mathbf{\\Sigma}_{11}^{-1/2} \\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1}\\limits_{\\lambda_1\\mathbf{e}_1 \\mbox{ by Equation (\\ref{eq:eq1})}}}}=\\frac{\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}_1}{\\sqrt{\\lambda_1}}. \\] Another way, an easier way to show that \\(\\mathbf{b}=\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{f}\\) where \\(\\mathbf{f}\\) is the unit eigenvector corresponding to the largest eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{22}^{-1/2}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1/2}\\) is \\[ \\rho=\\frac{\\mbox{Cov}(u, v)}{\\sqrt{\\mbox{Var}(u)\\mbox{Var}(v)}}=\\frac{\\mbox{Cov}(v, u)}{\\sqrt{\\mbox{Var}(v)\\mbox{Var}(u)}}\\frac{\\mathbf{b}^T \\mathbf{\\Sigma}_{21}\\mathbf{a}}{\\sqrt{(\\mathbf{b}^T \\mathbf{\\Sigma}_{22}\\mathbf{b})(\\mathbf{a}^T \\mathbf{\\Sigma}_{11}\\mathbf{a})}}. \\] We will arrive at the conclusion by the same logic as we did for \\(\\mathbf{a}\\) using Cauchy Schwarz inequality. \\(\\textbf{Result}\\) It can be shown that if \\(\\lambda\\) is an eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\mathbf{\\Sigma}_{11}^{-1/2}\\) with the corresponding unit eigenvectors \\(\\mathbf{e}\\), then \\(\\lambda\\) is also an eigenvalue of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\) with the corresponding eigenvector \\(\\mathbf{\\Sigma}_{11}^{-1/2}\\mathbf{e}\\). \\(\\textbf{Example}\\): Canonical Correlation Analysis A researcher has collected data on three psychological variables, four academic variables (standardized test scores) for 600 college freshman. She is interested in how the set of psychological variables relates to the academic variables. In particular, the researcher is interested in how many dimensions (canonical variables) are necessary to understand the association between the two sets of variables.The psychological variables are locus-of-control (\\(X_1\\)), self-concept (\\(X_2\\))and motivation (\\(X_3\\)). The academic variables are standardized tests in reading (\\(Y_1\\), read), writing (\\(Y_2\\), write), math (\\(Y_3\\), math) and science (\\(Y_4\\), science). Determine the sample canonical variates and their correlations. import pandas as pd import numpy as np data = pd.read_csv(&quot;data/mmreg.csv&quot;) x1 = data.iloc[:, 0:3].values # psychological variables x2 = data.iloc[:, 3:7].values # academic variables s11 = np.cov(x1, rowvar=False) s22 = np.cov(x2, rowvar=False) s12 = np.cov(x1.T, x2.T)[:x1.shape[1], x1.shape[1]:] s21 = s12.T # eigen A = np.linalg.inv(s11) @ s12 @ np.linalg.inv(s22) @ s21 eigvals_A, eigvecs_A = np.linalg.eig(A) sorted_idx = np.argsort(-eigvals_A.real) eigvals_A = eigvals_A[sorted_idx] eigvecs_A = eigvecs_A[:, sorted_idx] # Canonical correlations rsquared = eigvals_A.real r = np.sqrt(rsquared) print(&quot;Squared Canonical Correlations (rsuqre):&quot;) ## Squared Canonical Correlations (rsuqre): print(rsquared) ## [0.19930553 0.02351899 0.00050641] print(&quot;\\nCanonical Correlations (r):&quot;) ## ## Canonical Correlations (r): print(r) ## [0.44643648 0.15335902 0.02250348] # Canonical coefficients for X ca = np.diag(eigvecs_A.T @ s11 @ eigvecs_A) a = eigvecs_A / np.sqrt(ca) a = -a # match sign convention print(&quot;\\nCanonical Coefficients for X (a):&quot;) ## ## Canonical Coefficients for X (a): print(a) ## [[ 1.25012121 -0.76596331 0.49665288] ## [-0.23673315 -0.84211102 -1.20512253] ## [ 1.24914344 2.63596248 -1.09350847]] # Canonical coefficients for Y b0 = (np.linalg.inv(s22) @ s21 @ a.T).T / r[:, np.newaxis] print(&quot;\\nCanonical Coefficients for Y (b0):&quot;) ## ## Canonical Coefficients for Y (b0): print(b0) ## [[ 0.03363747 0.04705111 0.0136445 -0.00100402] ## [-0.06674536 -0.0554379 -0.03458983 0.00682534] ## [ 0.74832428 -0.15983915 0.37723787 0.95593596]] # Canonical loadings for Y B = np.linalg.inv(s22) @ s21 @ np.linalg.inv(s11) @ s12 eigvals_B, eigvecs_B = np.linalg.eig(B) sorted_idx_B = np.argsort(-eigvals_B.real) eigvecs_B = eigvecs_B[:, sorted_idx_B] cb = np.diag(eigvecs_B.T @ s22 @ eigvecs_B) b = eigvecs_B / np.sqrt(cb) print(&quot;\\nCanonical Loadings for Y (b):&quot;) ## ## Canonical Loadings for Y (b): print(b) ## [[ 0.04404713 0.00159291 0.08833171 0.12136827] ## [ 0.05508884 0.0904146 -0.09612884 -0.00036555] ## [ 0.0194011 0.00295546 0.08782244 -0.13423836] ## [-0.00379776 -0.12420898 -0.08849519 -0.00969114]] Since \\(p=3, q=4\\), we can obtain \\(\\min\\{p, q\\}=3\\) canonical variates pairs. Based on the comupter outputs, \\[ u_1=1.2501X_1-0.2367X_2+1.2491X_3, \\quad v_1=0.0440Y_1+0.0551Y_2+0.0194Y_3-0.0038Y_4 \\] \\(u_1\\) is more related to locus-of-control and motivation since their coefficients are larger in magnitude.\\(v_1\\) is contributed by reading, writing and math. And their correlation is \\(\\rho_1=\\mbox{Corr}(u_1, v_1)=0.4464\\). Similarly, the second and third pairs are \\[ u_2=-0.7660X_1-0.8421X_2+2.6360X_3, \\quad v_2=0.0016Y_1+0.0904Y_2+0.0030Y_3-0.1242Y_4 \\] \\(u_2\\) is more related to motivation and \\(v_2\\) is more related to science. Their correlation is \\(\\rho_2=\\mbox{Corr}(u_2, v_2)=0.1534\\). \\[ u_3=-0.4967X_1-1.2051X_2-1.0935X_3, \\quad v_3=-0.0883Y_1+0.0961Y_2-0.0878Y_3+0.0885Y_4 \\] \\(u_3\\) is more related to self-concept and motivation, \\(v_3\\) is quite uniformly contributed by reading, writing, math and science. Their correlation is \\(\\rho_3=\\mbox{Corr}(u_3, v_3)=0.0225\\). We can also obtain the canonical correlations and the canonical variate pairs using the built-in Python function. from sklearn.cross_decomposition import CCA cca = CCA(n_components=3) cca.fit(x1, x2) ## CCA(n_components=3) # Canonical coefficients xcoef = cca.x_weights_ # Like m1$xcoef in R ycoef = cca.y_weights_ # Like m1$ycoef in R U, V = cca.transform(x1, x2) # Canonical variates canonical_corrs = [np.corrcoef(U[:, i], V[:, i])[0, 1] for i in range(U.shape[1])] # Canonical correlations print(&quot;Canonical Correlations:&quot;) ## Canonical Correlations: print(np.array(canonical_corrs)) ## [0.44643648 0.15335902 0.02250348] print(&quot;\\nCanonical Coefficients for X (xcoef):&quot;) ## ## Canonical Coefficients for X (xcoef): print(xcoef) ## [[ 0.87680885 -0.47223469 -0.0905574 ] ## [-0.17475389 -0.48841469 0.85493401] ## [ 0.44795906 0.73378845 0.51077117]] print(&quot;\\nCanonical Coefficients for Y (ycoef):&quot;) ## ## Canonical Coefficients for Y (ycoef): print(ycoef) ## [[ 0.61720365 -0.33503872 0.49647279] ## [ 0.74314753 0.25780352 -0.59999653] ## [ 0.25333511 -0.12120509 0.46569993] ## [-0.05111483 -0.89810675 -0.4202886 ]] 9.3 Interpretation Even though we can investigate the association between the original variables and their corresponding canonical variates by comparing the magnitudes of the coefficients, it is better to look at the correlations between the original variables and the canonical variates. Now consider regressing \\(X_i\\)s on \\(u_i\\)s, \\[\\begin{align*} X_1&amp;=c_{11}u_1+c_{12}u_2+\\cdots+c_{1p}u_p\\\\ X_2&amp;=c_{21}u_1+c_{22}u_2+\\cdots+c_{2p}u_p\\\\ \\vdots\\\\ X_p&amp;=c_{p1}u_1+c_{p2}u_2+\\cdots+c_{pp}u_p \\end{align*}\\] which means \\(\\mathbf{X}=\\mathbf{C}\\mathbf{u}\\). Recall that the linear combinations in \\(\\mathbf{X}\\) are \\(\\mathbf{u}=\\mathbf{A}\\mathbf{X}\\) where \\[ \\mathbf{A}=\\left[ \\begin{array}{cccc} a_{11} &amp;a_{12}&amp;\\cdots&amp; a_{1p}\\\\ a_{21} &amp;a_{22}&amp;\\cdots&amp; a_{2p}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ a_{p1} &amp;a_{p2}&amp;\\cdots&amp; a_{pp} \\end{array} \\right]. \\] Therefore, \\(\\mathbf{C}=\\mathbf{A}^{-1}\\), i.e., \\(\\mathbf{X}=\\mathbf{A}^{-1}\\mathbf{u}\\). Similarly, \\(\\mathbf{Y}=\\mathbf{B}^{-1}\\mathbf{v}\\) where \\[ \\mathbf{B}=\\left[ \\begin{array}{cccc} b_{11} &amp;b_{12}&amp;\\cdots&amp; b_{1q}\\\\ b_{21} &amp;b_{22}&amp;\\cdots&amp; b_{2q}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ b_{q1} &amp;b_{q2}&amp;\\cdots&amp; b_{qq} \\end{array} \\right]. \\] We would like to explore the correlations between original variable \\(\\mathbf{X}\\) and \\(\\mathbf{u}\\), \\(\\mathbf{Y}\\) and \\(\\mathbf{u}\\), \\(\\mathbf{X}\\) and \\(\\mathbf{v}\\), and \\(\\mathbf{X}\\) and \\(\\mathbf{v}\\). Since \\[ \\mathbf{u}=\\mathbf{AX}\\Longrightarrow \\mathbf{X}=\\mathbf{A}^{-1}\\mathbf{u}; \\quad \\mathbf{v}=\\mathbf{BY}\\Longrightarrow \\mathbf{Y}=\\mathbf{B}^{-1}\\mathbf{v} \\] Then, \\[\\begin{align*} \\text{Corr}(\\mathbf{X}, \\mathbf{u})&amp;=\\text{Corr}(\\mathbf{A}^{-1}\\mathbf{u}, \\mathbf{u})=\\mathbf{A}^{-1}\\text{Corr}(\\mathbf{u}, \\mathbf{u})=\\mathbf{A}^{-1}\\mathbf{I};\\\\ \\text{Corr}(\\mathbf{Y}, \\mathbf{v})&amp;=\\text{Corr}(\\mathbf{B}^{-1}\\mathbf{v}, \\mathbf{v})=\\mathbf{B}^{-1}\\text{Corr}(\\mathbf{v}, \\mathbf{v})=\\mathbf{B}^{-1}\\mathbf{I};\\\\ \\text{Corr}(\\mathbf{X}, \\mathbf{v})&amp;=\\text{Corr}(\\mathbf{A}^{-1}\\mathbf{u}, \\mathbf{v})=\\mathbf{A}^{-1}\\text{Corr}(\\mathbf{u}, \\mathbf{v})=\\mathbf{A}^{-1}\\mathbf{\\Lambda};\\\\ \\text{Corr}(\\mathbf{Y}, \\mathbf{u})&amp;=\\text{Corr}(\\mathbf{B}^{-1}\\mathbf{v}, \\mathbf{u})=\\mathbf{B}^{-1}\\text{Corr}(\\mathbf{v}, \\mathbf{u})=\\mathbf{B}^{-1}\\mathbf{\\Lambda}, \\end{align*}\\] where \\(\\mathbf{\\Lambda}\\) is a diagonal matrix with \\(\\lambda_1\\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p\\) as the diagonal elements and \\(\\lambda_i^2\\) are the eigenvalues of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\). Take the correlation between \\(\\mathbf{X}\\) and \\(\\mathbf{u}\\) for example, \\[ \\mathbf{A}^{-1}=\\left[ \\begin{array}{cccc} r_{X_1, u_1} &amp;r_{X_1, u_2}&amp;\\cdots&amp; r_{X_1,u_p}\\\\ r_{X_2, u_1} &amp;r_{X_2, u_2}&amp;\\cdots&amp; r_{X_2,u_p}\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ r_{X_p, u_1} &amp;r_{X_p, u_2}&amp;\\cdots&amp; r_{X_p,u_p} \\end{array} \\right]. \\] The sum of squares of the first row of \\(\\mathbf{A}^{-1}\\) is \\(\\text{Var}(X_1)=0.44927511\\). The sum of squares of all elements in \\(\\mathbf{A}^{-1}\\) is the total variation in \\(\\mathbf{X}\\) is given by \\[ \\text{Var}(X_1)+\\text{Var}(X_2)+\\text{Var}(X_3)=\\text{trace}(\\mathbf{\\Sigma_{11}})=0.449+0.498+0.117=1.064. \\] Recall that in linear regression model, the \\(R^2\\) is the percentage of variation in the response variable explained by the predictor variables. Therefore, the sum of squares of the \\(j\\)th column of \\(\\mathbf{A}^{-1}\\) divided by the sum of squares of entire matrix of \\(\\mathbf{A}^{-1}\\) gives the proportion of variation in \\(\\mathbf{X}\\) explained by \\(u_j\\). Similarly, the sum of squares of the \\(j\\)th column of \\(\\mathbf{B}^{-1}\\) divided by the sum of squares of entire matrix of \\(\\mathbf{B}^{-1}\\) gives the proportion of variation in \\(\\mathbf{Y}\\) explained by \\(v_j\\). If we work on the correlation matrix, i.e., the standardized variables, the total variation in the standardized \\(\\mathbf{X}\\) is \\(p\\) and the total variation in the standardized \\(\\mathbf{Y}\\) is \\(q\\). Then, the sum of squares of the \\(j\\)th column of \\(\\mathbf{A}^{-1}\\) divided by \\(p\\) gives the proportion of variation in \\(\\mathbf{X}\\) explained by \\(u_j\\). Similarly, the sum of squares of the \\(j\\)th column of \\(\\mathbf{B}^{-1}\\) divided by \\(q\\) gives the proportion of variation in \\(\\mathbf{Y}\\) explained by \\(v_j\\). rxu = np.linalg.inv(a.T) print(rxu) ## [[ 0.61282695 -0.26386106 0.06399669] ## [ 0.07052849 -0.29723825 -0.63594265] ## [ 0.20060823 0.20773608 -0.18456833]] ryv = np.linalg.inv(b.T) print(ryv) ## [[ 8.89499166 -2.47432533 2.75869217 3.03590494] ## [ 8.85231264 2.14924967 -3.30501598 -0.83551342] ## [ 7.53170467 -1.76934159 2.66974562 -4.65323269] ## [ 6.73708844 -6.56029074 -2.30689461 -0.67997603]] var_x = np.sum(rxu**2, axis=1) print(var_x) ## [0.44927511 0.49774791 0.11746341] print(np.diag(s11)) ## [0.44927511 0.49774791 0.11746341] var_y = np.sum(ryv**2, axis=1) print(var_y) ## [102.07026375 94.60392652 88.6372611 94.2099054 ] print(np.diag(s22)) ## [102.07026375 94.60392652 88.6372611 94.2099054 ] prop_var_x1 = np.sum(rxu[:, 0]**2) / np.sum(rxu**2) print(prop_var_x1) ## 0.39528433214630615 prop_var_y1 = np.sum(ryv[:, 0]**2) / np.sum(ryv**2) print(prop_var_y1) ## 0.6840175053832112 rxv = rxu @ np.diag(np.sqrt(eigvals_A)) print(rxv) ## [[ 0.27358831 -0.04046547 0.00144015] ## [ 0.03148649 -0.04558417 -0.01431092] ## [ 0.08955883 0.0318582 -0.00415343]] ryu = ryv @ np.diag(np.sqrt(eigvals_B)) ## &lt;string&gt;:1: RuntimeWarning: invalid value encountered in sqrt print(ryu) ## [[ 3.97104879 -0.37946012 0.06208017 nan] ## [ 3.95199532 0.32960683 -0.07437436 nan] ## [ 3.36242774 -0.2713445 0.06007856 nan] ## [ 3.00768207 -1.00607979 -0.05191315 nan]] Based on the computer output, \\(u_1\\) accounts for 39.53% variation in \\(\\mathbf{X}\\) and \\(v_1\\) accounts for 68.40% variation in \\(\\mathbf{Y}\\). If we work on the standardized variables, i.e., we work on the correlation matrix rather than the covariance matrix, we will identical canonical correlations and canonical variates, but different matrices of correlations between the original variables and canonical variates. #if we work on the correlation matrix s11 = np.corrcoef(x1, rowvar=False) s22 = np.corrcoef(x2, rowvar=False) s12 = np.corrcoef(np.hstack([x1, x2]), rowvar=False)[:x1.shape[1], x1.shape[1]:] s21 = s12.T A = np.linalg.inv(s11) @ s12 @ np.linalg.inv(s22) @ s21 eigvals_A, eigvecs_A = np.linalg.eig(A) idx = np.argsort(-eigvals_A.real) eigvals_A = eigvals_A[idx].real eigvecs_A = eigvecs_A[:, idx] rsuqre = eigvals_A print(rsuqre) ## [0.19930553 0.02351899 0.00050641] r1 = np.sqrt(rsuqre)#the correlation, the same as r print(r1) ## [0.44643648 0.15335902 0.02250348] ca = np.diag(eigvecs_A.T @ s11 @ eigvecs_A)#figure out the normalizing constants, var(u1)=1 a1 = eigvecs_A / np.sqrt(ca)#normally, make the one with the largest magnitude positive a1 = -a1 print(a1) ## [[ 0.83793108 -0.51340979 0.33289643] ## [-0.1670182 -0.59411986 -0.85022903] ## [ 0.42811813 0.90342172 -0.37477745]] b01 = (np.linalg.inv(s22) @ s21 @ a1.T).T / r1[:, np.newaxis]#if want p variates print(b01) ## [[ 0.3889304 0.53118425 0.15184358 -0.05432815] ## [-1.0506522 -1.14822694 -0.50152911 0.43962014] ## [ 2.74125976 -2.39806227 1.27616057 5.69531967]] B = np.linalg.inv(s22) @ s21 @ np.linalg.inv(s11) @ s12 eigvals_B, eigvecs_B = np.linalg.eig(B) idx_B = np.argsort(-eigvals_B.real) eigvals_B = eigvals_B[idx_B].real eigvecs_B = eigvecs_B[:, idx_B] cb = np.diag(eigvecs_B.T @ s22 @ eigvecs_B)#figure out the normalizing constants, var(v1)=1 b1 = eigvecs_B / np.sqrt(cb)# the same as b print(b1) ## [[ 0.44500741 0.01609314 0.89241378 1.22618151] ## [ 0.53581915 0.87941356 -0.93499287 -0.00355549] ## [ 0.18265622 0.02782487 0.82682508 -1.26381876] ## [-0.03686176 -1.20559459 -0.85895011 -0.09406397]] rxu1 = np.linalg.inv(a1.T) print(rxu1) ## [[ 0.91428518 -0.39365803 0.09547756] ## [ 0.09996773 -0.42130826 -0.90139104] ## [ 0.58532551 0.6061228 -0.53852502]] ryv1 = np.linalg.inv(b1.T) print(ryv1) ## [[ 0.88043222 -0.24491037 0.27305719 0.3004959 ] ## [ 0.91012734 0.22096948 -0.33979656 -0.08590112] ## [ 0.79999103 -0.18793321 0.28357094 -0.49424992] ## [ 0.69410307 -0.6758881 -0.23767279 -0.070056 ]] # reuse canonical weights a from previous block (covariance-based) rxv1 = np.linalg.inv(a.T) @ np.diag(r1) print(rxv1) ## [[ 0.27358831 -0.04046547 0.00144015] ## [ 0.03148649 -0.04558417 -0.01431092] ## [ 0.08955883 0.0318582 -0.00415343]] ryu1 = np.linalg.inv(b.T) @ np.diag(np.sqrt(eigvals_B)) print(ryu1) ## [[ 3.97104879e+00 -3.79460120e-01 6.20801703e-02 1.19700552e-08] ## [ 3.95199532e+00 3.29606833e-01 -7.43743567e-02 -3.29428685e-09] ## [ 3.36242774e+00 -2.71344501e-01 6.00785637e-02 -1.83469025e-08] ## [ 3.00768207e+00 -1.00607979e+00 -5.19131537e-02 -2.68102948e-09]] We can also use the built-in function to obtain matrices of correlations between the original variables and canonical variates. data = pd.read_csv(&quot;data/mmreg.csv&quot;) x1 = data.iloc[:, 0:3] x2 = data.iloc[:, 3:7] x1_names = x1.columns.tolist() x2_names = x2.columns.tolist() x1 = (x1 - x1.mean()) / x1.std() x2 = (x2 - x2.mean()) / x2.std() cca = CCA(n_components=min(x1.shape[1], x2.shape[1])) cca.fit(x1, x2) ## CCA(n_components=3) X_c, Y_c = cca.transform(x1, x2) # xu: correlation between X and canonical variates of X xu = np.corrcoef(x1.T, X_c.T)[:x1.shape[1], x1.shape[1]:] print(pd.DataFrame(xu, index=x1_names, columns=[f&quot;U{i+1}&quot; for i in range(X_c.shape[1])])) ## U1 U2 U3 ## locus_of_control 0.914292 -0.393641 -0.095478 ## self_concept 0.099976 -0.421308 0.901390 ## motivation 0.585314 0.606133 0.538526 # yv: correlation between Y and canonical variates of Y yv = np.corrcoef(x2.T, Y_c.T)[:x2.shape[1], x2.shape[1]:] print(pd.DataFrame(yv, index=x2_names, columns=[f&quot;V{i+1}&quot; for i in range(Y_c.shape[1])])) ## V1 V2 V3 ## read 0.880434 -0.244905 0.273057 ## write 0.910126 0.220975 -0.339797 ## math 0.799992 -0.187928 0.283571 ## science 0.694107 -0.675884 -0.237673 # xv: correlation between X and canonical variates of Y xv = np.corrcoef(x1.T, Y_c.T)[:x1.shape[1], x1.shape[1]:] print(pd.DataFrame(xv, index=x1_names, columns=[f&quot;V{i+1}&quot; for i in range(Y_c.shape[1])])) ## V1 V2 V3 ## locus_of_control 0.408171 -0.060368 -0.002149 ## self_concept 0.044630 -0.064611 0.020284 ## motivation 0.261310 0.092956 0.012119 # yu: correlation between Y and canonical variates of X yu = np.corrcoef(x2.T, X_c.T)[:x2.shape[1], x2.shape[1]:] print(pd.DataFrame(yu, index=x2_names, columns=[f&quot;U{i+1}&quot; for i in range(X_c.shape[1])])) ## U1 U2 U3 ## read 0.393058 -0.037552 0.006145 ## write 0.406313 0.033895 -0.007647 ## math 0.357146 -0.028815 0.006381 ## science 0.309875 -0.103648 -0.005349 9.4 Testing \\(\\mathbf{\\Sigma}_{12}=0\\) If \\(\\mathbf{\\Sigma}_{12}=0\\), all canonical correlations must be 0; there is no point in conducting a canonical correlation analysis. By the result of a likelihood ratio test, reject \\(H_0: \\mathbf{\\Sigma}_{12}=0 (\\rho_1=\\rho_2=\\cdots=\\rho_p=0)\\) at the significance level \\(\\alpha\\) if \\(-\\left(n-1-\\frac{p+q+1}{2}\\right)\\ln(\\prod_{i=1}^p(1-\\hat \\rho_i^2))&gt;\\chi_{pq}^2(\\alpha)\\) where \\(\\hat \\rho_1^2\\ge \\hat \\rho_2^2\\ge \\cdots \\ge \\hat \\rho_p^2\\) are the eigenvalues of the matrix \\(\\mathbf{\\Sigma}_{11}^{-1}\\mathbf{\\Sigma}_{12}\\mathbf{\\Sigma}_{22}^{-1}\\mathbf{\\Sigma}_{21}\\). We can conduct a sequence of hypotheses to determine the number of canonical variates pairs to summarize the data: \\[\\begin{align*} H_0^{(k)}&amp;:\\rho_1 \\ne 0, \\rho_2\\ne 0, \\cdots, \\rho_k \\ne 0, \\rho_{k+1}=\\cdots=\\rho_p=0\\\\ H_a^{(k)}&amp;:\\rho_i\\ne 0 \\mbox{ for $i\\ge k+1$} \\end{align*}\\] Reject \\(H_0\\) if \\(-\\left(n-1-\\frac{p+q+1}{2}\\right)\\ln(\\prod_{i=k+1}^p(1-\\hat \\rho_i^2))&gt;\\chi_{(p-k)(q-k)}^2(\\alpha)\\). \\(\\textbf{Example}\\): Canonical Correlation Analysis (continued) How much data variation can be explained by the first canonical variates? At the 5% significance level, test whether the canonical relations are significant. How many pairs of canonical variates provide a good summary of the relations between the two sets of variables? We have done Part (a) in the previous section. Based on the computer output, \\(u_1\\) accounts for 39.62% of the variation in \\(\\mathbf{X}\\) and \\(v_1\\) accounts for 68.13% of the variation in \\(\\mathbf{Y}\\). \\(v_1\\) has a better representation of the academic variables. Here is another way to obtain the percentage. s11 = np.cov(x1, rowvar=False) s22 = np.cov(x2, rowvar=False) s12 = np.cov(x1.T, x2.T)[:x1.shape[1], x1.shape[1]:] s21 = s12.T pmatu = (a @ s11).T print(pmatu) ## [[ 1.2407433 -0.67630669 1.43233379] ## [-0.40863834 -1.23039995 2.53424601] ## [ 0.58206333 -1.50616207 -0.02664144]] pmatv = (b @ s22).T print(pmatv) ## [[ 0.1888782 0.04637217 -0.01180297 -0.14868063] ## [ 0.15424177 0.06401741 -0.00568862 -0.18809987] ## [ 0.1980915 -0.00174348 0.01567963 -0.17595242] ## [ 0.21007166 0.02670519 -0.06211309 -0.14048769]] prop_x = np.sum(pmatu[:, 0]**2) / np.sum(pmatu**2) print(prop_x) ## 0.1385681530989629 prop_y = np.sum(pmatv[:, 0]**2) / np.sum(pmatv**2) print(prop_y) ## 0.5446350243255978 Here is the R code for Part (b). from scipy.stats import chi2 A = np.linalg.inv(s11) @ s12 @ np.linalg.inv(s22) @ s21 eigvals_A, _ = np.linalg.eig(A) eigvals_A = np.sort(eigvals_A.real)[::-1] # sort descending alpha = 0.05 n = x1.shape[0] p = x1.shape[1] q = x2.shape[1] k = min(p, q) test = np.zeros(k) qvec = np.zeros(k) # Likelihood ratio test (Wilks&#39; Lambda transformed) for i in range(k): product_term = np.prod(1 - eigvals_A[i:k]) test[i] = -(n - 1 - (p + q + 1)/2) * np.log(product_term) df = (p - i) * (q - i) qvec[i] = chi2.ppf(1 - alpha, df) print(&quot; test qvec&quot;) ## test qvec for i in range(k): print(f&quot;[{i+1},] {test[i]:.6f} {qvec[i]:.6f}&quot;) ## [1,] 146.716501 21.026070 ## [2,] 14.462374 12.591587 ## [3,] 0.301388 5.991465 The steps for the test are as follows. Hypotheses.\\(H_0: \\rho_1=\\rho_2=\\rho_3=0\\) v.s. $H_a: $. Significance level \\(\\alpha=0.05\\) Test statistic. \\[ -\\left(n-1-\\frac{p+q+1}{2}\\right)\\ln(\\prod_{i=1}^p(1-\\hat \\rho_i^2))=146.7165 \\] Decision: since \\(146.7165&gt;\\chi_{(p=3)(q=4)}^2(0.05)=\\chi_{12}^2(0.05)=21.026\\), we reject \\(H_0\\). Conclusions: at the 5% significance level, we have sufficient evidence that the at least one canonical correlation is not zero. Based on a sequence of hypotheses, we reject the first two and can not reject the third one. As a result, we might need two pairs of canonical variates to provide a good summary of the relations between the two sets of variables. We can conduct the sequence of tests using built-in function to determine the number of canonical variates pairs. from scipy.stats import f # Test via Wilks&#39; Lambda + Rao&#39;s F approximation n = x1.shape[0] p = x1.shape[1] q = x2.shape[1] k = min(p, q) print(&quot;Wilks&#39; Lambda using F-approximation (Rao’s F):&quot;) ## Wilks&#39; Lambda using F-approximation (Rao’s F): print(&quot; From To stat approx df1 df2 p.value&quot;) ## From To stat approx df1 df2 p.value for i in range(k): r2 = eigvals_A[i:] wilks_lambda = np.prod(1 - r2) s = -np.log(wilks_lambda) m = n - 1 - (p + q + 1) / 2 t = (p - i) * (q - i) # Rao’s F approximation df1 = t df2 = m * 2 approx = ((1 - wilks_lambda ** (1 / np.sqrt((t ** 2 + 1)))) / wilks_lambda ** (1 / np.sqrt((t ** 2 + 1)))) * df2 / df1 pval = 1 - f.cdf(approx, df1, df2) print(f&quot; {i+1} to {k} {wilks_lambda:.6f} {approx:.6f} {int(df1)} {int(df2)} {pval:.7f}&quot;) ## 1 to 3 0.781467 2.051625 12 1190 0.0175418 ## 2 to 3 0.975987 0.794119 6 1190 0.5745254 ## 3 to 3 0.999494 0.134800 2 1190 0.8739039 Revisit Learning Outcomes After finishing this chapter, students should be able to Understand the objective of canonical correlation analysis. Prove important results related to canonical correlation analysis. Conduct a canonical correlation analysis in R. Interpret the computer output of a canonical correlation analysis. Conduct a hypothesis test to test whether the canonical relations are significant. Determine the proper number of canonical variates pairs. "],["multidimensional-scaling.html", "10 Multidimensional Scaling Learning Outcomes 10.1 Objective 10.2 Methods 10.3 Example Revisit Learning Outcomes", " 10 Multidimensional Scaling Learning Outcomes After finishing this chapter, students should be able to Explain the objective of multidimensional scaling. Distinguish the difference between metric and non-metric multidimensional scaling. Explain the main idea of classical multidimensional scaling. Explain the main idea of metric multidimensional scaling. Explain the main idea of non-metric multidimensional scaling. 10.1 Objective Given an \\(n\\times n\\) distance matrix \\(\\mathbf{D}\\) corresponding to \\(n\\) objects, try to construct a dataset in a lower-dimensional space with \\(n\\) observations so that the Euclidean distance between points is as close as possible to the provided distance matrix. Denote the distance between two observations \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) in the original space as \\(d_{ij}\\) and the distance in the subspace as \\(\\delta_{ij}\\). 10.2 Methods Depending on whether the magnitude of the original distance \\(d_{ij}\\) needs to be preserved as much as possible or just the relative distance (rank) to be preserved, multidimensional scaling can be classified as metric scaling and non-metric scaling. Metric scaling methods use the magnitude of \\(d_{ij}\\) to find the configuration of \\(\\delta_{ij}\\); while the non-metric scaling methods only use the ranking of \\(d_{ij}\\) to determine \\(\\delta_{ij}\\). 10.2.1 Classical Scaling The classical scaling is find \\(\\delta_{ij}\\) such that \\(\\delta_{ij} \\approx d_{ij}\\), i.e., the following stress function \\[ \\mbox{Stress}=\\left[\\frac{\\displaystyle\\sum_{i&lt;j}(\\delta_{ij}-d_{ij})^2}{\\displaystyle\\sum_{i&lt;j}\\delta_{ij}^2}\\right]^{\\frac{1}{2}} \\mbox{ is minimized}. \\] 10.2.2 Metric Scaling Metric scaling methods assume there exist a monotonic function \\(f\\) such that \\[ \\mbox{Stress}=\\left[\\frac{\\displaystyle\\sum_{i&lt;j}[\\delta_{ij}-f(d_{ij})]^2}{\\displaystyle\\sum_{i&lt;j}\\delta_{ij}^2}\\right]^{\\frac{1}{2}} \\mbox{ is minimized}. \\] For example, we can let \\(f(d_{ij})=\\alpha+\\beta d_{ij}\\). Numerical iteration procedures are used to search for the configuration \\(\\delta_{ij}\\), the values of \\(\\alpha\\) and \\(\\beta\\) such that the stress above is minimized. If the function \\(f(d_{ij})=d_{ij}\\), we have the classical scaling. ### Non-metric Scaling For non-metric scaling methods, the monotonic function \\(f\\) is implicitly defined to preserve the ranking of the original distance, i.e., \\[ d_{ij}&lt;d_{kl} \\Longrightarrow f(d_{ij})\\le f(d_{kl}) \\Longrightarrow \\delta_{ij}^{\\ast}\\le \\delta_{kl}^{\\ast} \\] where \\(\\delta_{ij}^{\\ast}\\) are not distance, they are just numbers representing the ranking of the original distance \\(d_{ij}\\). The configuration of \\(\\delta_{ij}\\) is obtained using numerical iterative methods such that the stress function \\[ \\mbox{Stress}=\\left[\\frac{\\displaystyle\\sum_{i&lt;j}[\\delta_{ij}-f(\\delta_{ij}^{\\ast})]^2}{\\displaystyle\\sum_{i&lt;j}\\delta_{ij}^2}\\right]^{\\frac{1}{2}} \\mbox{ is minimized}. \\] Non-metric scaling methods do not use the actual magnitude of \\(d_{ij}\\) to determine \\(\\delta_{ij}\\), only the rank of \\(d_{ij}\\) is used to guarantee that \\(\\delta_{ij}\\) maintains the same ranking as \\(d_{ij}\\). 10.3 Example The \\(\\verb`eurodist`\\) data in R consists of road distances (in km) between 21 cities in Europe. # python does not have the eurodist package, therefore download the csv from data folder import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv(&quot;data/eurodist.csv&quot;, index_col=0) D = df.values city_names = df.index.tolist() # Classical MDS (cmdscale) def cmdscale(D, k=2): n = len(D) H = np.eye(n) - np.ones((n, n)) / n B = -0.5 * H @ (D ** 2) @ H eigvals, eigvecs = np.linalg.eigh(B) idx = np.argsort(eigvals)[::-1] eigvals = eigvals[idx] eigvecs = eigvecs[:, idx] w = np.where(eigvals &gt; 0)[0] L = np.diag(np.sqrt(eigvals[w[:k]])) V = eigvecs[:, w[:k]] return V @ L loc = cmdscale(D) x, y = loc[:, 0], loc[:, 1] plt.figure(figsize=(10, 6)) ## &lt;Figure size 1000x600 with 0 Axes&gt; plt.xlim(x.min() - 500, x.max() + 500) # set limits manually ## (-2790.2746796314464, 2548.4491128658638) plt.ylim(y.min() - 500, y.max() + 500) ## (-2298.802928085294, 2336.790550393219) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) plt.xticks([]) ## ([], []) plt.yticks([]) ## ([], []) plt.xlabel(&#39;&#39;) ## Text(0.5, 0, &#39;&#39;) plt.ylabel(&#39;&#39;) ## Text(0, 0.5, &#39;&#39;) for i, city in enumerate(city_names): plt.text(x[i], y[i], city, fontsize=9) ## Text(-2290.2746796314464, -1798.8029280852938, &#39;Athens&#39;) ## Text(825.3827903533345, -546.8114799819331, &#39;Barcelona&#39;) ## Text(-59.18334054586801, 367.0813524640463, &#39;Brussels&#39;) ## Text(82.8459728969888, 429.914658184615, &#39;Calais&#39;) ## Text(352.4994348881585, 290.90843282618243, &#39;Cherbourg&#39;) ## Text(-293.6896331438712, 405.3119448051902, &#39;Cologne&#39;) ## Text(-681.9315445294149, 1108.6447775310025, &#39;Copenhagen&#39;) ## Text(9.423363810420714, -240.4059990007937, &#39;Geneva&#39;) ## Text(2048.4491128658638, -642.458543858908, &#39;Gibraltar&#39;) ## Text(-561.1089699422774, 773.3692895561543, &#39;Hamburg&#39;) ## Text(-164.92179949200258, 549.3670405243709, &#39;Hook of Holland&#39;) ## Text(1935.040810566062, -49.12513580493403, &#39;Lisbon&#39;) ## Text(226.42323642764808, -187.08779022879196, &#39;Lyons&#39;) ## Text(1423.3536965978385, -305.8751297911753, &#39;Madrid&#39;) ## Text(299.498710000716, -388.807256477343, &#39;Marseilles&#39;) ## Text(-260.8780456660397, -416.67380908914646, &#39;Milan&#39;) ## Text(-587.6756789484737, -81.18224195198489, &#39;Munich&#39;) ## Text(156.83625680196104, 211.13911235079675, &#39;Paris&#39;) ## Text(-709.4132816619821, -1109.36664746774, &#39;Rome&#39;) ## Text(-839.4459111695458, 1836.790550393219, &#39;Stockholm&#39;) ## Text(-911.2305004780745, -205.93019689753146, &#39;Vienna&#39;) plt.title(&quot;Classical MDS of European Cities&quot;) ## Text(0.5, 1.0, &#39;Classical MDS of European Cities&#39;) plt.grid(False) plt.show() plt.close() Does it make sense? We would expect Lisbon and Gibraltar to be in the West while Athens is in the East. Let’s simply invert this plot by multiply the first column by -1. Plot of recovered from the distances gives a reasonably accurate representation of the actual locations of the European cities. # Flip x axes to match cities&#39; locations x_metric = -loc[:, 0] y_metric = loc[:, 1] plt.figure(figsize=(10, 6)) ## &lt;Figure size 1000x600 with 0 Axes&gt; plt.xlim(x_metric.min() - 500, x_metric.max() + 500) ## (-2548.4491128658638, 2790.2746796314464) plt.ylim(y_metric.min() - 500, y_metric.max() + 500) ## (-2298.802928085294, 2336.790550393219) plt.gca().set_aspect(&#39;equal&#39;, adjustable=&#39;box&#39;) plt.xticks([]) ## ([], []) plt.yticks([]) ## ([], []) plt.xlabel(&#39;&#39;) ## Text(0.5, 0, &#39;&#39;) plt.ylabel(&#39;&#39;) ## Text(0, 0.5, &#39;&#39;) for i, city in enumerate(city_names): plt.text(x_metric[i], y_metric[i], city, fontsize=9) ## Text(2290.2746796314464, -1798.8029280852938, &#39;Athens&#39;) ## Text(-825.3827903533345, -546.8114799819331, &#39;Barcelona&#39;) ## Text(59.18334054586801, 367.0813524640463, &#39;Brussels&#39;) ## Text(-82.8459728969888, 429.914658184615, &#39;Calais&#39;) ## Text(-352.4994348881585, 290.90843282618243, &#39;Cherbourg&#39;) ## Text(293.6896331438712, 405.3119448051902, &#39;Cologne&#39;) ## Text(681.9315445294149, 1108.6447775310025, &#39;Copenhagen&#39;) ## Text(-9.423363810420714, -240.4059990007937, &#39;Geneva&#39;) ## Text(-2048.4491128658638, -642.458543858908, &#39;Gibraltar&#39;) ## Text(561.1089699422774, 773.3692895561543, &#39;Hamburg&#39;) ## Text(164.92179949200258, 549.3670405243709, &#39;Hook of Holland&#39;) ## Text(-1935.040810566062, -49.12513580493403, &#39;Lisbon&#39;) ## Text(-226.42323642764808, -187.08779022879196, &#39;Lyons&#39;) ## Text(-1423.3536965978385, -305.8751297911753, &#39;Madrid&#39;) ## Text(-299.498710000716, -388.807256477343, &#39;Marseilles&#39;) ## Text(260.8780456660397, -416.67380908914646, &#39;Milan&#39;) ## Text(587.6756789484737, -81.18224195198489, &#39;Munich&#39;) ## Text(-156.83625680196104, 211.13911235079675, &#39;Paris&#39;) ## Text(709.4132816619821, -1109.36664746774, &#39;Rome&#39;) ## Text(839.4459111695458, 1836.790550393219, &#39;Stockholm&#39;) ## Text(911.2305004780745, -205.93019689753146, &#39;Vienna&#39;) plt.title(&quot;Classical MDS of European Cities (Flipped Axes)&quot;) ## Text(0.5, 1.0, &#39;Classical MDS of European Cities (Flipped Axes)&#39;) plt.grid(False) plt.show() plt.close() Revisit Learning Outcomes After finishing this chapter, students should be able to Explain the objective of multidimensional scaling. Distinguish the difference between metric and non-metric multidimensional scaling. Explain the main idea of classical multidimensional scaling. Explain the main idea of metric multidimensional scaling. Explain the main idea of non-metric multidimensional scaling. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
